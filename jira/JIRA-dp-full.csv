Issue Id,Summary,Description,Date Created,Date Modified,Issue type,Milestone,Status,Resolution,Reporter,Labels,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments
1,The third argument to both the master and client must be the filename of...,... the sourceAddrFilePath file in order for this example to run.,01/12/12 03:53:38 PM,01/12/12 04:51:53 PM,Task,,Closed,Fixed,acfoltzer,,"Comment:zenzike:01/12/12 04:51:53 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/5f4d5a53b47f934d3f4ff3768f5e785431b51b3c
","Comment:zenzike:01/12/12 04:51:53 PM:

merged","Comment:zenzike:01/12/12 04:51:53 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2,TCP Transport appears to run messages together,"In the course of adding a new named pipes transport, I noticed the following behavior of the TCP transport on demo0.

Ten messages are sent, one concatenated message is received, resulting in the output below.

Note: I'm testing this in rev 16111710187ba8baaf749be97da0dcefa4f262f8 (not yet merged) but it should apply to rev 8c576934bf689861a9f690f24c7a530f9eb16237 as well.


MVAR Transport:
logServer rcvd: [""hello 1""]
logServer rcvd: [""hello 2""]
logServer rcvd: [""hello 3""]
logServer rcvd: [""hello 4""]
logServer rcvd: [""hello 5""]
logServer rcvd: [""hello 6""]
logServer rcvd: [""hello 7""]
logServer rcvd: [""hello 8""]
logServer rcvd: [""hello 9""]
logServer rcvd: [""hello 10""]

TCP Transport:
logServer rcvd: [""hello 1hello 2hello 3hello 4hello 5hello 6hello 7hello 8hello 9hello 10""]

Pipes Transport:
logServer rcvd: [""hello 1""]
logServer rcvd: [""hello 2""]
logServer rcvd: [""hello 3""]
logServer rcvd: [""hello 4""]
logServer rcvd: [""hello 5""]
logServer rcvd: [""hello 6""]
logServer rcvd: [""hello 7""]
logServer rcvd: [""hello 8""]
logServer rcvd: [""hello 9""]
logServer rcvd: [""hello 10""]",02/20/12 04:52:21 AM,02/24/12 10:46:25 AM,Task,,Closed,Fixed,rrnewton,,"Comment:zenzike:02/20/12 10:16:14 AM:

I believe this was fixed in rev d97505c71061002593cc4ceccdddb0702bb147a1, which came after the point where you forked for those revisions: would you mind updating and testing if you still have this issue?","Comment:zenzike:02/24/12 10:46:25 AM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3,Added named pipe backend,This has some limitations remaining (message size) but works for all of the demos in DemoTransport.,02/21/12 05:21:03 AM,02/21/12 03:45:46 PM,Task,,Closed,Fixed,rrnewton,,"Comment:rrnewton:02/21/12 03:45:46 PM:

merged","Comment:rrnewton:02/21/12 03:45:46 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4,"Switched from Binary to cereal, allowed switching between lazy and strict ByteString, fixed some bugs",See email for details; microbenchmark results here: https://gist.github.com/6d958ade26ff70c2cce0,02/23/12 08:45:47 PM,02/23/12 08:49:01 PM,Task,,Closed,Fixed,acfoltzer,,"Comment:acfoltzer:02/23/12 08:49:01 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5,"FIXED: Switched from Binary to cereal, allowed switching between lazy and strict ByteString, fixed some bugs","See email for details; microbenchmark results here: https://gist.github.com/6d958ade26ff70c2cce0

Note that the previous pull request did not merge the upstream changes recently made in this repo. I went ahead and took care of that :)",02/23/12 09:06:37 PM,02/24/12 04:19:52 PM,Task,,Closed,Fixed,acfoltzer,,"Comment:zenzike:02/24/12 10:27:03 AM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/acbc141c5f8b033fb3a7d6f19e43f81c3b9288fc
","Comment:zenzike:02/24/12 10:27:03 AM:

merged","Comment:zenzike:02/24/12 10:27:03 AM:

closed","Comment:rrnewton:02/24/12 03:59:29 PM:

Here are the microbenchmark results that I am currently seeing:

    https://gist.github.com/1901698

Also, Adam, I think this merge broke the ""distributed-process"" layer.  Can you take a look?

Second, the SendTransport benchmarks in your gist look fishy.  Why would 50K take an order of magnitude LESS time to send than a 10K message?

I'm afraid this might be because the results are from the server hulk (often under load).  What do you get when using an unoccupied workstation in the ""mine""?","Comment:rrnewton:02/24/12 04:19:52 PM:

Ok, weird.  I redid the benchmark on gabbro and it still shows the same pattern as yours.  I added some results in a comment on your gist:

   https://gist.github.com/6d958ade26ff70c2cce0_comments",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6,Fixes made for the distributed-process with strict bytestrings,,02/24/12 05:20:46 PM,02/25/12 10:35:28 AM,Task,,Closed,Fixed,acfoltzer,,"Comment:rrnewton:02/25/12 10:35:28 AM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/4e42a1ef69b3c55a6361b2e145cec0cafbfb600e
","Comment:rrnewton:02/25/12 10:35:28 AM:

merged","Comment:rrnewton:02/25/12 10:35:28 AM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7,killThread hangs on GHC 7.0.4 7.2.1 but NOT 6.12.3 and 7.4.1,"The bug is found in DemoTransport.hs, which will hang when trying to ""killThread""
(Pipes backend only, demo3 and demo4).  Yet this hang happens ONLY under GHC 
7.0.4 and 7.2.1. Under GHC 6.12.3 and GHC 7.4.1 it works fine!
    
At first I thought this may be an issue with non-allocating threads not
being preempted by the runtime system (and therefore not servicing the
ThreadKilled asynchronous exception).  But it's hard to explain that
pattern of outcomes on different GHC versions.

See commit: 48e257ad2456c1c4c953281b21c7aa80a9fa3e75
",02/25/12 11:34:08 AM,07/27/12 11:32:38 AM,Task,,Closed,Fixed,rrnewton,,"Comment:rrnewton:02/25/12 10:11:03 PM:

Note for reproducing:

You can use the following bash script inside the `examples` directory to build all four versions of this and compare them for yourself.

    _!/bin/bash

    set -e

    function go() {
      VER=$1
      make clean
      GHC=ghc-$1 make DemoTransport.exe
      mv -f DemoTransport.exe DemoTransport_$VER.exe
    }

    go 6.12.3
    go 7.0.4
    go 7.2.1
    go 7.4.1

","Comment:simonmar:02/27/12 08:40:57 AM:

Might have been a bug in the asynchronous-exception handling code in the RTS.  e.g. this commit might be the fix:

commit fa71e6c795489ec267e0d048395c2c52bea6a164
Author: Simon Marlow <marlowsd@gmail.com>
Date:   Wed Aug 31 22:45:01 2011 +0100

    Fix DPLEGACY-4988: we were wrongly running exception handlers in the
    maskUninterruptible state instead of ordinary mask, due to a
    misinterpretation of the way the TSO_INTERRUPTIBLE flag works.
    
    Remarkably this must have been broken for quite some time.  Indeed we
    even had a test that demonstrated the wrong behaviour (conc015a) but
    presumably I didn't look hard enough at the output to notice that it
    was wrong.
","Comment:edsko:07/27/12 11:32:38 AM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8,testConnectToSelf failing,"On line 451 of TestTransport.hs :
https://github.com/haskell-distributed/distributed-process/blob/edsko/network-transport-2/tests/TestTransport.hs#L451

If I add another line beneath this, so that two connections are made between an endpoint and its own address, then the 'testConnectToSelf' fails. e.g.

```
testConnectToSelf :: Transport -> Int -> IO ()
testConnectToSelf transport numPings = do
  done <- newEmptyMVar
  Right endpoint <- newEndPoint transport

  tlog ""Creating self-connection""
  Right conn <- connect endpoint (address endpoint) ReliableOrdered
  Right dummyConn1 <- connect endpoint (address endpoint) ReliableOrdered
```

It is throwning an error for line 473 : 
https://github.com/haskell-distributed/distributed-process/blob/edsko/network-transport-2/tests/TestTransport.hs#L473

where (Received cid' msg) is not correctly being pattern matched:
```
Running ""ConnectToSelf"": failed (exception: TestTransport.hs:473:46-99: Non-exhaustive patterns in lambda
```

So - having multiple connections from an endpoint to its address appears to be causing problems.",05/11/12 11:58:47 PM,05/12/12 10:42:57 AM,Task,,Closed,Fixed,robstewart57,,"Comment:edsko:05/12/12 10:22:55 AM:

The reason the test fails when you add another call to `connect` is that the endpoint will receive _two_ `ConnectionOpened` messages before it gets the first `Received` message. So the error you are seeing is an error in your test, not in the transport. The latest version of the transport gives more detailed feedback when a test fails. With your modification (the second call to connect) the test now fails with

    Running ""ConnectToSelf"": failed (exception: user error 
        (Pattern match failure in do expression at tests/TestTransport.hs:460:8-29)
    Trace:
    0	ConnectionOpened 1025 ReliableOrdered (EndPointAddress {endPointAddressToByteString = ""127.0.0.1:8080:19""})
    1	ConnectionOpened 1024 ReliableOrdered (EndPointAddress {endPointAddressToByteString = ""127.0.0.1:8080:19""})
    )

which very clearly shows what's going on: the test was expecting a `Received` message on line 460 but instead got a `ConnectionOpened` message.","Comment:edsko:05/12/12 10:22:55 AM:

closed","Comment:robstewart57:05/12/12 10:42:57 AM:

Quite right.

Thanks for clarifying!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9,a handful of understandability tweaks for the diagram,(hopefully an improvement…),05/17/12 03:35:37 PM,05/17/12 03:37:48 PM,Task,,Closed,Fixed,kowey,,"Comment:edsko:05/17/12 03:37:48 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/bcb69aa7529463c5ab07e1ed4c3faae42ce33ddd
","Comment:edsko:05/17/12 03:37:48 PM:

merged","Comment:edsko:05/17/12 03:37:48 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10,sorry!,,05/17/12 03:44:21 PM,05/17/12 03:54:43 PM,Task,,Closed,Fixed,kowey,,"Comment:edsko:05/17/12 03:54:43 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/64df2a24e582752937f31db34edc35fa7dbb1266
","Comment:edsko:05/17/12 03:54:43 PM:

merged","Comment:edsko:05/17/12 03:54:43 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
11,Can't build examples,"I'm having a crack at a zeromq transport and I'm having some trouble building the examples.

`DemoTransport.hs` refers to `Network.Transport.MVar` which doesn't seem to exist. When commenting DemoTransport from the make file (and get rid of the "".exe"" suffixes) I get:

```
ghc -package-conf ../network-transport-zmq/cabal-dev/packages-7.4.2.conf -O2 -rtsopts -threaded --make  DemoProcess.hs -o DemoProcess
[1 of 1] Compiling Main             ( DemoProcess.hs, DemoProcess.o )

DemoProcess.hs:7:31:
    Module `Network.Transport.TCP' does not export `mkTransport'

DemoProcess.hs:7:44:
    Module `Network.Transport.TCP' does not export `TCPConfig(..)'
make: *** [DemoProcess] Error 1
```

Are the examples still the preferred method to play around with the library or should I be looking at other things now, like benchmarks and tests?

Cheers,
Ben",07/02/12 07:44:33 AM,07/25/12 09:56:37 AM,Task,,Closed,Fixed,boothead,,"Comment:edsko:07/02/12 09:12:46 AM:

Hi Ben,

The directories benchmarks/, doc/, examples/ and network-transport-pipes/ are very out of date. I will clean this up soon, as we hope to do a first release on Hackage some time soon. 

If you're interested in examples for Network.Transport, have a look at the tests in network-transport/tests. Similarly, for examples of the Cloud Haskell layer look at distributed-process/tests. 

Also, feel free to ping me at _HaskellTransport on freenode if you have any questions.","Comment:boothead:07/02/12 09:23:19 AM:

Ok, thanks for the info Edsko. I'll ping you next time I run into trouble.","Comment:billdozr:07/25/12 09:47:23 AM:

In absence of the examples at this moment, I have built a simple ping/pong example utilizing the SimpleLocalnet for remote node discovery. However, the closure function referenced from my spawn never seems to get called and hence the receiveWait never returns. There are no errors raised at runtime either.

I'd appreciate any input in finding the cause.
https://gist.github.com/3175290

Regards,
-Alen","Comment:edsko:07/25/12 09:50:41 AM:

Hi Alan,

You are passing initRemoteTable, which does not include the remotable functions that you create in your module. Instead, you should pass (__remoteTable initRemoteTable) as an argument when initializing Cloud Haskell.

Incidentally, the Hackage documentation contains some self-contained examples, and you can find more examples in the test/ directory.","Comment:edsko:07/25/12 09:50:42 AM:

closed","Comment:billdozr:07/25/12 09:56:24 AM:

Thanks Edsko for pointing out the cause. Much appreciate it and thanks for the great work you have been putting into this project.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
12,build problems: time-1.4 -> 1.2.0.5 (with ghc-7.4.2),"I am not able to install distributed-process alongside other packages (e.g., snap).
It seems this is due to version conflicts with random and time.

cabal install distributed-process

Resolving dependencies...
In order, the following would be installed:
random-1.0.1.1 (reinstall) changes: time-1.4 -> 1.2.0.5
distributed-process-0.2.1 (reinstall) changes: random-1.0.1.1 added

when I put '--force-reinst', then others appear broken (red with ghc-pkg list),
when I force-reinstall those, then distributed-process is broken.

",07/15/12 04:31:53 PM,07/16/12 09:37:22 AM,Task,,Closed,Fixed,jwaldmann,,"Comment:edsko:07/16/12 09:37:22 AM:

Thanks for the report. Fixed in 0.2.1.1 (relaxed the upper bound on time). On a fresh install of ghc 7.4.2 with cabal 0.14, after a cabal install snap, cabal install distributed-process proceeds without installing any further dependencies.","Comment:edsko:07/16/12 09:37:22 AM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
13,please improve (API) doc for newcomers,"I just want to play around with distributed-process 
(and later I want my students to do this).

So I find it very useful to have a working example in the docs.
http://hackage.haskell.org/packages/archive/distributed-process-simplelocalnet/0.2.0.1/doc/html/Control-Distributed-Process-Backend-SimpleLocalnet.html

I can compile this (I put -threaded -rtsopts just out of habit, but
is it necessary? If so, the doc should say.) 

but for running,
I am missing a sentence that says how this program should be used.
I guess, start slave(s) and master - but on the same machine?

I do not understand what the second cmd line argument (""host"") is used for.
When I start slave and master on one machine,
and use ""localhost"" for host, it does not seem to work, but it does with ""127.0.0.1""
(so perhaps this is an issue with my resolver).

I do not understand whether this example is supposed to work with slaves/master 
on different machines. If so, then again I do not see what ""host"" arguments to use.

The API doc of ""findPeers"" does not help:
""findPeers t sends out a who's there? request,...""
since it does not say to whom this message is sent.

Sure, these are very basic questions, but if the package is 
going to be used widely,
then I guess they will come up over and over again.
If you think the answer would be too long to put into the API doc,
then put it elsewhere, and refer to it.

Thanks, Johannes.",07/16/12 08:54:22 PM,07/17/12 12:14:22 PM,Task,,Closed,Fixed,jwaldmann,,"Comment:edsko:07/17/12 10:35:53 AM:

Hi Johannes,

Does https://github.com/haskell-distributed/distributed-process/commit/c7d7f5b75eb79e3eeaf56f97fb6418fac61c38f2 answer your questions?","Comment:edsko:07/17/12 10:38:40 AM:

.. and https://github.com/haskell-distributed/distributed-process/commit/9c6cf0d61bd7b285caa6d7340d9d35fab6e8200a","Comment:jwaldmann:07/17/12 12:14:22 PM:

Yes, great. (Both.) I appreciate the wording ""broadcast"" since this refers to a known concept.","Comment:jwaldmann:07/17/12 12:14:22 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
14,"please improve API docs for: Closure, Static","(sorry Edsko, here's another beginner's request)

From reading the API docs I guess that  Control.Distributed.Process.call
is one basic method to ""run a process remotely and wait for it to reply"".
(I guess ""to reply"" is ""execute 'return x' in the Process monad?)

The type of spawn contains both Static and Closure,
and when I track their API docs, I am just seeing data decls with hidden
constructors. What I'm missing is a note (right there) about how such objects
should be created - or how the construction can be avoided.
",07/17/12 06:46:24 PM,07/18/12 07:56:09 AM,Task,,Closed,Fixed,jwaldmann,,"Comment:jwaldmann:07/17/12 06:54:33 PM:

I see that the answer is in Control.Distributed.Process.Closure ,
but still a reference would be nice.","Comment:edsko:07/18/12 07:56:06 AM:

closed","Comment:edsko:07/18/12 07:56:09 AM:

No problem. It's useful to know what parts of the documentation are lacking or confusing. As you already found out, there is a detailed discussion of Static and Clousre in Control.Distributed.Process.Closure. I have added a cross-reference:

https://github.com/haskell-distributed/distributed-process/commit/6274bb24d1ee736b09e9eb4add7d54f571a862a5

Let me know if the explanation in Control.Distributed.Process.Closure is unclear anywhere.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
15,typo in API doc of C.D.P.Closure ,"functionDict should be functionSDict?

https://github.com/haskell-distributed/distributed-process/blob/master/distributed-process/src/Control/Distributed/Process/Closure.hs#L125

",07/18/12 07:36:11 PM,07/19/12 09:30:38 AM,Task,,Closed,Fixed,jwaldmann,,"Comment:edsko:07/19/12 09:30:37 AM:

Fixed https://github.com/haskell-distributed/distributed-process/commit/ef0ef4dc6d3aaaf59a270fcc193fdeec8fea3a93. Thanks.","Comment:edsko:07/19/12 09:30:38 AM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
16,"consider adding a self-contained (compilable, runnable) example for ""call"" to C.D.P.Closure","I understand that with every library, you can only do so much
with the API doc - at some point users just have to read
the underlying paper to get the fundamental abstractions right, and there's no shortcut.

But - here for me the problem is not so much the abstraction,
but rather the technicalities of the work-arounds for not having ""static"".

With technicalities, examples are quite helpful.
You have them distributed over the API docs, that's fine,
but a minimal complete example would be a welcome addition.
I tried for the better part of an hour until arriving at the following. Is it idiomatic?

```
{-# language TemplateHaskell #-}

import System.Environment (getArgs)
import Control.Distributed.Process
import Control.Distributed.Process.Closure
import Control.Distributed.Process.Node (initRemoteTable)
import Control.Distributed.Process.Backend.SimpleLocalnet

compute :: Integer -> Process Integer
compute n = do
    liftIO $ putStrLn ""the slave does a computation""
    return $ n + 1

$(remotable ['compute])

master :: Backend -> [NodeId] -> Process ()
master backend slaves = do
  liftIO . putStrLn $ ""Slaves: "" ++ show slaves
  case slaves of
      [] -> liftIO $ putStrLn ""no slaves""
      s : _ -> do
           out <- call $(functionSDict 'compute) s 
                $  $(mkClosure 'compute) ( 10 ::Integer)
           liftIO $ putStrLn $ show out
  terminateAllSlaves backend

main :: IO ()
main = do
  args <- getArgs
  let rtable :: RemoteTable
      rtable = Main.__remoteTable $ initRemoteTable
  case args of
    [""master"", host, port] -> do
      backend <- initializeBackend host port rtable
      startMaster backend (master backend)
    [""slave"", host, port] -> do
      backend <- initializeBackend host port rtable
      startSlave backend
```
",07/18/12 07:54:09 PM,07/21/12 03:21:51 PM,Task,,Closed,Fixed,jwaldmann,,"Comment:edsko:07/19/12 09:42:54 AM:

Hi Johannes,

No need to qualify every request :) I have tried to document every function that I export but I understand that some high level documentation is missing -- but it's a little hard sometimes to see exactly what parts need more examples. So if you're having difficulties, just let me know and I'll try to fix it :) Feel free also to ping me on _haskell, I'm usually there (though perhaps not, as it happens, over the next week days as I'm travelling and working odd hours).

So it's a reasonable request and I'll make a note to add an example along the lines that you suggest. To answer your specific question: yes, your example is idiomatic, with one exception: if you call `remotable` on a function `f :: T1 -> T2` then 

    $(functionSDict 'f) :: Static (SerializableDict T1)

In your example you have a function `compute :: Integer -> Process Integer`. `call` needs a dictionary for the result of this process (`Integer`), but you're passing the dictionary that was created for the process argument. It happens to work because in your case the argument happens to be of the same type as the result of the process (`Integer`), but it would fail if, for instance, your function would have type `isPrime :: Integer -> Process Bool`.

Now `functionSDict` is for convenience only, and you can easily define your own serializable dictionaries:

    sdictInteger :: SerializableDict Integer
    sdictInteger = SerializableDict

    $(mkStatic 'sdictInteger) :: Static (SerializableDict Integer)

but perhaps the case you cite is common enough to warrant a function `functionTDict` such that if `f :: T1 -> Process T2` then

    $(functionTDict 'f) :: Static (SerializableDict T2)

I need to give that some thought, but I think I'll add that, and then add an example.","Comment:edsko:07/21/12 03:21:51 PM:

Hi Johannes,

I have added functionTDict and improved the documentation of C.D.P.Closure, and included a small self-contained example along the lines you suggested. Please reopen the issue the documentation is unclear.

Thanks for the report.

Edsko","Comment:edsko:07/21/12 03:21:51 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
17,bring back genericGet and genericPut?,"I don't know the technical implications of this, but I think functions like `genericGet` and [`genericPut`](http://hackage.haskell.org/packages/archive/remote/0.1.1/doc/html/Remote.html#v:genericPut) would make things jolly convenient for tutorial writers.",07/19/12 06:38:29 AM,08/07/12 07:28:58 AM,Task,,Closed,Fixed,kowey,,"Comment:edsko:07/19/12 09:28:15 AM:

I think this it outside the scope of Cloud Haskell proper. There are lots of packages for deriving Binary instances, for example http://hackage.haskell.org/package/binary-generic-0.1 (based on Data), http://hackage.haskell.org/packages/archive/binary-derive/0.1.0/doc/html/Data-Binary-Derive.html or http://hackage.haskell.org/packages/archive/generic-binary/1.0.0/doc/html/Data-Binary-Generic.html (based on Generic). Please open this issue if there is a good reason why those packages won't suffice.","Comment:edsko:07/19/12 09:28:15 AM:

closed","Comment:edsko:08/06/12 08:06:03 PM:

FWIW, I've submitted a pull request to make binary-generic work. Once that has been merged and released to Hackage I would recommend using binary-generic to define Binary instances.","Comment:edsko:08/07/12 07:28:58 AM:

This is now released as binary-generic-0.2.1.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
18,include the test source files in the sdist tarball,tests pass on gentoo amd64 with ghc 7.4.2 and 7.5.20120718,07/22/12 01:39:52 AM,07/22/12 11:37:46 AM,Task,,Closed,Fixed,markwright,,"Comment:edsko:07/22/12 11:37:34 AM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/1de977ad592988e4dc21ada69f08819f73eaa0b1
","Comment:edsko:07/22/12 11:37:34 AM:

merged","Comment:edsko:07/22/12 11:37:34 AM:

closed","Comment:edsko:07/22/12 11:37:46 AM:

Thanks for the patch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
19,locked thread in spawn,"This worked in the 'remote' Cloudhaskell package (and works on Erlang)

https://github.com/dysinger/distributed-process-broken/commit/8614e137765805688b0e7014a15d8d4362c63d7c

https://github.com/dysinger/distributed-process-broken/blob/master/Main.hs

https://www.evernote.com/shard/s17/sh/ccb0150e-7873-4dd9-af31-d5cb5a19f067/14471ddd5383dbc9096727543bb2a05b
",07/31/12 05:26:48 AM,07/31/12 10:17:13 AM,Task,,Closed,Fixed,dysinger,,"Comment:edsko:07/31/12 07:50:38 AM:

I don't think the issue is that we cannot spawn a closure on master -- when I run your example with just a single node (i.e. the master only), and stick in a threadDelay to make sure the master does not exit before it gets a chance to print the log message, it prints the result of isPrime and then shows the same MVar error that you're showing. I'll investigate.","Comment:dysinger:07/31/12 07:51:34 AM:

Awesome.","Comment:edsko:07/31/12 08:00:12 AM:

It's related to spawnLink (as opposed to spawn). ","Comment:dysinger:07/31/12 08:03:54 AM:

Yes I saw it in every variant/extension of spawn too.","Comment:edsko:07/31/12 10:12:23 AM:

Right, so: the new implementation of Cloud Haskell follows the semantics described in ""A Unified Semantics for a Future Erlang"". In this semantics, linking makes no distinction between regular termination and abnormal termination. In other words, if you link to a process, and that process dies normally, you die too. This is appropriate for slave processes that link to their masters, where the master should never terminate (masters should monitor their slaves, not link to them). In your example, you are linking to the processes that compute the prime numbers. As soon as this process terminates, the process that spawned it will die too.

The ""thread blocked indefinitely"" exception you were seeing *was* a bug, however. The backend starts the master process using ""runProcess"", which waits for a process to finish -- but it didn't take into account abnormal termination and would beb blocked indefinitely when the process threw an exception. I have fixed this and released it as 0.2.2.0 to Hackage.

If you run your example again (and insert a threadDelay to give the spanwed processes a chance to do anytihng) you will find that the program will now terminate quietly, possibly after or possibly before the log message. 

So in summary: you shouldn't use spawnLink here, but you did find a bug. Thanks for the report :)","Comment:edsko:07/31/12 10:12:23 AM:

closed","Comment:dysinger:07/31/12 10:17:13 AM:

Awesome thanks for the update.  I was mostly concerned because I didn't see the same behavior in the original 'remote' package (it waits for the spawnLink'ed processes to finish before the main exits) and also the mvar thread lock message.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
20,Minor changes/fixes and resurrecting ping/pong benchmark.,"I haven't committed in a long while so I thought I'd run this through you first, Edsko.

",08/07/12 04:24:19 PM,08/10/12 04:33:00 PM,Task,,Closed,Fixed,rrnewton,,"Comment:edsko:08/10/12 04:33:00 PM:

Cherry-picked ac37e8e and a5daa9a as agreed.","Comment:edsko:08/10/12 04:33:00 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
21,Is there any way that Closure/Static could be factored into a separate package?,"Perhaps this is too much to hope for, but it would be ideal for packages such as meta-par, HDpH and distributed-process to be able to share basic RPC functionality (Closure/Static).

With meta-par we copied and hacked a version of the original Cloud Haskell (""Remote"") Closure.  The ugly bit was that it had hard-coded recognition of the IO and ProcessM monads, and we had to tweak that to include monad-par's ""Par"" monad.

I don't understand the current Closure implementation, but it looks like it may be the case that CP.hs (the part that deals with Process values) is pretty well isolated from the rest.  Does that mean that everything but CP.hs could become its own package?

It looks like TH.hs also deals with Process presently.  TH.hs would either need to be replicated in all consumers of the hypothetical factored library, or it would need to become more extensible.  When we were using the Closure mechanism in monad-par/meta-par this was the sticking point -- we didn't see a way to do it without adding extra arguments to the compile-time TH functions (e.g. remotable), which would be very ugly.

Any good ideas here?
",08/07/12 06:15:47 PM,01/07/13 08:50:13 PM,Task,Misc/Admin/Support,Open,Unresolved,rrnewton,,"Comment:edsko:08/08/12 07:25:39 AM:

Yes, the old implementation support for Static/Closure was very tightly interwoven with the rest. This is no longer the case, although you are right that the Template Haskell stuff does recognize Process for convenience. It should be possible to factor most of this out. I think that's a good idea, even just for clarifying the current implementation. I will have a look at this (right after I give you your performance benchmarks :).","Comment:edsko:08/08/12 04:04:15 PM:

I have made a lot of progress on this today but haven't quite finished yet. Hopefully I'll be able to push something tomorrow.","Comment:edsko:08/10/12 04:12:27 PM:

I have pushed (and released to Hackage) a new version of CH where static is a separate package (Control.Distributed.Static -- http://hackage.haskell.org/package/distributed-static-0.1.0.0). This package does not provide any Template Haskell support but the TH layer in Cloud Haskell is now so thin, and so little of it is not Cloud Haskell specific, that I'm not sure it's worth abstracting it further (in fact, C.D.Static is quite usable without TH at all). 
Please have a look at let me know what part of the TH would be useful in metapar (or elsewhere) and would be worthwhile moving from CH proper to C.D.Static.","Comment:rrnewton:08/12/12 03:50:44 PM:

Awesome, thanks.  We will switch over to this as soon as we get a chance
and let you know how it goes.

On Fri, Aug 10, 2012 at 12:12 PM, Edsko de Vries
<notifications@github.com>wrote:

> I have pushed (and released to Hackage) a new version of CH where static
> is a separate package (Control.Distributed.Static --
> http://hackage.haskell.org/package/distributed-static-0.1.0.0). This
> package does not provide any Template Haskell support but the TH layer in
> Cloud Haskell is now so thin, and so little of it is not Cloud Haskell
> specific, that I'm not sure it's worth abstracting it further (in fact,
> C.D.Static is quite usable without TH at all).
> Please have a look at let me know what part of the TH would be useful in
> metapar (or elsewhere) and would be worthwhile moving from CH proper to
> C.D.Static.
>
> —
> Reply to this email directly or view it on GitHub<https://github.com/haskell-distributed/distributed-process/issues/21_issuecomment-7649171>.
>
>","Comment:edsko:10/04/12 02:20:20 PM:

Ryan, did you have a chance to look at this?","Comment:hyperthunk:01/04/13 05:09:14 PM:

Moving to Misc/Admin as 

(a) the move has already taken place and
(b) we've not heard anything back for >= 3months

Ryan, are you happy for me to close or is there more to us here?","Comment:robstewart57:01/05/13 05:36:50 PM:

To address an earlier question raised by @rrnewton on this issue. The closure/static representation in HdpH was also cut away from the rest of HdpH back in July 2012. At the time this was done to improve the chances of a unified representation for meta-par, hdph and cloud-haskell. @edsko has made similar de-coupling efforts from the CH side. The motivated design of the closure representation is described in Section 3.2 of [the first hdph paper](http://www.macs.hw.ac.uk/~pm175/papers/Maier_Trinder_IFL2011_XT.pdf), in particular pay attention to the <em>dual closure representation</em> paragraph. The key implementation facts about static in HdpH is in the documentation throughout [HdpH.Closure.Static.hs](https://github.com/PatrickMaier/HdpH/blob/master/HdpH/Closure/Static.hs) , and the template haskell code for <code>mkClosure</code> and so on is in [HdpH.Closure.Internal.hs](https://github.com/PatrickMaier/HdpH/blob/master/HdpH/Closure/Internal.hs) . I realise that this comes months after the CloudHaskell 2.0 closure implementation. I also, however, see many benefits to having one one representation across all distributed haskells. This would for example enable CloudHaskell's task layer to benefit from load balancing in HdpH's network-based work-stealing scheduler.

It is worth adding that a considerable HdpH refactor will soon be pushed onto github, which will see adoption of the network-transport-tcp backend, and an upload of the library to hackage.","Comment:rrnewton:01/05/13 05:36:50 PM:

mentioned","Comment:edsko:01/05/13 05:36:50 PM:

mentioned","Comment:hyperthunk:01/06/13 12:39:00 PM:

Hi @robstewart57 - thanks for explaining a bit of the history behind this.

> a unified representation for meta-par, hdph and cloud-haskell

We should definitely aim for that if possible.

> I also, however, see many benefits to having one one representation across all distributed haskells.

Me too - lots and lots of them. I'd *really* like to be able to mix and match.

> It is worth adding that a considerable HdpH refactor will soon be pushed onto github, which will see adoption of the network-transport-tcp backend, and an upload of the library to hackage.

Great. After that happens, we should look at properly unifying these layers then. Ryan (or someone on his behalf) will need to let one of us know what's left to be done before all three libraries can share the same implementation. From @edsko's last comment, it sounds like the main thing that's left to be done is to possibly share *some* of the TH code, though I'm not clear on which bits would be useful - are we talking about all of `mkClosure` available across all three libraries?

I'm happy to look into this, but it's a bit further down the priority stack than some of our pressing bug fixes right now and I know little about TH so it might not get done until I've had time to chat with @edsko about it. Unless someone wants to either contribute a pull request or give me some pointed ideas about how to achieve it, I'd estimate we're looking at post 0.2.0. Sorry I realise that's not the answer everyone was hoping for, but I want to give a realistic picture of what I'm likely to get done in my spare time over the next couple of months. I will probably drop a note out onto the parallel-haskell mailing list asking if anyone wants to step into the gap and assist with this specific task.","Comment:robstewart57:01/06/13 12:39:00 PM:

mentioned","Comment:edsko:01/06/13 12:39:00 PM:

mentioned","Comment:edsko:01/07/13 06:30:48 PM:

Note that the dual closure representation cannot be implemented without using unsafePerformIO (or proper implementation of 'static'). That's the main reason I didn't adopt it yet.","Comment:hyperthunk:01/07/13 08:50:13 PM:

> Note that the dual closure representation cannot be implemented without using unsafePerformIO (or proper implementation of 'static'). That's the main reason I didn't adopt it yet.

Hmn, I'm inclined to hold off on adopting it for the same reason.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
22,distributed-process builds with ghc 7.7.20120806,distributed-process-simplelocalnet and distributed-process tests pass with ghc 7.7.20120806 and ghc 7.4.2 on gentoo amd64.,08/11/12 01:52:45 PM,08/11/12 06:02:27 PM,Task,,Closed,Fixed,markwright,,"Comment:edsko:08/11/12 06:02:20 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/86b34224c1e4a89f13088056a65cc45f8da1a162
","Comment:edsko:08/11/12 06:02:20 PM:

merged","Comment:edsko:08/11/12 06:02:20 PM:

closed","Comment:edsko:08/11/12 06:02:27 PM:

Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
23,TH flag support,"Added the TH flag (default: True), enabling the build for architectures that don't have support for TH.

Enhanced the ignore file.

Reordered the exports/imports in C.D.P.Closure module.
",08/14/12 01:11:07 PM,08/16/12 01:21:54 PM,Task,,Closed,Fixed,billdozr,,"Comment:edsko:08/16/12 01:21:54 PM:

Merged manually. Thanks.","Comment:edsko:08/16/12 01:21:54 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
24,"Backends for Platform LSF, Oracle Grid Engine","Hello,

I'm primarily interested in using Cloud Haskell for running computations on clusters that use job schedulers like Platform LSF and Oracle Grid Engine.  The typical pattern is that I submit an array of N jobs to the scheduler, and the scheduler decides which machines to run them on, and at what time.  As a user, some key features of this setup are:

1) Different processes in the job array are started at different times.  This is typically because there are other users on the cluster, and the scheduler uses priorities and queues to determine what should be run when.

2) The number of processes running at any given time is almost always less than N.  The simplest example of this is if I schedule 1000 jobs on a cluster with 100 machines, then obviously some of the jobs must be run in sequence.  A more common example for me is that the cluster is busy, and my jobs get interleaved with jobs from other users, reducing the effective number of available machines.

3) I have no control over which machines processes are started on.  There's also no way to know which machine a process will be started on before the process actually starts.

4) Individual processes may be killed or suspended at any time.  This is most common when they happen to be running on a machine for which another user has higher priority (enough to kick me off).

I'm wondering what would be involved in writing a Cloud Haskell backend for this type of environment.  I have written some ad-hoc programs to deal with this sort of thing before.  An example situation is that I have a function f which is very expensive to compute, and I would like to farm different calls to this function out to different machines.  The model I've used is:

- A single master which decides for which x's to compute f(x).
- A bunch of workers, each equipped to compute f(x).
- A process determines it's own master/slave status based on its index in the job array (an environment variable).  Job index 1 is the master, the rest are slaves.
- When a slave starts up, it uses the job scheduler to find the IP address of the master, and sends a ""ready"" message to the master.
- The master keeps a queue of available slaves which is updated whenever a ""ready"" message arrives, or whenever the result of a computation arrives.
- The master also keeps a list of running slaves and what computations they're performing.
- If a slave dies, it's discarded from the list of running slaves, and its computation is sent to the next available slave.

This is all to cope with the (somewhat frustrating) fact that the ""cloud"" is dynamic, and many of its properties are only known at runtime.  The number of available slaves can grow or shrink during the course of the computation.  From what I've read, it looks like Cloud Haskell prefers to assume that the size and topology of the cloud is static.  Is this necessary?  Any recommendations on writing a backend for the environment above?",08/17/12 07:10:24 PM,08/21/12 07:46:46 AM,Task,,Closed,Fixed,davidsd,,"Comment:edsko:08/20/12 09:21:41 AM:

Hi David,

The core Cloud Haskell infrastructure (`distributed-process`) does not provide any functionality for node discovery or node creation. You can spawn processes to, and monitor, nodes with arbitrary node IDs (NIDs). As such, `distributed-process` certainly makes no assumptions about the dynamics of the network topology.

The origin of those node IDs is the province of Cloud Haskell backends. So it is conceivable that some backends might assume a static network topology, whereas others might work with dynamic network topologies. At the moment there is only a single backend available, called `distributed-process-simplelocalnet` (and we are working on an Azure backend). This provides a `findPeers` function which uses a UDP broadcast to discover peer nodes -- hence, it too does not assume that the network is static. I am therefore a bit surprised that you got the impression Cloud Haskell assumes a network topology. If you can tell me what part of the documentation gave you this impression, I'd appreciate it so that I can fix it. 

A major reason for separating the Cloud Haskell core from the Cloud Haskell backend on one side and from the underlying network transport (as abstracted by `network-transport`) is precisely so that it would be possible to design backends for specific infrastructures such as the one you sketch above. 

So although Cloud Haskell as it stands doesn't provide specific functionality for the kind of things you enumerate, it won't get in your way either, and the architecture is expressly designed to support the design of such backends. I would suggest to have a look at the `distributed-process-simplelocalnet` backend. It is only small, and it might serve as a good starting point. If you have questions, feel free to ask me -- here, at edsko@well-typed.com or on _haskell.","Comment:davidsd:08/20/12 09:01:12 PM:

Hi Edsko,

Thanks for the response.  I think I was thrown off by the type signature

```haskell
startMaster :: Backend -> ([NodeId] -> Process ()) -> IO ()
```

where the `NodeId` list is determined once and for all just before starting the master process.  This, combined with my recollections from reading the original Cloud Haskell paper, made me think that the set of slaves could not be added to during the running of the program.

After reading the source for `distributed-process-simplelocalnet`, it's clear that this is not the case. `startMaster` is not right for my purposes, but it looks like the more general features of `distributed-process-simplelocalnet` are already pretty close to what I want.  In particular, the list of slaves can grow at any time as the discoveryDaemon receives new messages.  I guess I need to add some functionality for pruning the list of slaves when processes get killed or suspended.  I think I can figure this out.

Thanks,
David","Comment:edsko:08/21/12 07:46:45 AM:

I see. I have added the following comment to 'startMaster':

    -- If you start more slave nodes after having started the master node, you can
    -- discover them with later calls to 'findSlaves', but be aware that you will
    -- need to call 'redirectLogHere' to redirect their logs to the master node.  

Hopefully that will avoid confusion. I'm glad that you think `distributed-process-simplelocalnet` might be a good starting point; as I mentioned before, if you get stuck somewhere, feel free to ask me any questions you might have.","Comment:edsko:08/21/12 07:46:46 AM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
25,Issues with IPv4 vs IPv6 on OSX,"It seems that the current implementation doesnt like IPv4 when binding the server and it doesnt like IPv6 when connecting from a client. 

Using distributed-process v0.3.1 installed with cabal install.
 
Starting the server using IPv4 (DOESNT WORK):
--------------
Workstation :: ~DEV » runhaskell Server.hs 127.0.0.1 12493         
Bind to 127.0.0.1:12493
Server.hs: bind: unsupported operation (Can't assign requested address)
 
Starting the server using IPv6 (WORKS):
--------------
Workstation :: ~DEV » runhaskell Server.hs ::ffff:127.0.0.1 12493  
Bind to ::ffff:127.0.0.1:12493
Echo server started at ::ffff:127.0.0.1:12493:0

Running the client using IPv4 (WORKS):
--------------
Workstation :: ~DEV » runhaskell Client.hs ::ffff:127.0.0.1 11111 127.0.0.1:12493:0
ConnectionOpened 1024 ReliableOrdered 127.0.0.1:12493:0
Received 1024 [""Hello world""]
ConnectionClosed 1024

Running the client using IPv6 (DOESNT WORK):
--------------
Workstation :: ~DEV » runhaskell Client.hs ::ffff:127.0.0.1 11111 ::ffff:127.0.0.1:12493:0
Client.hs: Error connecting: TransportError ConnectFailed ""user error (Could not parse)""

",08/18/12 04:47:24 PM,08/20/12 10:53:46 AM,Task,,Closed,Fixed,rodlogic,,"Comment:edsko:08/20/12 09:36:55 AM:

I use IPv4 all the time on Mac OS X -- but I have found that it doesn't work from within ghci. Can you try compiling your example and running the executable directly, and see if that solves the problem?

The IPv6 issue is unrelated, I think -- I think the parser gets confused by the additional colons. I will look at that now.","Comment:edsko:08/20/12 10:53:46 AM:

I have just uploaded `network-transport-tcp-0.2.0.3` to Hackage. I hope that this will resolve the IPv6 issue. If not, please reopen this issue.","Comment:edsko:08/20/12 10:53:46 AM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26,Tutorial for the task layer?,"Is there a chance we see a tutorial on task/promise layer as implemented in the prototype?

Or is it job of another package?

I would like to implement a simple tasks on a master connected to many worker nodes, like:

  do promises <- mapM startComputation inputList
       results <- mapM redeemPromise promises

And I have no idea where to start...",08/23/12 08:43:22 AM,08/23/12 09:30:51 AM,Task,,Closed,Fixed,mgajda,,"Comment:edsko:08/23/12 09:30:51 AM:

There is no tutorial because there is no implementation :) There is no task layer (yet) for `distributed-process` -- it will indeed be the the job of a separate package. ","Comment:edsko:08/23/12 09:30:51 AM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
27,How to implement process groups (similar to Erlang's pg/pg2) in distributed-process?,"The underlying motivation is pub-sub and Erlang process groups seems like a good starting point. Is there support for process groups in distributed-process or are there plans to support it in the future? 
",09/18/12 12:52:08 PM,09/18/12 01:10:05 PM,Task,,Closed,Fixed,rodlogic,,"Comment:edsko:09/18/12 12:58:53 PM:

There is no explicit support for process groups in distributed-process. Some people have been talking about starting a Control.Distributed.Process.Platform package, but so far not much has happened yet. Process groups are unlikely to make it into the core Cloud Haskell libraries.","Comment:edsko:09/18/12 12:58:53 PM:

closed","Comment:rodlogic:09/18/12 01:04:34 PM:

Thanks for the answer. Is that discussion happening somewhere where I can follow/participate? And a follow up to that: is there a mailing list for distributed-process discussions in general (at least some of these questions would not pollute the issues database)?

","Comment:edsko:09/18/12 01:10:05 PM:

It was on Twitter, so I'm not sure there's much of a record :-/ I just did a quick search on twitter.com but no results came up. As regards a mailing list, feel free to posts questions to the parallel-haskell mailing list (https://groups.google.com/forum/?hl=en&fromgroups_!forum/parallel-haskell).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
28,Reduce message overhead,"Small CH messages have a large overhead. For instance, consider sending `(Nothing :: Maybe ProcessId)`. The binary encoding of `Nothing` is one byte, but the total message sent looks like

    <channel ID>                4 bytes
    <message length>            4 bytes
    <message type fingerprint>  16 bytes
    <payload>                   1 byte

For an overhead of 2400% :) Obvious ways to reduce the overhead are

* Use a single byte for the first 255 connection IDs
* Use a single byte for message length <= 255
* Use a single byte to index into some cache for commonly used fingerprints (since most applications will probably send only a handful of different types of messages across the network, this might be very effective)
",09/24/12 12:21:30 PM,01/04/13 04:40:45 PM,Task,Horizon,Open,Unresolved,edsko,Feature-Request distributed-process,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
29,Support message fragmentation,"Messages sent by Cloud Haskell processes are always sent whole, independent of their size. This means that when a process sends a very large message, it can hog the network connection (moreover, `Network.Socket.Bytestring` [can't handle large messages](https://github.com/haskell/network/issues/58)). 

See comment at https://github.com/haskell/network/issues/58#issuecomment-8929906 -- number of chunks passed to sendMany should be short (<= 16).",09/24/12 12:24:21 PM,01/04/13 04:09:27 PM,Task,Horizon,Open,Unresolved,edsko,Feature-Request distributed-process,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
30,Polymorphic expect,"With the standard Cloud Haskell primitives it is impossible to write processes such as a proxy; for instance, we cannot write something like

```Haskell
proxy :: Process ()
proxy = forever $ do
  msg <- expect
  send someOtherProcess msg
```

which forwards messages _of any type_. The most recent version of Cloud Haskell supports `matchAny`, with which the above process can be written as

```Haskell
proxy :: Process ()
proxy = forever $ do
  msg <- receiveWait [ matchAny return ]
  forward someOtherProcess msg
```

but we still cannot write something like

```Haskell
proxy :: Process ()
proxy = forever $ do
  (destination, msg) <- expect
  send destination msg
```

For this we would need an alternative `Binary` encoding (maybe even something like [protocol buffers](https://developers.google.com/protocol-buffers/)?) which would allow us to decode a message into a pair of messages without knowing the types of the pair components, i.e., something like

```Haskell
decodePair :: ByteString -> (ByteString, ByteString)
```",09/24/12 12:28:37 PM,01/29/13 10:55:41 AM,Task,distributed-process-0.5.0,Open,Unresolved,edsko,Feature-Request distributed-process In-Progress,"Comment:edsko:11/06/12 05:17:26 PM:

This functionality is also useful for implementing generic protocols such as Erlang's `gen_server`. For instance, we can implement a semi-typed `gen_server` as in https://gist.github.com/4025934. However, provided that we add functionality 

    wrap :: Serializable a => a -> AbstractMessage
    matchAbstractMessage :: AbstractMessage -> [Match b] -> Process (Maybe b)

and make `AbstractMessage` serializable (different from forwarding!) we could change that `GenServer` example as follows:

    data Reply :: * where
      Reply :: Serializable a => a -> Reply
    
    data Server = Server {
        init       :: Process () 
      , handleCall :: [Match Reply] 
      }
    
    start :: (Serializable request, Serializable response) 
          => Name -> Process Server -> Process ()
    start name createServer = do
      server <- createServer
      pid <- spawnLocal $ do
        init server
        forever $ do
          (them, request) <- expect
          mReply <- matchAbstractMessage request (handleCall server)
          case mReply of
            Nothing            -> undefined -- deal with error
            Just (Reply reply) -> send them x 
      register name pid
    
    call :: (Serializable a, Serializable b) => Name -> a -> Process b
    call name request = do
      us <- getSelfPid
      nsend name (us, wrap request)
      expect

(Although for the specific case of `GenServer` I'm not 100% sure this this an improvement over the semi-typed version. But it illustrates the ideas.)","Comment:edsko:11/06/12 05:25:41 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
","Comment:edsko:11/18/12 10:05:02 AM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
","Comment:edsko:11/18/12 10:07:16 AM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
","Comment:edsko:11/18/12 10:31:37 AM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
","Comment:hyperthunk:11/18/12 12:27:33 PM:

I do think would represent an improvement, even though it weakens the guarantees the type system is able to make. The reason I see this as valuable is that library and infrastructure code often cares little about the content of data, but does need to deal with routing, concurrency and error handling, delegating the 'functional' aspects of an application (or integration scenario) to so other code that *does* care about the specific types involved. So having the ability to interact with `AbstractMessage` at this level is useful in some cases (where it simplifies the code required to deal with non-functional concerns) and the more specific types available via the higher level API (where `Serializable` data is coerced into its actual runtime type based on `Typeable` information) is better for *application authors* who're simply trying to leverage CH to build cool stuff.

I may attempt to contribute a pull request for this at some point in the future, though I have no idea when. :) ","Comment:dcoutts:11/22/12 10:30:46 AM:

So I think this looks fine. We just need to think of sensible names for these things. Perhaps:

    data Message
    wrapMessage :: Serializable a => a -> Message
    matchMessage :: Message -> [Match a] -> Process (Maybe a)
","Comment:edsko:11/22/12 10:36:21 AM:

Note that the `Message` type already exists, it is the internal wrapper that contains a `FingerPrint` and a `ByteString`. It is precisely the thing that `AbstractMessage` provides a layer over; initially I didn't want to expose the `Message` type directly because I thought it would be confusing if we regarded `Message` as itself `Serializable`, because now it the difference between `send`ing an `Message and `forward`ing it might be confusing. But if we add support for serializability anyway we can just export the internal type.","Comment:dcoutts:11/22/12 11:03:23 AM:

I'm not particularly advocing that we expose an internal type, just trying to think of a sensible name for the public API.","Comment:edsko:11/22/12 11:05:55 AM:

No, I understand, but there is no reason _not_ to export `Message` (the type at least, not the constructor).","Comment:hyperthunk:01/14/13 03:29:25 PM:

I haven't gone the whole hog with implementing this, however I'd like to draw your attention to [this branch](https://github.com/haskell-distributed/distributed-process/compare/semi-typed-recv) which I just pushed. I am **not** going to merge this without having taken some guidance from you guys first, so I'd appreciate it if you could take a peek and see what you think.

Whilst implementing generic server processes, I noticed that sometimes I want to dequeue a message from the mailbox, but not handle it myself. This is typical of generic servers where the process mailbox is managed by the implementation, and the actual handling of messages is supplied by the user as a callback of sorts.

For `GenProcess` it is not necessary to expose `Message` as `Serializable` or otherwise, but we *do* need a way to provide a *handle_info* equivalent, and I achieved this by providing a `maybeHandleMessage` operation on `AbstractMessage` which (providing the *if* condition matches) defers type checking until the handler attempts to run and returns a `Maybe` so you can determine whether or not the handler was applied. This *always dequeue* behaviour makes it possible to implement handle_info really neatly.

The key thing about this code is that I *do* want to dequeue the message, but it don't necessarily know if I can handle it. That latter part is up to the user callbacks, so I take the `AbstractMessage` and *see* if the user can handle it or not. Because I have an array of handlers, I'm sort-of duplicating the *loop through available matches* concept here, but I have variable policies in place to decide what to do with un-matched messages. Unlike vanilla cloud haskell, the gen-server needs to decide what to do with these...

```haskell
-- | Policy for handling unexpected messages, i.e., messages which are not
-- sent using the 'call' or 'cast' APIs, and which are not handled by any of the
-- 'handleInfo' handlers.
data UnhandledMessagePolicy =
    Terminate
  | DeadLetter ProcessId
  | Drop
  | ReQueue

data InfoDispatcher s = InfoDispatcher {
    dispatchInfo :: s -> AbstractMessage -> Process (Maybe (ProcessAction s))
  }
```

Each info handler is a pretty simple function, with the wrapper simply deferring to `maybeHandleMessage`

```haskell
handleInfo :: forall s a. (Serializable a)
           => (s -> a -> Process (ProcessAction s))
           -> InfoDispatcher s
handleInfo h = InfoDispatcher { dispatchInfo = doHandleInfo h }
  where 
    doHandleInfo :: forall s2 a2. (Serializable a2)
                             => (s2 -> a2 -> Process (ProcessAction s2))
                             -> s2
                             -> AbstractMessage
                             -> Process (Maybe (ProcessAction s2))
    doHandleInfo h' s msg = maybeHandleMessage msg (h' s)
```

The real meat of the work is in the process implementation, which walks all the info handlers and makes a policy based decision if it gets to the end without having handled the message. This is work in progress, but I hope it gives a good idea of how `maybeHandleMessage` can be used:

```haskell
-- Process Implementation

applyPolicy :: s
            -> UnhandledMessagePolicy
            -> AbstractMessage
            -> Process (ProcessAction s)
applyPolicy s p m =
  case p of
    Terminate      -> stop (TerminateOther ""unexpected-input"")
    DeadLetter pid -> forward m pid >> continue s
    Drop           -> continue s

initLoop :: Behaviour s -> s -> Delay -> Process TerminateReason
initLoop b s w =
  let p   = unhandledMessagePolicy b
      t   = timeoutHandler b 
      ms  = map (matchMessage p s) (dispatchers b)
      ms' = ms ++ addInfoAux p s (infoHandlers b)
  in loop ms' t s w
  where
    addInfoAux :: UnhandledMessagePolicy
               -> s
               -> [InfoDispatcher s]
               -> [Match (ProcessAction s)]
    addInfoAux p ps ds = [matchAny (infoHandler p ps ds)] 
        
    infoHandler :: UnhandledMessagePolicy
                -> s
                -> [InfoDispatcher s]
                -> AbstractMessage
                -> Process (ProcessAction s)
    infoHandler pol st [] msg = applyPolicy st pol msg
    infoHandler pol st (d:ds :: [InfoDispatcher s]) msg
        | length ds > 0  = let dh = dispatchInfo d in do 
            -- NB: we *do not* want to terminate/dead-letter messages until
            -- we've exhausted all the possible info handlers
            m <- dh st msg
            case m of
              Nothing  -> infoHandler pol st ds msg
              Just act -> return act
          -- but here we *do* let the policy kick in
        | otherwise = let dh = dispatchInfo d in do
            m <- dh st msg
            case m of
              Nothing -> applyPolicy st pol msg
              Just act -> return act 
```

The generic process implementation looks quite neat with this in place:

```haskell
data Reset = Reset 
    deriving (Typeable)
$(derive makeBinary ''Reset) 

type MyState = [String]

demo :: Behaviour MyState
demo = Behaviour {
     dispatchers = [
         handleCall add
       , handleCast reset
       ]
   , infoHandlers = [handleInfo handleMonitorSignal]
   , timeoutHandler = onTimeout
   , terminateHandler = undefined
   , unhandledMessagePolicy = Drop 
   }

add :: MyState -> String -> Process (ProcessReply MyState String)
add s x =
  let s' = (x:s)
  in reply ""ok"" s'

reset :: MyState -> Reset -> Process (ProcessAction MyState)
reset _ Reset = continue []

handleMonitorSignal :: MyState -> ProcessMonitorNotification -> Process (ProcessAction MyState)
handleMonitorSignal s (ProcessMonitorNotification _ _ _) = continue s

onTimeout :: TimeoutHandler MyState
onTimeout _ _ = stop $ TerminateOther ""timeout""
```
","Comment:hyperthunk:01/15/13 01:46:31 AM:

And the canonical use-case of course, is in my supervisor implementation: https://github.com/haskell-distributed/distributed-process-platform/blob/supervisor/src/Control/Distributed/Process/Platform/Supervisor.hs_L162

I'm wondering whether the API should be more like what you suggested above though. I guess the main difference is that I'm not so interested in *sending* the messages on elsewhere. Even if the `Message` type is exported, without the `messageFingerprint` being available there's no way to decode it and I actually need to decode the `Message` in the callback routines, but **not** in the code that calls `receive` - that code *doesn't care what the decoded type will be* and I think this is possibly a better solution to handling input messages in a generic fashion. If you're going to decode, *someone somewhere* needs to know the type.","Comment:hyperthunk:01/16/13 01:10:53 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
","Comment:hyperthunk:01/16/13 09:59:13 PM:

assigned","Comment:hyperthunk:01/17/13 11:27:17 AM:

> I'm wondering whether the API should be more like what you suggested above though.

I'm not convinced that issue DPLEGACY-71 really was a duplicate. *This* issue talks about being able to use expect and receive primitives to obtain a `Message` (or some other type that contains a `Message` in it) and then forward that `Message` which is a useful feature. Issue DPLEGACY-71 however is about layering of code that does not care about message types with code that does. Exporting `Message` and allowing for `matchMessage` and/or `wrapMessage` would not solve the latter case AFAICT.  ","Comment:hyperthunk:01/17/13 12:08:03 PM:

> I'm not convinced that issue DPLEGACY-71 really was a duplicate.

To that end, I've re-opened it and submitted a patch (see pull request DPLEGACY-116) to resolve it. This issue remains open as implementing a *proxy* in the way @edsko describes above would still require more work (i.e., the approach suggested here). ","Comment:edsko:01/17/13 12:08:03 PM:

mentioned","Comment:hyperthunk:01/27/13 09:02:33 PM:

I've decided to still go ahead with what @edsko proposed here *as well* - I need the ability to pull messages out of the mailbox without processing them immediately, and this fits in nicely with that requirement.

One thing that *does* frustrate here is that once we expose `Message` like this, `AbstractMessage` will quickly become pointless. I don't however, want to wait for the next major version bump to release this, as I really need it for the `ManagedProcess` (i.e., gen-server) API in d-p-platform. I think I'll just *deprecate* `AbstractMessage` instead.","Comment:edsko:01/27/13 09:02:33 PM:

mentioned","Comment:hyperthunk:01/28/13 12:28:22 AM:

> I think I'll just deprecate AbstractMessage instead.

That's precisely what I've needed to do, which according to http://www.haskell.org/haskellwiki/Package_versioning_policy means this requires a major version bump. Re-assigning to 0.5.0, again.","Comment:hyperthunk:01/28/13 12:58:38 AM:

Implementation is park in https://github.com/haskell-distributed/distributed-process/compare/expose-message. We can't merge this until 0.5.0 because it removes `AbstractMessage`.","Comment:hyperthunk:01/28/13 01:02:25 AM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
","Comment:edsko:01/29/13 08:55:01 AM:

I am not sure what, if anything, still needs to be decided here. Can you give me a three line summary of the current approach?","Comment:hyperthunk:01/29/13 10:03:35 AM:

Sure.

Make `Message` serializable, export the type (not the constructors) and provide an API to converting to/from `Message` and other serializable types. Make `matchAny` work with `Message` instead of `AbstractMessage` and continue to provide a `forward` operation on `Message`.","Comment:edsko:01/29/13 10:07:36 AM:

Right, sounds like a plan :)","Comment:hyperthunk:01/29/13 10:55:41 AM:

> Right, sounds like a plan :)

Awesome - this will get merged into 0.5.0 then! :D",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
31,Network.Transport.TCP may reject incoming connection request ,"Suppose A and B are connected, but the connection breaks. When A realizes this immediately and sends a new (heavyweight) connection request to B, then it /might/ happen that B has not yet realized that the current connection has broken and will therefore reject the incoming request from A as invalid.

This is low priority because 

* the window of opportunity for the problem to occur is small, especially because in the case of a true network failure it will take some time before a new connection can be established
* even if the problem _does_ arise, A can simply try to connect again (A might have to that anyway, to find out if the network problem has been resolved).",09/24/12 12:35:00 PM,01/04/13 04:08:37 PM,Task,Horizon,Open,Unresolved,edsko,network-transport-tcp Bug,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
32,CH or NT must implement keep-alive,"When one CH process A monitors another B, it expects to be notified if the connection between them breaks, _even_ when A never sends anything to B (but only receives messages from B). This means that it is not enough to rely on `send` to detect network problems. This can be solved at the CH level or at the NT level.",09/24/12 12:38:23 PM,01/04/13 04:08:28 PM,Task,Horizon,Open,Unresolved,edsko,distributed-process Bug network-transport,"Comment:edsko:11/18/12 11:40:56 AM:

http://nitoprograms.blogspot.co.uk/2009/05/detection-of-half-open-dropped.html might provide some useful insights.","Comment:hyperthunk:12/13/12 08:49:03 PM:

See http://rabbitmq.1065348.n5.nabble.com/Re-The-rabbitmq-server-stop-command-hangs-td23180.html for an example of this happening in another network protocol. I have some thoughts about this. Firstly, it can be solved at the OS level by tuning kernel params and/or switching on TCP keep-alive, but that only works for TCP.

Personally I think this should be handled at the NT level and that it should be configurable.","Comment:edsko:12/13/12 09:38:14 PM:

> Personally I think this should be handled at the NT level and that it should be configurable.

Agreed.","Comment:hyperthunk:12/14/12 12:28:30 AM:

We do this in RabbitMQ so I've some familiarity with the problem space. I'll take a look at submitting a patch, but things are rather busy at the moment so it might not materialise for a week or so.","Comment:hyperthunk:12/17/12 02:03:14 AM:

assigned","Comment:hyperthunk:12/17/12 02:03:48 AM:

> so it might not materialise for a week or so.

Nah, it's going to be quite a bit longer than that before I even start thinking about this one. Still interested in picking it up though => assigning to myself unless someone else wants to come and steal it first. :)","Comment:hyperthunk:12/18/12 04:31:35 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
33,EC2 backend,,09/24/12 12:39:36 PM,01/04/13 04:08:13 PM,Task,Horizon,Open,Unresolved,edsko,Feature-Request distributed-process,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
34,Network backend with security measures,"Erlang uses cookies to authenticate nodes; we could do something similar or something much more sophisticated. This should be done in individual backends, as security might vary widely from one setup to another. Ideally, security is handled entirely within the backend (within the `network-transport`?) so that the core Cloud Haskell library is unaffected.",09/24/12 12:42:19 PM,01/04/13 04:08:08 PM,Task,Horizon,Open,Unresolved,edsko,Feature-Request distributed-process,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
35,Network.Transport.Chan is incomplete,"In particular, various `closeXYZ` operations are not implemented.",09/24/12 12:43:36 PM,01/28/13 10:50:51 AM,Task,network-transport-inmemory-0.3.0.1,Open,Unresolved,edsko,Bug network-transport-inmemory,"Comment:edsko:01/03/13 04:40:17 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
","Comment:hyperthunk:01/09/13 05:13:37 PM:

assigned",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
36,Automatically GC connections,"Consider a process such as 

```Haskell
pingServer :: Process ()
pingServer = forever $ do
  client <- expect
  send client ()
```

This server ""leaks"" connections. Since CH guarantees ordering, we cannot close ""time out"" connections for instance. The programmer can fix this manually: 


```Haskell
pingServer :: Process ()
pingServer = forever $ do
  client <- expect
  send client ()
  reconnect client
```

(perhaps the name `reconnect` is confusing?), but ideally we would take care of this at the CH level instead.",09/24/12 12:49:07 PM,11/07/12 12:58:14 PM,Task,,Closed,Fixed,edsko,Feature-Request distributed-process,"Comment:edsko:11/07/12 12:58:14 PM:

Duplicate of DPLEGACY-64.","Comment:edsko:11/07/12 12:58:15 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
37,Support `closeConnectionTo`,"which closes the entire ""bundle"" of (outgoing _and_ incoming) connections to another endpoint. Basically, ""disconnect completely from this other endpoint"" (a ""heavyweight disconnect""). 

Once this is implemented we can resolve a TODO in `Control.Distributed.Process.Node`. ",09/24/12 12:54:55 PM,01/04/13 04:07:37 PM,Task,Horizon,Open,Unresolved,edsko,Feature-Request network-transport,"Comment:netogallo:12/16/12 09:24:41 PM:

Hi, I am trying to get involved in the development of CH and thought about starting with this issue. I just wanted to ask if I understand the issue at a technical level:

Each Node has an EndPoint (localEndPoint) and every time newLocalNode is called, a new endpoint is created for that node so there is one node for every endpoint (even though it is theoretically possible to have multiple nodes for a single endpoint using the createBareLocalNode function). So the issue consists of implementing a function closeConnectionsTo which given a node it should call the close function on all connections in the LocalNodeState? I know nodes and endpoints are different, but I don't find any place where endpoints and connections opened with the endpoint can be associated except the node to which the endpoint belongs. If some light could be shed here would be nice.

Thank you","Comment:edsko:12/17/12 08:29:14 AM:

No, `closeConnectionTo` is a purely `Network.Transport` level construct. You'd be working with `Network.Transport` and `Network.Transport.TCP`. You'd also have to think very carefully about how this construct would work in potentially other network transports. This one isn't particularly easy.","Comment:hyperthunk:12/18/12 11:34:55 AM:

So @edsko it sounds like what we're asking for is a means of tracking all the connections associated with a specific `Endpoint` and having a kind of bulk-close. And that needs to work at the transport level.

I think the abstraction isn't too hard to get right. If you move the mechanism into the policy layer (i.e., `Network.Transport` API) and basically provide the data structures and manipulation logic there, then the utility layer (e.g., `Network.Transport.Foo` or whatever) can simply call into these.

Better still, move the API calls into the policy layer itself and parameterise them by the implementation itself. Say for example, that we have some type class (I know, I'm obsessed with these right!) that we can work with. Somewhere in `Network.Transport` we might see this kind of code...

```haskell
type CreateTransportResult = Either (TransportError NewEndPointErrorCode) EndPoint

data TransportInternals a = TransportInternals a

class TransportProvider a where    
  createNew :: a -> TransportConfiguration -> IO CreateTransportResult
  getInternals  :: a -> Transport -> TransportInternals a

createTransport :: (TransportProvider a) => a -> TransportConfiguration -> IO CreateTransportResult
createTransport provider = createNew provider config

```

Now you can do something similar (with or without the type classes) so that the implementations have to provide some sort of `EndPointProvider` as well. The **key** to this is to make sure that the functions which deal with an endpoint are defined in the policy layer, where we can handle tracking them properly. Quite where we choose to maintain this state is debatable. We could use shared memory or go some other way, e.g.,

```haskell
data EndPoint = EndPoint {
    provider :: EndPointProvider   -- provides the callback functions
  , address :: EndPointAddress
  , connections :: MVar (StrictList Connection)
    --- etc
  }

connectEndpoint :: EndPoint -> Reliability -> ConnectHints -> IO (Either (TransportError ConnectErrorCode) Connection)
connectEndpoint ep r h = do
  addr <- address ep
  impl <- provider ep
  -- let the backend do the real world
  conn <- backendConnect impl addr r h
  case conn of
      Left _ -> return conn
      Right cn -> appendSTM (connections ep) cn >> return conn

disconnectEndpoint :: EndPoint -> IO ()   --- probably want some return value but hey...
disconnectEndpoint ep = do
  conns <- connections ep
  foldrSTM conns closeConnection   -- we can collect the outcomes here...
  writeSTM conns newEmptyStrictList  
```

Anyway, that's just a sketch, but hopefully you see my point. By moving some of the mechanism into `Network.Transport` we can handle these kinds of things in that layer, and leave the particulars of mechanism and utility (where you're dealing with specific implementation details) to the backends.

So gentlemen - how does that sound? Certainly it's a bigger refactoring than just implementing `closeConnectionTo` but does it seem like the right approach in principle?

And if so, how do you feel about picking this up and making a start @netogallo? We'll step in and help as needed of course, as and when we're able to.

","Comment:edsko:12/18/12 11:34:55 AM:

mentioned","Comment:netogallo:12/18/12 11:34:55 AM:

mentioned","Comment:edsko:12/18/12 11:43:31 AM:

No, I don't think that's the right way to go. You need to add `closeConnectionTo` to `Network.Transport` alright, but nothing else. None of the above data structures should go there, because different transport implementations might implemented this in entirely different ways. For instance, `Network.Transport.TCP` goes to great lengths to make sure there is at most a single TCP connection between any two endpoints. So, in that case, `closeConnectionTo` is simply a matter of closing that socket, and dealing with the fallout.

`Network.Transport` is tiny -- it is just the API specification really, and it should probably stay that way. 

You are welcome to try and modify `Network.Transport.TCP` but be aware that it is a very complicated piece of code -- at least, by my standards. Probably the hardest bit of code I've ever written. I don't think `closeConnectionTo` should be *too* difficult to add, but the issues are subtle so I can't say for sure.","Comment:edsko:12/18/12 11:50:03 AM:

As regards getting the abstraction right -- you will need to talk to people involved with other kinds of transports. Is it realistic to demand something like `closeConnectionTo` in a UDP transport? (Probably.) What about Infiniband (CCI)? (Less sure there.) ","Comment:hyperthunk:12/18/12 11:51:37 AM:

> None of the above data structures should go there, because different transport implementations might implemented this in entirely different ways. For instance, Network.Transport.TCP goes to great lengths to make sure there is at most > a single TCP connection between any two endpoints

Hmn, good point, I hadn't thought about that. Clearly it's much better for `Network.Transport.TCP` to just close the socket, so folding over the lightweight connections and closing them is pointless.

> I don't think closeConnectionTo should be too difficult to add, but the issues are subtle so I can't say for sure.

Well if your comment about just closing the socket is right then it doesn't sound too onerous. Where would you expect the API to live? On `EndPoint` like so?

```haskell
data EndPoint = EndPoint {
  receive :: IO Event
  --- snip
  disconnectEndpoint :: IO ()
```

What is the different between `closeConnectionTo` and `closeEndPoint :: IO ()` that already exists? Is it because basically the other providers *might* want to tear their connections down without closing the endpoint itself? What's the use-case for that? The *TODO* in `Node.hs` says that you're completely disconnecting from the EndPoint because of the error, if I'm reading it right.

It does sound like for TCP that `closeConnectionTo == closeEndPoint` anyway, perhaps with some added error/exception handling. ","Comment:hyperthunk:12/18/12 11:53:25 AM:

> As regards getting the abstraction right -- you will need to talk to people involved with other kinds of transports. Is it realistic to demand something like closeConnectionTo in a UDP transport? (Probably.) What about Infiniband (CCI)? (Less sure there.) 

Yes indeed.

> What's the use-case for that?

Just realised - you're closing the connections to the **other endpoint** but not closing the local end! ","Comment:edsko:12/18/12 11:58:32 AM:

> Just realised - you're closing the connections to the other endpoint but not closing the local end!

Yes, this is not `closeEndpoint`. The `Network.Transport` API has this concept of a 'bundle' of connections between two endpoints. `closeConnectionTo` closes such a bundle in one go, without closing the endpoints themselves or connections they might have to other endpoints. 

Note that that ""in one go"" is in serious need of expansion. How synchronous is this operation? What about data currently in the network? Etc. Etc. These are not easy questions to answer.","Comment:netogallo:12/18/12 12:12:45 PM:

Implementing such function does seem to require careful consideration since users will be expecting somewhat homogeneous behavior in all transports which might be tricky. I will probably return to this issue once I have better understanding about transports, especially since I am not a networks guy. Thanks for the valuable feedback, I believe this discussion will be very useful to tackle the problem, but for the moment I will review CH a little more to figure out where my skills can be most useful. Thank you for the feedback.","Comment:hyperthunk:12/18/12 01:41:04 PM:

@edsko - we deal with these kinds of problems every day at RabbitMQ and they are indeed very tricky to get right. In fact, I'd say I spend > 30% of my engineering efforts hardening the broker against situations that arise because of unreliable networks, and that's just considering TCP/IP.

The problem with making this operation synchronous is that it can block for an arbitrary period of time, depending on the semantics of the underlying network/transport/protocol/etc. That's not good news when you're trying to shut down, upgrade, restart or whatever. One solution is timeouts, but then you're never sure just how long they should be and of course this should be a matter of policy. For connected protocols like TCP, using heartbeats is usually the right solution, with configurable parameters to control the frequency. Even for disconnected protocols this is usually a good idea.

Making this operation asynchronous is fine, in theory, but there would need to be some kind of completion callback or result on which the caller can block if required.","Comment:edsko:12/18/12 01:41:04 PM:

mentioned","Comment:edsko:12/18/12 01:45:23 PM:

Yes, agreed on all the above. My main point is: don't rush into this one :)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
38,More automated testing,"Write a script-driven `Network.Transport` implementation with which we can test for specific network failures, and then use QuickCheck to generate scripts (similar to how `ByteString` is tested).",09/24/12 12:59:49 PM,01/04/13 04:07:20 PM,Task,Horizon,Open,Unresolved,edsko,Feature-Request distributed-process network-transport In-Progress,"Comment:edsko:11/06/12 02:42:48 PM:

We started to do this for Network.Transport.TCP, and it's quite aways there, but not quite done yet.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
39,Check for ByteString memory leaks,"Since ByteString's can point to (larger) bytestrings, it is easy to create ""bytestring memory leaks"". We have already fixed some of these, but there may be others.",09/24/12 01:11:37 PM,10/05/12 08:47:07 AM,Task,,Closed,Fixed,edsko,distributed-process network-transport,"Comment:edsko:10/05/12 08:47:07 AM:

We store bytestrings primarily in two locations: As PIDs and as undecoded messages. We now copy the bytestring in both those places, and moreover make sure that this copy happens eagerly. In addition, messages put into type channels are now decoded eagerly, so we don't retain bytestrings there at all. 

This doesn't mean this issue is definitely completely fixed, but I'll close it until further evidence appears that there are additional leaks.","Comment:edsko:10/05/12 08:47:07 AM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
40,rank1dynamic not ghc 7.6 compatible (requires base 4.5.*),"if I  change the cabal constraints to 
  Build-Depends:       base <4.7 && > 4.4,
                       ghc-prim >= 0.2 && < 0.4,
                       binary >= 0.5 && < 0.6 

it seems to build fine, and then I can cabal install distributed-process without any compile time problems 
(i'll start playing around and testing if it works as desired on ghc 7.6 shortly :) )",09/24/12 08:53:32 PM,09/27/12 03:04:33 PM,Task,,Closed,Fixed,cartazio,,"Comment:edsko:09/27/12 03:04:33 PM:

I'm confused. ghc 7.6 ships with base 4.6 (which was already allowed in the .cabal file). Base 4.4 is ghc 7.2; you are right that base 4.4 was not supported. I have now uploaded version 0.1.0.1 which does compile with ghc 7.2/base 4.4 (as well as ghc 7.6, but it already did).","Comment:edsko:09/27/12 03:04:33 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
41,functionSDict and functionTDict should use FQN,"The static label used for mkStatic is ModuleName.function_name, but the ModuleName is not used for the source and target dictionaries.",10/02/12 08:49:12 AM,10/05/12 08:44:47 AM,Task,,Closed,Fixed,edsko,distributed-process Bug,"Comment:edsko:10/05/12 08:44:47 AM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
42,Asynchronous exception bug in spawn,"The following code in `Network.Transport.Util` is unsafe with regard to asynchronous exceptions. If an asynchronous exception is thrown to the forked thread before the endpoint address is put to the `addr` MVar then the final `takeMVar` will dead-lock:

```haskell
spawn :: Transport -> (EndPoint -> IO ()) -> IO EndPointAddress
spawn transport proc = doo
  addr <- newEmptyMVar
  forkIO $ do
    Right endpoint <- newEndPoint transport
    putMVar addr (address endpoint)
    proc endpoint
  takeMVar addr
```

One way to solve this is using a combination of `mask` and `try`. However a way more simple implementation is:

```haskell
spawn :: Transport -> (EndPoint -> IO ()) -> IO EndPointAddress
spawn transport proc = do
  Right endpoint <- newEndPoint transport
  forkIO $ proc endpoint
  return $ address endpoint
```

Since the original code has to wait for the completion for `newEndPoint` anyway we could just as well execute it in the main thread and only execute `proc endpoint` in a separate thread. No need for an `MVar` so no posibility of dead-lock.

However since this code is so simple I wonder if there's actually a need for this combinator. Is it used anywhere? If not, I propose to remove it.
",10/05/12 03:54:16 PM,12/17/12 06:56:53 PM,Task,distributed-process-0.5.0,Open,Unresolved,basvandijk,Bug network-transport,"Comment:edsko:10/10/12 09:28:33 AM:

closed","Comment:edsko:10/10/12 09:29:56 AM:

Asynchronous exceptions are not an issue here because the thread ID of the new thread is not exposed and hence nobody can throw an asynchronous exception to that thread. Nevertheless, you are right of course that we were ignoring the possibility of error in newEndPoint. That's now fixed.

(You are also right that we could potentially removed this completely, but that would require a major version increment. Meh.)","Comment:basvandijk:10/11/12 09:35:21 PM:

> Asynchronous exceptions are not an issue here because the thread ID of the new thread is not exposed and hence nobody can throw an asynchronous exception to that thread. 

But the RTS can still throw [asynchronous exceptions](http://hackage.haskell.org/packages/archive/base/latest/doc/html/Control-Exception.html_t:AsyncException) to the new thread.

> Nevertheless, you are right of course that we were ignoring the possibility of error in newEndPoint. That's now fixed.

But why execute the `newEndPoint` inside the new thread and go to the trouble of creating an MVar and waiting for it when you can just execute it in the main thread which is simpler, safer and probably a bit faster to?","Comment:edsko:10/11/12 09:48:45 PM:

Yeah, I'm not sure either, but I probably had a (possibly wrong) reason at the time :) You're probably right that it can be simplified, but I couldn't remember why I had done it this way in the first place so I was a bit hesitant to change it. 

And if your heap is overflowing inside spawn you have bigger problems :-) Point taken though.","Comment:edsko:10/12/12 09:53:26 AM:

It was something to do with not wanting to do certain operations on the main thread, because the main thread is by default a bound thread.. Even so it should probably be okay because newEndPoint will spawn its own threads to do most of the work.. ","Comment:edsko:10/23/12 03:27:40 PM:

reopened",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
43,Improve efficiency of message matching,"In particular, storing messages by type could be very beneficial.",10/15/12 10:20:36 AM,01/07/13 08:49:21 PM,Task,Horizon,Open,Unresolved,edsko,Feature-Request distributed-process,"Comment:hyperthunk:01/07/13 07:12:18 PM:

So would we store the accepted mailbox as Map Fingerprint (StrictList a) or some such? Is Map the right data structure for this or donee want a strict version?","Comment:edsko:01/07/13 07:17:10 PM:

You have to be very careful here: you still need to guarantee ordering. If a process does a receiveWait for type A *or* type B then we need to make sure that if a message of either type exists they are delivered in the order that they were received.","Comment:hyperthunk:01/07/13 08:49:21 PM:

> You have to be very careful here: you still need to guarantee ordering.

Yes of course, I was jumping ahead without thinking there. I suspect then, some kind of custom data structure that maintains the priority property but supports searching/inserting by key. Ralf Hinze et al present a Priority Search Queue which IIRC was used (or adapted) in the GHC IO manager, so perhaps we can take some inspiration from there. The primary difference is that we want to select by a range of keys and then order by priority.","Comment:hyperthunk:01/08/13 12:45:26 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
44,Better warning if binary is not installed on VM,,10/16/12 09:44:17 AM,01/04/13 04:04:48 PM,Task,Horizon,Open,Unresolved,edsko,Bug distributed-process-azure,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
45,Make all distributed-process-demos work on Azure,,10/16/12 09:44:44 AM,01/04/13 04:04:44 PM,Task,Horizon,Open,Unresolved,edsko,Feature-Request distributed-process-azure,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
46,add OTP style framework of standard *behaviours*,"It'd be great to have a go at implementing OTP style behaviours (a la gen_server, supervisor, etc) now that distribute-process is stable (looking). I'm quite happy to contribute, but there are no contribution guidelines so I'm not sure if I should fork and add a distributed-process-framework sub package to the repository or do this as a separate project or what. Any pointers would be much appreciated.",10/18/12 12:28:21 PM,10/19/12 05:57:32 PM,Task,,Closed,Fixed,hyperthunk,,"Comment:edsko:10/18/12 12:30:37 PM:

Yes, various people have talked about this. It's outside the scope of the Cloud Haskell project proper, so I would suggest you start a separate Github repository for it and invite people on Twitter to contribute to it (I'll be happy to retweet, a number of people have asked about OTP on twitter). I suggest 'distributed-process-platform' for the name of the package. ","Comment:edsko:10/18/12 12:32:18 PM:

I'm happy enough to answer questions (by email, or on the parallel-haskell mailing list) but cannot contribute myself.","Comment:hyperthunk:10/18/12 01:00:07 PM:

Ok great - I'll start a new repo and come ask questions on the mailing list (which I didn't know existed!) as and when I need to. Thanks!","Comment:hyperthunk:10/18/12 01:00:07 PM:

closed","Comment:cartazio:10/19/12 03:26:42 PM:

This site page for documenting interest right?","Comment:edsko:10/19/12 03:31:08 PM:

For now.","Comment:edsko:10/19/12 03:31:10 PM:

reopened","Comment:dysinger:10/19/12 04:42:51 PM:

<- interest","Comment:KirinDave:10/19/12 04:54:39 PM:

Likewise, interested. OTP isn't so much a requirement as a natural evolution of having distributed processes and process trees and wanting to manage them sanely.","Comment:hyperthunk:10/19/12 05:18:28 PM:

FWIW as a professional erlang programmer, I suspect many of the otp concepts are I'll fitting to cloud Haskell. For example, gen_server is probably far less useful than a set o abstraction for working with typed or untyped channels in a consistent way (request/reply channels, higher order/delegated channels and the like). The supervisor concept is definitely worth porting however, and the application concept maybe have a place too.

I will post back when I get around to setting up a repo an for those who live in/around London, I'll be attending the next LHUG meeting later this month.","Comment:hyperthunk:10/19/12 05:18:28 PM:

closed","Comment:hyperthunk:10/19/12 05:41:15 PM:

Guys - please add ideas/feature requests to the issue tracker at hyperthunk/distributed-process-platform. I am not as *hot* a Haskell hacker as I am an Erlang/ML one, so of course suggestions and contributions will be most welcome. In particular I'm keen to get early feedback on the model for describing type constraints within the various generic  moving parts, staring with supervision trees. And feel free to spread the word on the mailing list do we can have a wider discussion. I'll post something tonight to kick that off.","Comment:edsko:10/19/12 05:57:32 PM:

Great, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
47,Timeout version for typed channels read,,10/19/12 04:40:41 PM,11/06/12 10:01:19 AM,Task,,Closed,Fixed,edsko,Feature-Request distributed-process,"Comment:wiz:11/05/12 08:33:28 AM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
","Comment:wiz:11/05/12 08:50:51 AM:

Perhaps an injection of System.Timeout.timeout could do the trick?

    receiveChanTimeout :: Serializable a => Int -> ReceivePort a -> Process (Maybe a)
    receiveChanTimeout n = liftIO . timeout n . atomically . receiveSTM
","Comment:edsko:11/05/12 09:54:55 AM:

That implementation would be fine, if it wasn't for the slightly strange requirement that if a timeout of 0 is specified the function should do a non-blocking check for messages; that check is allowed to take time, but is not allowed to block. An implementation that does support that behaviour is rather more tricky.","Comment:wiz:11/05/12 10:25:13 AM:

So, the problem is absence of tryTakeMVar-like function in STM?

Maybe a version with such requirement lifted can be provided before a nonblocking TVar-peeking function could be implemented.

Alternanatively, a minimal delay value can be imposed with a value in a ballpark of some ""optimistic"" STM section run time. There could be a problem with slow systems unable to retrieve their messages, but one can use a larger custom value instead of default pseudo-zero.","Comment:edsko:11/05/12 05:04:32 PM:

Actually, I think this should be straightforward, now that I think about it a little more. Let me have a look at this today or tomorrow. There is a bunch of functionality I want to add to typed channels anyway.","Comment:edsko:11/06/12 10:01:19 AM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
48,Functor instance for ReceivePort ,,10/19/12 04:41:03 PM,11/06/12 11:59:54 AM,Task,,Closed,Fixed,edsko,Feature-Request distributed-process,"Comment:edsko:10/19/12 04:45:26 PM:

And point to the functor instance in the documentation for `mergePortsXXX`.","Comment:edsko:11/06/12 11:59:54 AM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
49,Applicative instance for ReceivePort,,10/19/12 04:52:48 PM,11/06/12 11:59:54 AM,Task,,Closed,Fixed,edsko,Feature-Request distributed-process,"Comment:edsko:11/06/12 11:59:54 AM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
50,Alternative instance for ReceivePort,,10/19/12 04:53:00 PM,11/06/12 11:59:54 AM,Task,,Closed,Fixed,edsko,Feature-Request distributed-process,"Comment:edsko:11/06/12 11:59:54 AM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
51,Monad instance for ReceivePort,,10/19/12 05:01:15 PM,11/06/12 11:59:54 AM,Task,,Closed,Fixed,edsko,Feature-Request distributed-process,"Comment:edsko:11/06/12 11:59:54 AM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
52,The SimpleLocalnet backend should take (RemoteTable -> RemoteTable) rather than RemoteTable,This avoids the need for Control.Distributed.Process.Node in applications that use it.,10/22/12 03:12:04 PM,01/28/13 10:47:57 AM,Task,distributed-process-simplelocalnet-0.2.1,Open,Unresolved,edsko,Bug distributed-process-simplelocalnet,"Comment:hyperthunk:01/28/13 10:47:53 AM:

assigned",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
53,ntohl is not available on Windows,,10/23/12 08:00:46 AM,01/04/13 04:04:24 PM,Task,Horizon,Open,Unresolved,edsko,network-transport-tcp Bug,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
54,multicast does not work on Windows," ""bind: failed (Cannot assign requested address (WSAEADDRNOTAVAIL))",10/23/12 08:30:57 AM,10/23/12 08:30:57 AM,Task,,Open,Unresolved,edsko,Bug distributed-process-simplelocalnet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
55,unmonitor should remove monitor messages from the mailbox (if any),,10/23/12 12:23:33 PM,12/17/12 06:56:53 PM,Task,distributed-process-0.5.0,Open,Unresolved,edsko,Feature-Request distributed-process,"Comment:hyperthunk:12/13/12 08:53:24 PM:

Personally I think this should be an *option* rather than the default behaviour.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
56,"Add ""cookie"" or other identification mechanism to SimpleLocalnet",so that we have multiple independent Cloud Haskell applications running on the same network.,10/23/12 01:59:26 PM,01/04/13 04:02:42 PM,Task,Horizon,Open,Unresolved,edsko,Feature-Request distributed-process-simplelocalnet,"Comment:edsko:10/23/12 03:11:52 PM:

(Possibly it might suffice to make the multicast port configurable.)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
57,Re-export runProcess and forkProcess in SimpleLocalnet,so that startSlave and startMaster are truly optional.,10/23/12 02:51:52 PM,01/04/13 04:02:29 PM,Task,distributed-process-0.5.0,Open,Unresolved,edsko,Feature-Request distributed-process-simplelocalnet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
58,"Update ""binary"" to 0.6","Is it possible to make `distributed-process` depend on `binary-0.6` rather than 0.5 ?

I wanted to use Cloud Haskell with this new database library (which requires binary-0.6)
http://hackage.haskell.org/package/CurryDB
",10/30/12 01:01:13 PM,11/07/12 11:25:50 AM,Task,,Closed,Fixed,nushio3,,"Comment:nushio3:10/31/12 05:22:59 AM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
","Comment:edsko:11/05/12 09:16:44 AM:

closed","Comment:edsko:11/05/12 09:16:53 AM:

Thanks for the patch.","Comment:edsko:11/05/12 09:23:12 AM:

Changed it so that the lower bound is still 0.5, rather than _insisting_ on 0.6 (https://github.com/haskell-distributed/distributed-process/commit/f89a686a8f4e89db9f1a954667d346b9fa72d03b). Let me know if that's a problem.","Comment:nushio3:11/07/12 11:25:50 AM:

It worked for me. Thank you very much!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
59,"Update ""binary"" to 0.6",Fix issue #58,10/31/12 05:22:59 AM,11/05/12 09:16:14 AM,Task,,Closed,Fixed,nushio3,,"Comment:nushio3:10/31/12 05:29:45 AM:

Just updating the version constraint https://github.com/nushio3/distributed-process/commit/0f030915bb6ff26bb74d9c63d6a2ac8342bf726e worked for me (tested using cabal-dev.) Please check if there are any side effects.","Comment:edsko:11/05/12 09:16:14 AM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/b81f8aceeb276aa1f402d982bdf4e8b32b842d67
","Comment:edsko:11/05/12 09:16:14 AM:

merged","Comment:edsko:11/05/12 09:16:14 AM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
60,Meta package to install all the other packages?,"I had some issues where cabal had installed more than one version of these packages and apps were compiled using a mix of versions. Wondering if having a meta package (no code) that includes the other ones will help managing this.

I had to do something like this to get out of the cabal mess:
```sh
ghc-pkg list | grep -e distributed -e network-transport | xargs -t -I pkg ghc-pkg unregister pkg --force
```
And then reinstall the packages.

Maybe the following is already a good way?

```shell
cabal install distributed-process-simplelocalnet
```",11/02/12 08:33:27 PM,01/28/13 02:19:08 PM,Task,distributed-process-0.5.0,Open,Unresolved,rodlogic,Feature-Request,"Comment:hyperthunk:01/28/13 02:18:52 PM:

assigned",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
61,Missing receiveChanTimeout,"There should be a something like receiveTimeout/expectTimeout for channels.

    receiveChanTimeout :: Serializable a => Int -> ReceivePort a -> Process (Maybe a)
",11/05/12 08:32:19 AM,11/05/12 08:35:30 AM,Task,,Closed,Fixed,wiz,,"Comment:wiz:11/05/12 08:33:28 AM:

oops.. a dupe of DPLEGACY-47.","Comment:wiz:11/05/12 08:33:28 AM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
62,"Messages from the ""main channel"" as a receivePort","At the moment it is not possible to wait for a message from the main channel or from a typed channel. It might be useful to add a function

    expectChan :: Serializable a => Process (ReceivePort a)

which creates a `ReceivePort` for messages of a specific type sent to the main channel. This `ReceivePort` can then be merged with other `ReceivePort`s as usual.",11/06/12 12:01:36 PM,12/14/12 02:21:05 PM,Task,,Closed,Fixed,edsko,Feature-Request distributed-process,"Comment:hyperthunk:11/10/12 03:19:59 PM:

+1 - this would be very useful for distributed-process-platform e.g., GenServer could make good use of this.","Comment:simonmar:12/13/12 12:13:18 PM:

I can't see how to implement this, however I have implemented an alternative that achieves the desired effect; see 847abf494233523dba7d0b40628c3af9e870be91

Let me know what you think, and maybe we can merge it in.","Comment:hyperthunk:12/14/12 02:18:56 AM:

I'm reading through this to understand it at the moment. Forgive me for talking out loud here - I'm relatively green and it seems to help. So this definition is pretty neat:

```haskell
 waitIncoming = fmap Left (readTChan incoming) `orElse` fmap Right readchans
```

Am I right in reading this as meaning that the implementation of `readTChan` must call `retry` if the channel is empty? 
I see the non-blocking read in `CQueue` composes this with another STM action that just returns `Nothing` so we can get out if all of the received queue, the main input channel **and** whatever is given for `readchans` evaluates to, are all empty or contain no messages that satisfy the list of matches given.

Then in `Primatives.receiveWait` where we want to block indefinitely (until we get a message) you pass `retry` as `readchans` and therefore we restart the transaction again. For the xChans variants in `Primatives` you're basically passing `matchChans` instead of `retry` which uses `receiveSTM` on the receive port, and this presumably works in a similar vein.

Have I grokked that properly? It seems like a reasonable approach from what I've understood so far. 

I can't see at all how to create the alternative `expectChan` API either. In fact, from what I've seen I think that would be far too invasive a change. You'd surely need to change the node controller's inbound message handling to do that, because from what I can see at the moment, channels are implemented using a different data structure, which is accessed and written to by `handleIncomingMessages` when the destination of the incoming message is addressable by `SendPortIdentifier` rather than `ProcessIdentifier`.

So even if there is some way of getting at that process state from outside the node controller, I can't see how to make that work - how would you know that a message addressed to the `ProcessIdentifier` needed to be *re-routed* as it were?

","Comment:simonmar:12/14/12 08:16:39 AM:

Yes, you're understanding it properly. To implement expectChan, the main process inbox would need to be an STM-only data structure, so that we could compose it with the other channel reads.  However, doing this would make it very difficult to implement efficiently, because there would be no way to avoid re-reading the whole message queue each time a new message came in.

I talked this over with @edsko and @dcoutts on IRC yesterday, and we think that it's possible to implement the other design alternative:

```haskell
matchChan :: ReceivePort a -> (a -> Process b) -> Match b
```

Which is a slightly nicer API, but a bit more difficult to implement.  I'm looking into it now.
","Comment:edsko:12/14/12 08:16:39 AM:

mentioned","Comment:dcoutts:12/14/12 08:16:39 AM:

mentioned","Comment:edsko:12/14/12 08:27:49 AM:

> So even if there is some way of getting at that process state from outside the node controller, I can't see how to make that work - how would you know that a message addressed to the ProcessIdentifier needed to be re-routed as it were?

Rerouting would not be necessary -- you would simply inspect the mailbox when somebody tries to read from the `ReceivePort`, which is just a wrapper around an STM transaction. The difficulty with this approach is that we need to have an `STM` transaction to extract a message from the mailbox. The mailbox is already an `STM` channel but at the moment I try hard to keep the `STM` transactions used in `match` and co very short. In order to implement `expectChan` we'd have to change it so that the entire operation to extract a mailbox message would have to run inside STM. This would lead to larger STM transactions when reading from the mailbox, which may be interrupted by writers. This would be bad for performance. There may be a clever way to do it, but it needs more thought.

@simonmar's `matchChan` solution is very similar to `expectChan` in that it gives you a way to look at the mailbox as well as a channels, just like `expectChan` does, but it has the advantage that we can group all `match`es that look at the mailbox, scan the mailbox as usual, if that fails, more to the next group of `match`es that look at channels, and so on. In the worst case, if you alternate mailbox and channel matches, this may still have pretty bad performance, but with a bit of care from the person writing the `match`es it should work.","Comment:simonmar:12/14/12 08:27:49 AM:

mentioned","Comment:hyperthunk:12/14/12 12:37:38 PM:

@simon @edsko thanks for the explanations, I can see why a course grained transaction would suck. What irc channel(s) are worth hanging out on btw - is there one for parallel Haskell in particular?","Comment:simon:12/14/12 12:37:38 PM:

mentioned","Comment:edsko:12/14/12 12:37:38 PM:

mentioned","Comment:edsko:12/14/12 12:47:45 PM:

Support for `matchChan` has now been added -- many thanks to @simonmar.","Comment:simonmar:12/14/12 12:47:45 PM:

mentioned","Comment:edsko:12/14/12 12:47:45 PM:

closed","Comment:hyperthunk:12/14/12 02:21:05 PM:

Thanks to both of you for the quick turn-around! As @rodlogic mentioned elsewhere, this will be enormously useful in distributed-process-platform. Now all I have to do is read through it and understand how it works... :)","Comment:rodlogic:12/14/12 02:21:05 PM:

mentioned",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
63,runProcess should propagate exception,,11/06/12 01:44:12 PM,12/17/12 06:56:53 PM,Task,distributed-process-0.5.0,Open,Unresolved,edsko,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
64,Enable GC of connections without losing ordering constraints,"When process A sends a message to process B, we must open a connection from A to B and, currently,  *keep that connection open* for the duration of A's lifetime in order to maintain ordering guarantees. If A sends messages to lots of different processes (think of a server responding to clients) this will result in a space leak.

What we need is a way to garbage collect connections when they are no longer used, but still maintain ordering guarantees. There are (at least) two ways in which we might implement this:

* *Client side*. When we close the connection, we wait for an acknowledgement from the other side that the connection has been closed. Once we have received this acknowledgement we know that all messages we sent have been received and hence it's safe to open a new connection. If the sender starts sending more messages before receiving the acknowledgement this messages must be buffered. 

  We could implement this at the Cloud Haskell level, and have the *node controller* send the acknowledgement. This introduces the question ""what about connections to the node controller itself"". We already have implicit reconnect to and from node controllers, implying potential message loss, but what we *don't* want is reordering of messages to node controllers. 

  We could instead implement it at the Network.Transport level and introduce a new `OutgoingConnectionClosed` event, but there is a technical difficulty here too: outgoing connections don't have identifiers on the sender side, only on the receiver side, so we somehow need to be able identify to the sender *which* connection got closed, without making `connect` synchronous. (The sender allocates part of the receiver-side ID, but only half of it.) One easy solution is to introduce

      closeWithAck :: Connection -> Int64 -> IO ()

  so that we allow the sender to provide a token which will be returned in the `OutgoingConnectionClosed` event.

* *Server side*. When we close the connection, we remember the connection ID (but see discussion above: we don't know connection IDs sender side -- similar solutions could be proposed here). Then when we open a new connection we first send a message saying ""all messages you receive on this connection must be delivered after you have got all messages on connection *X*""; this implies server side buffering.

  Of course, this by itself doesn't gain us much because now we still have to maintain state: the connection ID of the last connection to every process we ever sent a message to. So the question becomes when can we forget this state? This seems to require some sort of acknowledgement from the server, though.

Client side seems the easier way to solve this problem.",11/07/12 09:56:20 AM,11/07/12 12:50:36 PM,Task,,Open,Unresolved,edsko,Feature-Request distributed-process network-transport,"Comment:edsko:11/07/12 09:59:58 AM:

A related issue is *when* do we garbage collect connections. One option is to clear them periodically after some timeout period. Another possibility is to collect them when process A has no further references process B (using weak references somehow). The main difficulty there is that ProcessId at the moment is a stateless object, and *must be* as long as we have pure decoding (ProcessIds are serializable, after all), unless we resort to `unsafePerformIO` trickery.","Comment:edsko:11/07/12 12:50:36 PM:

Note that there is a manual workaround for people for whom this is an important issue: you can manually cleanup connections by using `reconnect`. This is described in more detail [in a recent blog post](http://www.well-typed.com/blog/72).","Comment:edsko:11/07/12 12:58:14 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
65,Tracing support,"Erlang has tracing built in to the runtime system, which is very lightweight and has little runtime performance impact on the traced process. Traces can be set up to match processes (all, [pids...], named/registered, etc) and flags turned on to trace calls to specific modules/functions/etc. Traces are sent to one or more tracer processes, and these typically either throw the trace data straight on to a socket (to reduce impact on the traced system) or print to a file descriptor.

I'm not sure how much of this makes sense for Cloud Haskell, but it would be good to see if we can come up with some corollary mechanism that allows us to trace processes simply and efficiently. I don't think the typical traceEvent style would be useful here, but if the message queue for a process could be transparently used to forward messages to an additional tracer process (or process group) then that would be useful! ",11/07/12 11:18:53 AM,01/04/13 04:00:26 PM,Task,Horizon,Open,Unresolved,hyperthunk,Feature-Request distributed-process,"Comment:edsko:11/07/12 12:56:31 PM:

Yes, debugging support is definitely something that would be worthwhile to add. It would also be useful to add metrics such as message queue size (possibly which types).","Comment:gbaz:11/07/12 04:16:49 PM:

Erlang uses lamport clocks internally to give an ordering on the traces. As far as I can tell, the trick is that every message contains a (Maybe (Clock,Destination)) and the primitives will manage/pass that along if it exists. It would be relatively simple to extend the process internal state with an optional lamport clock, and expose it directly. There's of course a small overhead even when the clock isn't in use, but I imagine it could be useful in a number of circumstances. My suggestion would be to have the state contain both a Maybe Clock and a [Trace Destinations] so that we decouple the ordering functionality given by the clock which is useful even without a trace from the trace functionality which is a bit useful even without a clock.","Comment:edsko:11/07/12 04:24:30 PM:

I think we should separate out concerns about distributed ordering (like Lamport clocks), which can be implemented on top of the core infrastructure, with hooks into the guts of the system that allow to extract the relevant information. I'm not convinced that the core libraries need to do the former, but obviously they do need to do the latter.","Comment:gbaz:11/07/12 04:27:17 PM:

Sure -- my concern is just that traces are less useful if you don't have some ordering on causality. A trace mechanism that let userland lamport clocks be hooked in (i.e. some customizable action to generate the traces) would indeed probably be cleaner and more elegant.","Comment:edsko:11/07/12 04:28:19 PM:

This causality is also the main difficulty in implementing generic distributed logging. Perhaps that's the core concept that should be implemented (as a separate package, `distributed-process-logging` perhaps).","Comment:hyperthunk:11/07/12 04:30:05 PM:

Erlang supports both kinds of tracing. The one @gbaz mentioned based on lamport clocks is http://www.erlang.org/doc/man/seq_trace.html, whereas the dynamic process tracing I mentioned is a separate, complimentary feature built into the runtime. A vclock based tracing feature would be nice, but should be a separate package IMO.

For an example of simple tracing facilities, see http://www.erlang.org/doc/man/dbg.html.","Comment:gbaz:11/07/12 04:30:05 PM:

mentioned","Comment:hyperthunk:12/14/12 02:59:19 AM:

> It would also be useful to add metrics such as message queue size (possibly which types).

Yes that would be nice. I'm going to split it out into a separate issue however, as it seems distinct from tracing/debugging.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
66,SimpleLocalnet should remove peers that no longer exist,"This is not a *major* issue, because even if we do, there is still no guarantee that a peer still exists after it has been returned by `findPeers`. ",11/07/12 03:33:22 PM,01/04/13 04:00:22 PM,Task,Horizon,Open,Unresolved,edsko,Feature-Request distributed-process-simplelocalnet,"Comment:hyperthunk:12/18/12 04:31:35 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
","Comment:hyperthunk:01/04/13 04:00:22 PM:

Setting the target release as ""some time in the future"" as this ""is not a major issue"". Feel free to push it back into 0.2.0 if you want though.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
67,Introduce convention for process registry,"Some of the backends may register processes internally (like the logger process in SimpleLocalnet), and we may even register some standard processes in the core libraries too. The registered names for these processes should not clash with names that the user defines.",11/07/12 03:51:10 PM,01/04/13 03:59:22 PM,Task,distributed-process-0.5.0,Open,Unresolved,edsko,Feature-Request distributed-process,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
68,provide local versions of spawnLink and spawnMonitor,"Linking and monitoring *after* a process has started can lead to races, and atomic versions of these are very useful. Providing local versions of these would be nice. ",11/10/12 03:24:28 PM,12/03/12 03:19:10 PM,Task,,Closed,Fixed,hyperthunk,Feature-Request distributed-process,"Comment:edsko:11/18/12 10:13:51 AM:

Actually, `spawnLink` and co are very simple at the moment. Since `link` and `monitor` are asynchronous anyway, it's not clear that there is much to do. The asynchronous nature of these primitives is different from their implementation in Erlang, and follows the ""Unified Semantics for Future Erlang"" paper. If you think the current implementation is wrong, do let me know though.","Comment:hyperthunk:11/22/12 04:02:52 PM:

Thanks @edsko let me spend some more time understanding what the paper has to say and we'll see.","Comment:edsko:11/22/12 04:02:53 PM:

mentioned","Comment:hyperthunk:12/03/12 03:19:10 PM:

Having read the paper, I agree there's not much to do here. Closing.","Comment:hyperthunk:12/03/12 03:19:10 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
69,killing processes,"Erlang provides exit(Pid) to terminate a process, and an exit(Pid, Reason :: any()) overload which allows the 'to be exited' process to grok **why** it is being stopped, which turns out to be very useful.

Erlang allows processes to trap exit signals by setting a *process flag* - this allows the exit signals sent via links to propagate without terminating the process which initiated the link. Whilst this sounds conceptually similar to monitors, there is a difference. If we are writing, for example, an OTP style supervisor, we need a way to tell our child (supervised) processes to exit but we do **not** always want them to just crash because an asynchronous exception was thrown to them using `throwTo` or whatever.

In Erlang, the 'trapping exit signals' feature allows a process to intercept exit signals and decide to do a 'clean shutdown' with the exception of `{'EXIT', _Pid, kill}` which is *un-trappable* and kills the process immediately.

So, I have a couple of questions, such as how do I tell a process to exit? terminate :: Process () appears to be a call they make themselves and is `throwTo` really the way to do this?

What I really **don't** want to do is have to force the processes I'm supervising (or interacting with generally) to deal with some specific shutdown protocol, as any process that doesn't isn't *safe* to supervise in the general case and this limitation seems artificial.",11/10/12 03:58:58 PM,12/05/12 01:51:01 PM,Task,,Closed,Fixed,hyperthunk,Feature-Request distributed-process,"Comment:edsko:11/18/12 10:11:31 AM:

Yes, this is currently missing from the implementation. I think what would make most sense is to implement a primitive

```Haskell
exit :: ProcessId -> DiedReason -> Process ()
```

and have the node controller raise an exception when it receives the corresponding message. The ""Unified Semantics for Future Erlang"" paper describes this, but it has not been implemented.","Comment:rodlogic:11/28/12 01:02:55 PM:

Based on the above comments and a quick search on the subject, we have basically two major scenarios to cover: 
* exit - 'give me a reason to exit and I'll see what I can do'.
* kill - 'sorry, I am killing you now and the reason is none of your business!'

Another point that seems important is that the exit reason must be typed and application defined as opposed to a closed enum or a weak/informational String. Instead of adding another data type to wrap an exit or a kill and based on the fact that they have slightly different semantics, maybe these could be two separate primitives:

```haskell
-- | Kill a process
kill :: ProcessId -> Process ()

-- | Exit a process given a reason
exit :: Serializable a => ProcessId -> a -> Process ()
```
The implementation will send a control message to the target node using two new data constructors added to ProcessSignal:
```haskell
-- | Signals to the node controller (see 'NCMsg')
data ProcessSignal =
   ....
  | Kill !ProcessId
  | Exit !ProcessId !Message
  deriving Show
```
And the node controller will handle these with two functions:
```haskell
-- |
ncEffectKill :: Identifier -> ProcessId -> NC ()
ncEffectKill from pid = undefined

-- |
ncEffectExit :: Identifier -> ProcessId -> Message -> NC ()
ncEffectExit from pid msg = undefined
```

Here is a partial commit of the above for your input: [exit/kill commit](https://github.com/rodlogic/distributed-process/commit/4d83bf9051cc6afb963fd7c34d4f6e33ee12303c)

What I am not entirely sure about is the back-end implementation. How should we deliver these two actions to the receiving process? This question is part understanding better how this is done in Erlang and part how that could be done in CH (throwing exceptions? Or sending a message instead of a control message?)

","Comment:hyperthunk:11/28/12 03:17:44 PM:

This looks pretty good - how is the `Serializable` reason going to be translated to the `DiedReason` constructor `DiedException !String` though? As exception will only carry a string back, and we **do** need to detect `DiedReason` in things like links and monitors, then I think you'd be better off just using `DiedReason` *or* perhaps making the object you use and instance of `show` as well? ","Comment:rodlogic:11/28/12 03:57:49 PM:

It seems that this translation from Serializable a to a String could be done by the process being asked to exit, after all this process is supposed to deny or approve the exit request anyway (different than killing).

Here is a simple scenario:
* Process A calls **exit pidB WorkerShouldNotHaveMigrated**
* The exit reason **WorkerShouldNotHaveMigrated** is handled by Process B somehow (receive? channel? catch?) 
* Process B handles the exit reason and makes a decision:
  1. Ignore the exit reason - function returns and nothing happens
  2. Accept the exit reason by terminating the process giving a String reason: ""Yes, I shouldn't have done that!""

Here is a slight modification to the above proposal (won't compile but conveys the intent):

```haskell
-- | Kill another process
kill :: ProcessId -> String -> Process ()
kill them reason = undefined

-- | Kill this process
kill :: String -> Process ()
kill reason = undefined

-- | Exit a process given a reason
exit :: Serializable a => ProcessId -> a -> Process ()
exit them reason = undefined
```

There are two variations of **kill** here: one to kill another process and one to commit suicide. Both take in a String reason that maps directly to a String reason in a DiedReason data constructor. 

The process handling an exit reason **exit pidB WorkerShouldNotHaveMigrated** would then call **kill ""Yes, I shouldn't have done that!""**, if that is what it wishes to do. The 'suicidal' kill would then make sure that links/monitors are properly notified.

Re: the GenServer design, it could have a std dispatcher for an exit request that would call back the application module and, in the appropriate case, make sure that the terminateHandler is called before calling the 'suicidal' kill.

What about delivering exits? Should those be regular messages to the process? Or should both kill and exit be catched as exceptions?
","Comment:rodlogic:11/28/12 04:20:35 PM:

Alternatively, could we change **terminate** to accept a String reason? This would keep **exit** / **kill** conceptually separate from **terminate** and committing suicide is already it's purpose:

```haskell
terminate :: String -> Process a
terminate reason = liftIO $ throwIO (ProcessTerminationException reason)
```
and

```haskell
-- | Kill another process
kill :: ProcessId -> String -> Process ()
kill them reason = undefined

-- | Exit a process given a reason
exit :: Serializable a => ProcessId -> a -> Process ()
exit them reason = undefined
```","Comment:hyperthunk:11/28/12 06:42:14 PM:

> It seems that this translation from Serializable a to a String could be done by the process being asked to exit, after all > this process is supposed to deny or approve the exit request anyway (different than killing).

Actually I'm fairly sure that is not what is wanted. This catching of exit signals (i.e., exceptions) and choosing what to do with them, whilst it is very erlang-ish, has been advised against by @edsko in [distributed-process-platform issue 7](https://github.com/hyperthunk/distributed-process-platform/issues/7):

> > trapping exceptions is fraught with difficulties: if you find yourself in a position where you want to trap an exit signal > > (or, in Haskell parlor, catch an exception) you should consider using monitoring instead.

So I think we should get some clarification from @haskell-distributed about whether we want exits to be trap-able or not. Personally I use it all the time in erlang, but there the semantics are quite different.

If we choose not to allow trapping exceptions, then we can just implement `exit` as per @edsko's original suggestion:

```haskell
exit :: ProcessId -> DiedReason -> Process ()

kill :: ProcessId -> Process ()
kill them = exit them ""killed""
```

Although I agree that matching on a string (taken out of an exception) feels a bit unstructured, the point about this is that it is the monitor who actually *cares* about the exit reason, not the process being killed (assuming we do not allow for trapping exits). And in that case, no amount of cleverness with custom exit reason types will help, as the termination **must** be implemented using `throwTo` afaict and that raises an exception (which results in `DiedException !String` on the other end of the link/monitor) so we might as well stick with `DiedReason` anyway I think.

Now this (above) seems to hold even if we **do** decide to allow trapping exits, but in that latter case we'll also need to make changes to what the `forkProcess` implementation does when an exception signal is trapped (see https://github.com/haskell-distributed/distributed-process/blob/master/distributed-process/src/Control/Distributed/Process/Node.hs_L245) and I'm not convinced that this is the right thing to do. It really does look, as @edsko put it, a bit risky and at the very least it will complicate that code significantly.

**Another way of doing this** would be to change the implementation such that we store the exit reason (in an IORef or MVar) before throwing our `ProcessTerminationException`.","Comment:edsko:11/28/12 06:42:14 PM:

mentioned","Comment:haskell-distributed:11/28/12 06:42:14 PM:

mentioned","Comment:rodlogic:11/28/12 11:27:21 PM:

sorry for the long posts ...

    Actually I'm fairly sure that is not what is wanted. This catching of exit signals (i.e., exceptions) and choosing what to do with them, whilst it is very erlang-ish, has been advised against by @edsko 

The proposal above does not prescribe the use of exception/catch as a way to deliver exit signals: it actually leaves it open pending further discussions (you may well be seeing much farther than I had, though).


Now, the more important question, imo, is settling on actual requirements for this killing business. Here are a few assumptions the previous proposal embodies, but worth stepping back to discuss:
 1. Should the process being killed be able to perform some last house-keeping before dying? My understanding is yes.
 2. Should a process be able to 'ask' another process to kill itself given a proper reason? I thought so based on your initial comment on the issue, but maybe this is not the case? Or maybe we should consider this as a nice-to-have only at this point? 
 3. Should a process be able to kill another process without mercy? My understanding is that this is the core requirement here.
 4. Should a forced killing by other processes be protected somehow or every process can be a potential killer?

Based on your comment: 

      the monitor who actually cares about the exit reason, not the process being killed (assuming we *do not allow for trapping exits*)

Do you mean that req 2 is not a requirement if we don't allow trapping exists, i.e. exceptions/catch?




Now, considering this snippet:

```hassle
exit :: ProcessId -> DiedReason -> Process ()

kill :: ProcessId -> Process ()
kill them = exit them ""killed""
```

It may be a naming issue, but my initial confusion was that DiedReason was an 'event' telling monitors/links that a process has died (past) not an actual command or reason for killing a process. In addition, looking at DiedReason:

```haskell
-- | Why did a process die?
data DiedReason = 
    -- | Normal termination
    DiedNormal
    -- | The process exited with an exception
    -- (provided as 'String' because 'Exception' does not implement 'Binary')
  | DiedException !String
    -- | We got disconnected from the process node
  | DiedDisconnect
    -- | The process node died
  | DiedNodeDown
    -- | Invalid (process/node/channel) identifier 
  | DiedUnknownId
  deriving (Show, Eq)
```
... it doesnt seem to make sense to use it since it has constructors that would not make sense here, afaik. I would expect, instead, that a monitor would receive a ProcessMonitorNotification containing a DiedReason.



    as the termination must be implemented using throwTo afaict and that raises an exception (which results in DiedException !String on the other end of the link/monitor)

I was assuming that requirement 2 could be implemented by sending a regular message to the process's message queue instead of using throwTo. That would be a way to coordinate requirement 1 with the kill request/command and also fulfill req 2, if that is really needed here. However, there is one problem: if the process never consumes that message it would never die. The only generic way around I can think of here would be to associate a timeout with the message in the queue: if not consumed in a given timeframe the process would be forced to terminate with a throwTo. However, I understand that this is cutting deep into the existing CH semantics and most likely in the worse ways possible, though.



I also agree that we need to get some feedback and insights from @edsko and @haskell-distributed before we can realistically move on at this point.","Comment:edsko:11/28/12 11:27:21 PM:

mentioned","Comment:haskell-distributed:11/28/12 11:27:21 PM:

mentioned","Comment:edsko:11/29/12 09:26:47 AM:

A few comments guys:

1. _Trapping signals_. A ""signal"" in the Haskell context is an exception. The Haskell equivalent of ""trapping"" the exception is an exception handler (`catch`). So, the right way to deliver the exit ""signal"" is indeed `throwTo`, and if the process wants to be able to intercept these it will need to use `catch`. It might be useful to introduce a new exception type `ProcessKilled` so that process can look for those kinds of exceptions explicitly.

2. _Untrappable signals_. Since there is no such thing as an uncatchable exception in Haskell, this cannot be implemented. Even the Haskell concurrency primitive `killThread` sends a `ThreadKilled` exception to the thread, which it can intercept if it wishes. Hence, there should be only a single primitive (`exit`), not two (`exit` vs `kill`).

3. _Typed exit reasons_. This is a little harder to do. It's easy enough to implement `exit :: Serializable a => ProcessId -> a -> Process ()` but how is the receiving process going to decode the message? You could define 

        newtype ProcessKilled = ProcessKilled { killReason :: Message }

    where `Message` is the internal type that wraps a serialized message along with its type fingerprint. Then you could implement something like `catchKilled :: (a -> IO b) -> IO b -> IO b` or something along those lines, which would install an exception handler for `ProcessKilled` exceptions, see if the type of the of the `Message` in the `ProcessKilled` exception matches the type of the handler, and if so call the specified exception handler, and if not, rethrow. (In client code you could then compose multiple `catchKilled` messages if you wanted to catch kill-messages of different types.)

    I don't think you need to extend `DiedReason` at all, or indeed change monitoring at all. The infrastructure already detects when a process dies because of an exception (in this case, that would be a `ProcessKilled` exception), and already translates this into the appropriate `DiedReason` type, and already delivers this to monitoring or linking processes. I think all that stuff can they just as it is. The only tricky part is that the the `Show` instance for `ProcessKilled` won't show very much information (in particular, we cannot decode and show the `Message`).

    (Perhaps `ProcessKilled` should include the `ProcessId` of the process that did the killing?)

4. _Type of exit_. I agree that the type of `exit` I specified above is probably too low-level. Probably better is `exit :: ProcessId -> String -> Process ()` or alternatively `exit :: Serializable a => ProessId -> a -> Process ()` if you wanted to go down the route I suggest in (3) -- (which might be nice, actually, the more than I think about it).

5. _Danger of trapping exceptions_. You cite me above as warning against trapping exceptions. While you are correct that this can be difficult, I think for this use case it's fine (and certainly I don't think we should go down the Erlang route of having a stateful ""trapping signals"" flag). It's particularly tricky with `link` (which is where the quote is coming from) because programs such as

        link them
        expect
        unlink them

    don't work quite as expected. I think for the purposes of killing processes however it's much less of an issue, because you probably don't want to catch in such a fine-grained manner.

My proposal is:

1. Implement `exit :: Serializable a => ProcessId -> a -> Process ()`
2. Extend `ProcessSignal` with `Kill Message` 
3. Define `newtype ProcessKilled = ProcessKilled Message` and make it an exception type
4. Implement `catchKilled` as suggested above.","Comment:hyperthunk:11/29/12 11:07:40 AM:

> sorry for the long posts ...

Well for my part don't apologize - this is an insightful discussion and I'm glad we're having it.

> Now, the more important question, imo, is settling on actual requirements for this killing business.

Yes you're right of course.

> 1. Should the process being killed be able to perform some last house-keeping before dying? My understanding is yes.

Not according to the comment @edsko made which I referenced earlier! The problem is that Erlang/OTP doesn't provide this *as a primitive* either. Let's stop and consider what Erlang actually does provide:

```erlang
main(_) ->
    Pid = spawn(fun loop/0),
    note(""send ~p go_away~n"", [Pid]),
    exit(Pid, go_away),
    check(Pid),
    Pid2 = spawn(fun loop2/0),
    note(""send ~p go_away~n"", [Pid2]),
    exit(Pid2, go_away),
    check(Pid2),
    note(""send ~p kill~n"", [Pid2]),
    exit(Pid2, kill),
    check(Pid2),
    ok.

note(Fmt, Args) ->
    io:format(Fmt, Args),
    timer:sleep(10).

loop() ->
    receive
        X -> io:format(""~p received ~p~n"", [self(), X])
    end,
    loop().

loop2() ->
    process_flag(trap_exit, true),
    loop().

check(Pid) ->
    timer:sleep(10),
    io:format(""is ~p alive? ~p~n"",
             [Pid, erlang:is_process_alive(Pid)]).
```

When we run this, we see that only when the `trap_exit` process flag is set, can a process ignore exit signals. We also see that the `kill` reason overrides the process flags and **always** kills the process.

```
t4@iske:spikes $ escript demo 
send <0.31.0> go_away
is <0.31.0> alive? false
send <0.32.0> go_away
<0.32.0> received {'EXIT',<0.2.0>,go_away}
is <0.32.0> alive? true
send <0.32.0> kill
is <0.32.0> alive? false
t4@iske:spikes $ 
```

In light of this behaviour....

> 2.  Should a process be able to 'ask' another process to kill itself given a proper reason? I thought so based on your initial comment on the issue, but maybe this is not the case? Or maybe we should consider this as a nice-to-have only at this point?

Well I'm not sure it is just a *nice to have* but this is something that, as you've pointed out, we need to get a broad agreement on. Let's digress into Erlang/OTP again for a moment and consider why this behaviour is useful. Let's take a look at how a supervisor terminates its children. We will consider the two most commonly used shutdown strategies:

1. timeouts
2. brutal_kill

For a `brutal_kill` the implementation is very simple!

```erlang
terminate_children(Pids) ->
    [exit(Pid, kill) || Pid <- Pids].
```

Well that wasn't hard. So how do we implement shutdowns with a timeout? Let's assume a supervisor that isn't using the `simple_one_for_one` strategy and therefore the shutdown timeout is applied to each child. The child specification includes the timeout setting, so we can use monitors to confirm the child's death and an `after Timeout` clause to timeout after the specified delay. Based on the way `erlang:monitor/2` works, we perform the monitor operation first so that if the child dies (before the monitoring is set up) we get a `DOWN` message with reason 'noproc' - we should get the real reason in the `EXIT` message (unless a misbehaving child has already called `unlink` **and** we are completely reliant on trapping exits here!

```erlang
terminate(Child, Timeout) ->

    erlang:monitor(process, Pid),
    unlink(Pid),

    receive
	%% If the child dies before the unlik we must empty
	%% the mail-box of the 'EXIT'-message and the 'DOWN'-message.
	{'EXIT', Pid, Reason} -> 
	    receive 
		{'DOWN', _, process, Pid, _} ->
		    {error, Reason}
	    end
    after 0 -> 
        %% monitoring has been set up, so we can assume we will see
        %% 'DOWN' here!
        exit(Pid, shutdown), %% Try to shutdown gracefully
	    receive 
		{'DOWN', _MRef, process, Pid, shutdown} ->
		    ok;
		{'DOWN', _MRef, process, Pid, OtherReason} ->
		    {error, OtherReason}
	    after Timeout ->
		    exit(Pid, kill),  %% Force termination.
		    receive
			{'DOWN', _MRef, process, Pid, OtherReason} ->
			    {error, OtherReason}
		    end
	    end
    end.
```

Now I do **not** see how you can do this generically without allowing processes to trap `{'EXIT', _Pid, shutdown}` messages when someone calls `exit(Pid, shutdown)`. Of course you **could** implement some other protocol to handle *graceful exits* but I will explain later on why that is a bad idea. 

> 3. Should a process be able to kill another process without mercy? My understanding is that this is the core requirement here.

Yes that is absolutely vital regardless of whether we support trapping exits or not.

> 4. Should a forced killing by other processes be protected somehow or every process can be a potential killer?

I would say no. Having spent many years writing fairly complex systems using OTP, I'd suggest that the simpler the better is the right way to go, and this concept introduces a great deal of complexity into understanding how exit signals can be used.

> It may be a naming issue, but my initial confusion was that DiedReason was an 'event' telling monitors/links that a > process has died (past) not an actual command or reason for killing a process.

You're probably right about that - I was just going on the suggestion @edsko made about the type signature. I have no problem with creating a different type to represent exit reasons, though I'm not sure how general we should make this.

> it doesnt seem to make sense to use it since it has constructors that would not make sense here, afaik. I would expect, instead, that a monitor would receive a ProcessMonitorNotification containing a DiedReason. 

I think that when we talk about why exceptions are the **only** way to implement process exits sanely, then this might make more sense. But you're correct about the monitors receive `ProcessMonitorNotification` though.

> I was assuming that requirement 2 could be implemented by sending a regular message to the process's message > queue instead of using throwTo. That would be a way to coordinate requirement 1 with the kill request/command and also fulfill req 2, if that is really needed here. However, there is one problem: if the process never consumes that > message it would never die. 

Right, exactly! You've hit the nail on the head here. The problem with using the process' mailbox to handle 'EXIT' signals is that they can be queued up behind a ton of other stuff. Now this is *fine* when you're allowing the process to trap exits, as the whole point of that is the exit signal will not necessarily kill the process. But when you're using `exit(Pid, kill)` and/or the process is **not** trapping exits then you want the process to die asap.

> The only generic way around I can think of here would be to associate a timeout with the message in the queue: if > not consumed in a given timeframe the process would be forced to terminate with a throwTo. However, I understand that this is cutting deep into the existing CH semantics and most likely in the worse ways possible, though.

Yeah indeed. Whenever I see the words *timeout* and *semantics* in the same sentence, I get a cold shiver.

@edsko - so things have changed in your mind then?

> Trapping signals. A ""signal"" in the Haskell context is an exception. The Haskell equivalent of ""trapping"" the exception is an exception handler (catch). So, the right way to deliver the exit ""signal"" is indeed throwTo, and if the process wants to be able to intercept these it will need to use catch. It might be useful to introduce a new exception type ProcessKilled so that process can look for those kinds of exceptions explicitly.

Ok - this is quite interesting, it seems that you're less opposed to trapping exits than before.

> Untrappable signals. [snip] there should be only a single primitive (exit), not two (exit vs kill).

Well, I agree that the thread which implements the Process (in CH) should not have to worry about that, but I'm not convinced that doing away with 'kill' is a good idea. We need a way to **ensure** that we have killed a process regardless of what exception handling is installed, so....

Going back to your comment above:

> if the process wants to be able to intercept these it will need to use catch. It might be useful to introduce a new exception type ProcessKilled so that process can look for those kinds of exceptions explicitly.

I think this is a good idea, but I do not think user code should be involved in trapping exceptions at all! That feels *far* too low level to me. But I can't see how we could install an exception handler in the code ourselves, so I guess this is unavoidable?

> Typed exit reasons...

I could live with or without this. I think it's less important than getting the API for *catching* `ProcessKilled` exceptions right.

> Danger of trapping exceptions. You cite me above as warning against trapping exceptions. While you are correct that this can be difficult, I think for this use case it's fine (and certainly I don't think we should go down the Erlang route of having a stateful ""trapping signals"" flag). It's particularly tricky with link (which is where the quote is coming from) because programs such as
> 
> link them
> expect
> unlink them
> 
> don't work quite as expected. I think for the purposes of killing processes however it's much less of an issue, because you probably don't want to catch in such a fine-grained manner.

Right, so this is potentially a bit of a problem (waiting for a nice solution of course!). If you take a look at the supervisor code I posted above, that is **exactly** the level of granularity we want. I completely understand why, due to the asynchronous nature of link/unlink in CH this is difficult, and I do agree that the CH semantics are cleaner and better than Erlang's so it's not a complaint as such. But *how* do we implement something like this in another way?

Depending on various parameters and settings, the supervisor is variously responsible for all these things:

1. making sure that children are restarted when appropriate
2. making sure that restarts/stops are logged with the proper exit reasons
3. making sure that children are stopped/killed in the correct manner

Now to ensure (3) works as expected, we **do** need to make sure that we are guaranteed to get either an 'exit' signal or a 'down' message from a monitor, and this must happen **regardless of whether or not the process is alive when we link to it!** We **cannot** let stray processes stay alive and we cannot finish shutting down our own supervision tree until we're sure that all our children have been stopped, so we do need some guarantees here.

It *may* be that due to the nature of Haskell's asynchronous exceptions and the async nature of link/unlink/monitor, that trying to do this using primitives is the wrong thing to do for CH! Whilst my initial feelings were that we should make these kind of primitives available so that the distributed-process-platform code is generically re-usable for *any* kind of process, perhaps that's not the way to go. If we encoded the rules about process deaths into gen server (or better, a lower level `GenProcess` on which other behaviours can be built) then we could implement a 'go away' protocol that has to be followed by any process that wishes to be included in a supervision tree. But that is subject to numerous timing issues and the fact that using the process' mailbox to handle exits is fraught with its own set of problems.

> My proposal is:
>
> 1. Implement exit :: Serializable a => ProcessId -> a -> Process ()
> 2. Extend DiedReason with DiedKilled Message
> 3. Extend ProcessSignal with Kill Message

I agree that these steps make sense.

> 4. Define newtype ProcessKilled = ProcessKilled Message and make it an exception type
> 5. Implement catchKilled as suggested above.

If I've understood this properly, does that mean that the user code (calling `catchKilled`) will be able to continue executing after the exception handler runs, without further intervention from us? We could presumably created another exception type for immediate/brutal 'kill' and wrap the handler installed via `catchKilled` so that it only deals with the (trappable) `ProcessKilled` exceptions right?


And for the supervisor case, to quote myself:

> It *may* be that due to the nature of Haskell's asynchronous exceptions and the async nature of link/unlink/monitor, > that trying to do this using primitives is the wrong thing to do for CH!

Is it possible to make my supervisor example code (above) work reliably even with these things in mind? I do not actually care about the order of `unlink` as we can simply use monitors and do away with linking when we're detecting/ensuring that child processes have been successfully killed. We *do* need to make sure that we do not see a `ProcessMonitorNotification` in a timely fashion however, that we can brutally kill the process and if we had some guarantee about that behaviour then perhaps we'd be ok in this instance. What are your thoughts on this @edsko ? ","Comment:edsko:11/29/12 11:07:40 AM:

mentioned","Comment:edsko:11/29/12 11:23:53 AM:

I'm not opposed to catching exceptions; it's just for linking it's very subtle. I am not sure why similar issues would arise here. Can you give an example? 

As regards ""untrappable"" exceptions -- as I already mentioned, no such thing in Haskell.","Comment:hyperthunk:11/29/12 11:52:09 AM:

> As regards ""untrappable"" exceptions -- as I already mentioned, no such thing in Haskell.

Yeah that's fine, as long as our primitive for trapping exceptions installs an exception handler for a specific type and we use a different type for brutal kill, that will still work fine.

Also even though we're installing an exception handler on behalf of the user, I'd suggest that we call the API function `trapExit` or something of that ilk, rather than using the word *catch* - basically we want to avoid users installing their own exception handlers and do it for them, only catching `ProcessKilled` exceptions (which equate to `exit(Pid, Reason) where Reason =/= kill` in erlang) and using something like `ProcessBlownToBits` to indicate *untrappable* exit signals :)

>  I am not sure why similar issues would arise here. Can you give an example?

Well, assuming we're shutting down an individual child, can we implement this reliably like so:

```haskell

terminateChild them timeout = do
    monitor them
    exit them Normal
    stopSignal <- expectTimeout timeout 
    case stopSignal of
        Nothing -> killChild them
        Just (ProcessMonitorNotification ref' them reason') -> putState (StoppedOk them)

killChild them = do
    kill them
    stopSig <- expect
    putState (StoppedKilled them)
```

Obviously for a set of children we'd need to keep calling `expectTimeout` and/or `expect` until we'd seen a monitor notification for *each child* and the count of remaining/waiting signals = 0. Here I'm assuming that the exit and kill functions send messages to the relevant node controller which result in something like:

```haskell
ncEffectKill _ pid = do
    throwException pid $ ProcessKill

ncEffectExit _ pid msg = do
    throwException pid $ ProcessExit msg
``` 

I'm also noticing that `postAsMessage` allows us to put something in the mailbox instead, so is it really unfeasible that we could check the process state (as we do in `withLocalProc` for example) and put an exit *signal* in the mailbox instead of throwing, if the process is marked as 'trapping exits' after all? That doesn't seem so bad to me, although it's entirely possible I'm missing something here. 


","Comment:edsko:11/29/12 11:54:48 AM:

I'm not convinced. Idiomatic Haskell is to provide an exception handler: something *scoped*, not something *stateful*. ","Comment:edsko:11/29/12 11:58:55 AM:

I don't think you should attempt to *force* the behaviour you want. If the supervisor wants to kill the children, it can call the `exit` we discussed above. If the child wants to do cleanup, it can use `catchKilled`, do the cleanup, and then exit. It is the child's responsiblity to do that cleanup nicely. 
","Comment:hyperthunk:11/29/12 12:51:32 PM:

> It is the child's responsiblity to do that cleanup nicely. 

If the child is in control of how it shuts down, then isn't it meaningless to specify `timeout :: integer` as a shutdown strategy if there is no way to forcibly terminate the child once the timeout has been exceeded? How does the supervisor respond if the child gets 'stuck' trying to perform its cleanup logic? This happens **all the time** in my experience.

> I don't think you should attempt to force the behaviour you want.

But that's exactly what a supervisor **needs** to do. It - and not the child process - is responsible for ensuring that the system remains in a consistent state and shutting down in a timely fashion is part of that *consistency* that we require. 

Let me take a real world example from RabbitMQ with which to make this more concrete. We use the AMQP protocol to communicate both internally (within the broker) and externally (to clients, federating multiple brokers, etc) and because AMQP is a connection-based protocol over TCP, the network can introduce all kinds of funny behaviours. When an AMQP connection is being asked to shut down, it will send a `conection.close` payload to the other end of the connection and await a `conection.close_ok` response package, after which the connection is considered closed by both ends.

Now connections need to be supervised (obviously) and when we're shutting down a broker (or a plugin running in the broker) we try initially to close the connection cleanly like this. Of course, sometimes the network can become unresponsive (for various reasons) and you can easily simulate this by selectively dropping packets between two communicating ports. When this happens, you end up in a situation where one side is stuck waiting for the other end to agree to close the connection, and with the default TCP network stack on linux, minus any tweaking of kernel parameters, this results in a wait of up to 20 (ish) minutes just to stop the broker! How did we fix this? By putting a timeout into the close function of course.

```erlang
%% @spec (ConnectionPid, Timeout) -> ok | Error
%% where
%%      ConnectionPid = pid()
%%      Timeout = integer()
%% @doc Closes the channel, using the supplied Timeout value.
close(ConnectionPid, Timeout) -> ....
```

But it turns out, unsurprisingly, that starting up a connection is **just as likely to hang**, and that in turn can bring your application to halt just as painfully: http://rabbitmq.1065348.n5.nabble.com/Re-The-rabbitmq-server-stop-command-hangs-td23180.html

Now in erlang you use the supervision mechanism to *control* this kind of thing, so the obvious solution is to make sure that if the **supervisor** is forced to kill a connection (because the application supervision tree is shutting down faster than the `connection:close/2` can cope with *or* because the connection got stuck during startup and hasn't returned by the time we're shutting down again) then the process handling the connection will be terminated regardless of any exit handling code therein. 

```erlang
start_link(AmqpParams) ->
    {ok, Sup} = supervisor2:start_link(?MODULE, []),
    {Type, Module} =
        case AmqpParams of
            _amqp_params_direct{}  -> {direct,  amqp_direct_connection};
            _amqp_params_network{} -> {network, amqp_network_connection}
        end,
    SChMF = start_channels_manager_fun(Sup, Type),
    SIF = start_infrastructure_fun(Sup, Type),
    {ok, Connection} = supervisor2:start_child(
                         Sup,
                         {connection, {amqp_gen_connection, start_link,
                                       [Module, AmqpParams, SIF, SChMF, []]},
                          intrinsic, brutal_kill, worker,
                          [amqp_gen_connection]}),
    {ok, Sup, Connection}.
```

Note the use of `brutal_kill` - we want to ensure that if the supervisor performs the termination then it will go away instantly.

> I'm not convinced. Idiomatic Haskell is to provide an exception handler: something scoped, not something stateful. 

Yes I fully realise that idiomatic erlang isn't always going to make for idiomatic haskell. So your point about using exception handlers is well taken - we should do this, **but** I'm convinced that having two different exception types to throw and only providing an API for users to catch the 'non-brutal' one is a more workable approach. Of course a user can technically install any exception handler they want, but we can document this and recommend against it, providing an API for catching the exception type which represents a 'trap-able exit signal' only and throwing the other kind of exception (for which there is no `catchKilled` API call) when we want to terminate a process 'brutally' like this.

Does that sound like a reasonable compromise? I have seen *so many* issues over the years where a supervision tree is stuck waiting for something to terminate that not being able to brutally kill something like this worries me.

One thing I **do** like about installing exception handlers for this kind of thing is that we can have 'try..finally' semantics with regards resource cleanup so that processes which *are* doing things like opening sockets and the like, can ensure that regardless of how they're terminated, the resources will be correctly released by the time they've exited. That's actually an improvement over the way such things work in Erlang, at least in terms of it being easier to understand.
","Comment:rodlogic:11/30/12 03:40:41 AM:

Ok, straightening our Haskell terminology:
- signal == exception
- trapping a signal == exception handler (catch)
- deliver a signal == raise an exception (throwTo)
- untrappable signals == not possible in Haskell. 

Re: 2) Untrappable signals. This must then be handled as a DON'T DO IT in the docs and possibly address this in the distributed-process-platform by making sure that users can write processes/servers without ever needing to use 'catch' -- and possibly shooting themselves in the foot by affecting an internal protocol. I think this is fine, imo.

Re: 3) Typed exit reasons. I brought up this user-defined 'reason' because I thought that it was a common pattern in Erlang to allow the receiving process to decide whether to accept the kill request or not (based on the reason, pattern matches, etc). Correct me if I am wrong, but it seems that this is not the case: the supervisor must kill the process and that happens with a regular exit signal or an untrappable exit signal to force it, if the process is not cooperating. If the process being killed don't actually have a say, then the purpose of 'reason' is strictly to give any interested parties an additional piece of information (it's just informational). In any case, I think we should drop this idea for now.

Based on the above I would like to propose a change to @edsko proposal:
```
My proposal is:

Implement exit :: Serializable a => ProcessId -> a -> Process ()
Extend ProcessSignal with Kill Message
Define newtype ProcessKilled = ProcessKilled Message and make it an exception type
Implement catchKilled as suggested above.
```

I would suggest that we stick to 'kill' here since it is much stronger than 'exit' (or is there an interest in keeping the terminology here aligned with Erlang or the 'Unified Semantics for Future Erlang'? Fine is that is the case) and I would drop the typed reason for now. More concretely:

```haskell
kill :: ProcessId -> String -> Process ()
```

@hyperthunk Thanks for the concrete examples. They reinforced to me that the typed reason discussed above is not really needed here and that the key point under discussion here is the gracefull kill vs the forced kill (kill -9).

Regarding a forced kill, we don't have much of a choice here since the throwTo exceptions can be caught anywhere in the thread (there is no 100% sure way to guarantee a kill -9). We have then to assume that the user code is playing by the rules (a reasonable expectation, I think) and we should then make sure that the distributed-process-platform is hiding as much as possible from this exception handling business. Basically, GenServer (or the FSM or the Event server, etc) should hide the 'catchKill' handler from the user's view, change the server state to TERMINATING once that exception is caught, and call terminateHandler. Once the handler is complete, it will rethrow the original exception.

In the case that the user's terminateHandler function hangs for whatever reason, the supervisor implementation would timeout and then 'force' a kill, but how? I see two main ways:

1. call 'kill' again (no new primitive introduced). This will hit the GenServer while it was executing the terminateHandler and the handler will simply rethrow the exception since it is in the TERMINATING state.
2. call another function, say 'kill-9', that will throw a differen exception that will not be caught by the GenServer 'catch'. Or throw the same ProcessKilled exception but with an additional flag indicating that it is a forced kill and no termnateHandler should be called.

In both cases, the runLocalProcess will catch the DiedException and notify monitors/links, as it already does.

I would lean towards option (1) above as it seems simpler. Besides, how can we formalize a specialized kill-9 primitive in CH when there is no way to guarantee that the process implementation will play along?

To keep things concrete, the additional refinements to @edsko's proposal:

```haskell
kill :: ProcessId -> String -> Process ()
kill them reason = undefined

data ProcessKilled = ProcessKilled { killer :: ProcessId, killReason :: String }

instance SomeException ProcessKilled where
   ...

catchKilled :: (a -> IO b) -> IO b -> IO b
```

Then it is the GenServer's job to use catchKilled appropriately and implement a forced kill (i.e. don't call the terminateHandler) if this is the second time the ProcessKilled exception is thrown.","Comment:edsko:11/30/12 03:40:41 AM:

mentioned","Comment:hyperthunk:11/30/12 03:40:41 AM:

mentioned","Comment:rodlogic:11/30/12 04:57:52 AM:

Here is a preliminary commit :  

https://github.com/rodlogic/distributed-process/commit/6be0b4fe38ec467cf253aac666dba3d8e34f75b2
https://github.com/rodlogic/distributed-process/commit/37efa276ac95726be5409738da033892b5755c7f

The catchKilled was not implemented so far.","Comment:edsko:11/30/12 09:11:54 AM:

Hey,

I haven't looked at the commit yet (too busy right now), but if you're not going to have typed exit reasons, then you can just define

    newtype ProcessKilled = ProcessKilled String

and then you don't need to implement a special `catchKilled` -- just a regular `catch` (already implemented) will do. The only reason for `catchKilled` is to deal with deserialization.","Comment:hyperthunk:11/30/12 11:25:43 AM:

Right - I'm feeling quite good about where this is going now. I spent some time last night reading the Svensson/Frelund/Earle *Unified Semantics...* paper and I'm seeing more clearly why uni-directional links and no trapping exits seems attractive. I would *not* like to be responsible for making CH move away from it's chosen reference semantics (i.e., that paper) however:

> Regarding a forced kill, we don't have much of a choice here since the throwTo exceptions can be caught anywhere in the thread (there is no 100% sure way to guarantee a kill -9).

This - exactly! There is **no way** to stop users from trapping exit signals if exit signals == exceptions. That's it, end of story. So as long as the semantics of exit in CH are based on `exit = throwTo . processThread` then we **cannot** observe the unified semantics and avoid trapping exits. I think this point needs to be documented properly, as there is no other *sane* way of having exit signals and this **is** veering away from the semantic foundations on which CH is based. I'm not saying its wrong (or even bad) just that we need to make it clear to users somehow. If someone points me at the right place, I'll be happy to write the blurb.

> 1. call 'kill' again (no new primitive introduced). This will hit the GenServer while it was executing the terminateHandler and the handler will simply rethrow the exception since it is in the TERMINATING state.
> 2. call another function, say 'kill-9', that will throw a differen exception that will not be caught by the GenServer 'catch'.

Of these two, I vastly prefer option (2a) - using a different exception type - and to explain why I will quote @edsko 

> ... Idiomatic Haskell is to provide an exception handler: something scoped, not something stateful. 

Option (1) does not fit in light of that design guideline afaict. Also, option (1) would **only** really work for `GenServer` with its stateful take on things. Option (2) could potentially work for other implementations in distributed-process-platform, as `GenServer` isn't necessary the base of everything.

Other than that, I think we're on the right track here.","Comment:edsko:11/30/12 11:25:43 AM:

mentioned","Comment:edsko:11/30/12 11:29:42 AM:

Well, I don't Erlang experience you guys have, so it might be useful to define

    newtype ProcessExit = ProcessExit String deriving (Show, Eq)
    newtype ProcessKill = ProcessKill String deriving (Show, Eq)

    instance Exception ProcessExit
    instance Exception ProcessKill

and export `ProcessExit` (which would correspond to `exit`) but not `ProcessKill` (corresponding to `kill`), so that processes cannot catch `ProcessKill`. This still doesn't stop processes from catching _all_ exceptions, but that's bad form anyhow.

I suppose you are right that this in fact deviates from the semantics, as processes _can_ choose to ignore `ProcessKill`. I don't think there is a way around that though.","Comment:hyperthunk:11/30/12 11:33:59 AM:

@edsko  

> ... it might be useful to define
> 
> [snip]
> 
> and export ProcessExit (which would correspond to exit) but not ProcessKill (corresponding to kill), so that processes > cannot catch ProcessKill. This still doesn't stop processes from catching all exceptions, but that's bad form anyhow.

I think that's a perfectly brilliant way of doing it! Any half decent programmer will avoid catch-all anyway, as you say.

> I suppose you are right that this in fact deviates from the semantics, as processes can choose to ignore ProcessKill. I > don't think there is a way around that though.

I agree. I think your approach above (to only export the exception type used for 'normal' exits) is ideal. ","Comment:rodlogic:11/30/12 11:33:59 AM:

mentioned","Comment:edsko:11/30/12 11:33:59 AM:

mentioned","Comment:edsko:11/30/12 11:37:14 AM:

Incidentally, if you wanted typed exit reasons, you could do something similar: only provide the single `ProcessKilled` exception type, which carries the `Message` recording the reason; in this case, you need to provide a `catchKilled` which catches `ProcessKilled` messages that have a reason of a particular type; by having a special type `ForceKilled` of whatever, which is not exported, you could stop people from catching `ProcessKilled` message with a `ForceKilled` reason. 

(But you seem to have decided that you don't need typed exit reasons, which is fine; this is just FYI :)","Comment:rodlogic:11/30/12 12:25:38 PM:

Good, I think that we are coming to a consensus here. 

Btw, re:
```
(But you seem to have decided that you don't need typed exit reasons, which is fine; this is just FYI :)
```
I dont think we have real strong reasons to stay away from it in the same way that we don't have strong reason to adopt it, imho. What if we keep this first iteration as it is without typed reasons and revisit if if/when we have other use cases?

@hyperthunk re option (2a), I concur. The idea of having two separate exceptions also makes this option even stronger.

@edsko naming: I see that CH uses ProcessKilled as the exception name, but the code seems to use XXXException for other types of exception. Just making sure I conform to the naming conventions: should it be XXXXXException or XXXXX?

@edsko the Erlang experience that Tim has. My experience there is not as deep as I would like.

One more question: should these two exceptions map to a DiedException, i.e. these exceptions are used mainly as a process catch mechanism, but are raised in links/monitors as a DiedException String? That is my understanding.


Aside from any other comments related to the proposal with your recent suggestions, a few questions about naming:
1. 'kill' is forced and 'exit' is graceful termination or the other way around?
2. CH already has a 'terminate', which terminates the current process: is that a terminology we may want to associated somehow with either graceful or forced termination? Or are we good with exit, kill and a terminate?
3. If DiedException is the exception thrown to communicate interested links/monitors that a process died (past tense), should we instead call these two exceptions above: KillProcessException and ExitProcessException? It is a minor point, but catching a ProcessKilledException in the process that is supposedly killed already seems a bit strange :-)
","Comment:hyperthunk:11/30/12 12:25:38 PM:

mentioned","Comment:edsko:11/30/12 12:25:38 PM:

mentioned","Comment:edsko:11/30/12 12:33:51 PM:

> What if we keep this first iteration as it is without typed reasons and revisit if if/when we have other use cases?

To be honest, I'd prefer not to. This will be a change to the core Cloud Haskell infrastructure. I'd prefer not to release one version now and then make an incompatible change later, or (worse, in my opinion), provide both. Pick one or the other :)

> Naming

I guess you're right. `ProcessExitException` and `ProcessKillException`, with `exit` being the ""nice"" and `kill` being the ""not so nice"" one (I am following you guys here; I took it that this was Erlang terminology? I don't insist.)

> Relation to `terminate`

`terminate` doesn't take a reason, and in my opinion shouldn't be in the API at all. It's there only for backwards compatibility with the older `remote` package. 

> Re `DiedException`

`DiedException`is a constructor of `DiedReason`, and not itself an exception; it's an argument to `ProcessMonitorNotification` and to `ProcessLinkException`. Monitoring should not change _at all_ with this change: since monitoring (and linking) already deals with process dying because of an exception, this should deal without difficulty with the case where this exception happens to `ProcessExitException` or `ProcessKillException`.","Comment:rodlogic:11/30/12 12:48:51 PM:

@hyperthunk I am really neutral wrt to typed or untyped reason, but if we have to make a decision now about this then it is worth coming to a final conclusion. Should we go the typed route?","Comment:hyperthunk:11/30/12 12:48:51 PM:

mentioned","Comment:hyperthunk:11/30/12 02:25:19 PM:

@rodlogic - I would say *yes* go with typed exit reasons. They make it easier to understand why the process is being killed (i.e., part of a supervision tree shutting down versus killing a temporary worker) and at the very least they enable cleaner logging output.

@edsko 

>  with exit being the ""nice"" and kill being the ""not so nice"" one

That's exactly what it looks like in Erlang/OTP land.

> terminate doesn't take a reason, and in my opinion shouldn't be in the API at all

So `exit` and `kill` need to be callable from the current (executing) process. This shouldn't make any difference but I just wanted to raise that now, as processes that wish to terminate themselves should be able to provide an exit reason as well.","Comment:rodlogic:11/30/12 02:25:19 PM:

mentioned","Comment:edsko:11/30/12 02:25:19 PM:

mentioned","Comment:edsko:11/30/12 02:27:51 PM:

> So exit and kill need to be callable from the current (executing) process. 

    do me <- getSelfPid ; exit me <<whatever reason>>","Comment:rodlogic:11/30/12 02:32:24 PM:

I made the changes to the kill branch in my fork:

https://github.com/rodlogic/distributed-process/commit/7065abd61cfc1d4aa199c58bf98cc8b3f303a806

We now have two exceptions, ProcessKillException (forced) and ProcessExitException (graceful), and the reason is properly relayed to DiedException constructor.

One difference between ProcessExitException and ProcessKillException is that the former has an additional ProcessId, the pid of the process that called the 'exit' primitive.

@edsko What about **destNid**? This is what I have done:
```haskell
destNid (Kill pid _)        = Just $ processNodeId pid
destNid (Exit pid _)        = Just $ processNodeId pid
```
I am assuming that the node controller may need to forward the ProcessSignal if necessary, but I am not sure I am missing something here.


Now, on to the last open item afaik: typed vs untyped reason. I will experiment adding the changes to see what happens in my local workspace :-) Worse case I will learn a bit more about the CH internals.
","Comment:edsko:11/30/12 02:34:20 PM:

(Lazy, not looking at the code): 

> and the reason is properly relayed to DiedException constructor.

did you mean, automatically, or did you have to do something to make that work?","Comment:rodlogic:11/30/12 02:40:09 PM:

I had to do something to make it work. Here is what I did:
```haskell
        tid' <- forkIO $ do
          reason <-
            (runLocalProcess lproc proc >> return DiedNormal)
              `Exception.catch` (\(ProcessKillException reason) -> return $ DiedException reason)
              `Exception.catch` (\(ProcessExitException _ reason) -> return $ DiedException reason)
              `Exception.catch` (return . DiedException . (show :: SomeException -> String))
```
... otherwise the DiedException would contain a reason with the original reason plus some other garbage that was unimportant, or so it seemed.","Comment:edsko:11/30/12 02:41:44 PM:

Just define

    instance Show (ProcessKillException reason) = reason
    instance Show (ProcessExitException reason) = reason
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
70,Process is an instance of MonadBaseControl IO,"How do you think about making the Process monad an instance of MonadBaseControl IO?

This will make it easier to use the Process monad together with other packages, such as Persistent http://hackage.haskell.org/packages/archive/persistent/1.0.1.3/doc/html/Database-Persist.html and CurryDB http://hackage.haskell.org/packages/archive/CurryDB/0.1.0.0/doc/html/Database-Curry.html .


I am sorry for the large diff, because my editor have removed a lot of trailing whitespaces, but the essential change is as follows:

https://github.com/nushio3/distributed-process/commit/a527a0589fead076942e3a1d94d2f66000c7ecfb",11/12/12 01:27:01 PM,11/16/12 08:14:34 AM,Task,,Closed,Fixed,nushio3,,"Comment:nushio3:11/16/12 08:14:33 AM:

Excuse me, but I realized that pushing the forked master branch is not a good idea. Let me retry this pull request.","Comment:nushio3:11/16/12 08:14:34 AM:

closed","Comment:nushio3:11/16/12 08:30:03 AM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
","Comment:nushio3:11/17/12 08:58:03 AM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
71,open up AbstractMessage,"If I want to write code that deals with messages of *any* type, I'm currently rather stuck. For example, I cannot write

```haskell
demo = do
  receiveWait
    [ (match handleCall) ]
  demo

handleCall :: (Serializable m) => m -> Process m
handleCall m = return m  
```

because

```
Ambiguous type variable `a0' in the constraints:
  (Typeable a0)
    arising from a use of `match'
    at src/Control/Distributed/Platform/GenProcess.hs:56:8-12
  (Binary a0)
    arising from a use of `match'
    at src/Control/Distributed/Platform/GenProcess.hs:56:8-12
```

What this means is that if I want to write a `handleCall` implementation, I have to specify both the *input* and the *output* types, which seems to completely defeat the purpose of having `expect` and `receiveWait` primitives that will take **any** kind of message off the queue. If I wanted to specify *exactly* which types I'm dealing with, I'd use typed channels. ",11/13/12 09:31:06 AM,01/29/13 10:05:32 AM,Task,distributed-process-0.5.0,Closed,Fixed,hyperthunk,Feature-Request distributed-process,"Comment:edsko:11/18/12 10:07:16 AM:

Duplicate of https://github.com/haskell-distributed/distributed-process/issues/30 .","Comment:edsko:11/18/12 10:07:16 AM:

closed","Comment:hyperthunk:01/16/13 10:02:42 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
","Comment:hyperthunk:01/17/13 11:31:23 AM:

No, I'm not convinced this is a duplicate of DPLEGACY-30 after all. That talks about being able to pull `Message` out of the inbox and send it somewhere, whereas **this** talks about being able to layer code such that

* layer 1 can `matchAny` in order to inspect the head of the queue
* layer 2 can define a function with a *specific* type and use the fingerprint mechanism to check if it handles the input from layer 1
* **neither** layer has to actually know about how the fingerprint mechanism works

So in a sense, this is about decoupling the run-time type checking so it can be used in an alternative context. Re-Opening.  ","Comment:hyperthunk:01/17/13 11:31:24 AM:

reopened","Comment:hyperthunk:01/17/13 11:31:31 AM:

assigned","Comment:hyperthunk:01/17/13 12:06:02 PM:

Proposed resolution is pull request DPLEGACY-116 .","Comment:hyperthunk:01/18/13 05:55:43 PM:

merged (DPLEGACY-116)","Comment:hyperthunk:01/18/13 05:55:43 PM:

closed","Comment:edsko:01/29/13 08:53:35 AM:

It was a duplicate only insofar as the same API (what was `AbstractMessage`) could be used for both. ","Comment:hyperthunk:01/29/13 10:05:32 AM:

Yes that's right, and of course the solution to DPLEGACY-30 subsumes this anyway. :)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
72,Process is an instance of MonadBaseControl IO (repost),"This will make  the Process monad an instance of MonadBaseControl IO.

Makes it easier to use the Process monad together with other packages, such as Persistent http://hackage.haskell.org/packages/archive/persistent/1.0.1.3/doc/html/Database-Persist.html and CurryDB http://hackage.haskell.org/packages/archive/CurryDB/0.1.0.0/doc/html/Database-Curry.html .

I am sorry for making the confusion, I hope this patch is cleaner than the previous one ( #70 )",11/16/12 08:29:20 AM,11/17/12 08:49:57 AM,Task,,Closed,Fixed,nushio3,,"Comment:nushio3:11/17/12 08:49:57 AM:

closed","Comment:nushio3:11/17/12 08:58:03 AM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
73,Make Process monad an instance of MonadBaseControl IO.,"

This will make the Process monad an instance of MonadBaseControl IO.

Makes it easier to use the Process monad together with other packages, such as Persistent http://hackage.haskell.org/packages/archive/persistent/1.0.1.3/doc/html/Database-Persist.html and CurryDB http://hackage.haskell.org/packages/archive/CurryDB/0.1.0.0/doc/html/Database-Curry.html .

I am sorry for the repeated confusion, I hope this patch is cleaner than the previous ones ( #70 and #72 )
",11/17/12 08:58:03 AM,12/17/12 11:24:38 AM,Task,,Closed,Fixed,nushio3,,"Comment:edsko:11/17/12 09:10:51 AM:

I'm not sure that we want the additional dependency.@dcoutts - thoughts?","Comment:dcoutts:11/17/12 09:10:51 AM:

mentioned","Comment:dcoutts:11/19/12 03:13:10 PM:

Yeah, I'm suspicious. I'm not familiar with this monad lib. Is it an obviously standard thing that ought to be integrated into transformers / mtl?","Comment:nushio3:11/19/12 05:20:46 PM:

In fact, MonadBaseControl and MonadTransControl are under debate, and may disappear or be replaced in the future. 
Nevertheless, the community haven't found any better method to handle exceptions in Monad towers yet, and I have to use them for a while. 
Maybe an orphan instance library like http://hackage.haskell.org/package/vector-instances is a better solution for us.
Please close this ticket if you think so too.
","Comment:hyperthunk:12/17/12 01:56:57 AM:

@dcoutts @edsko - this pull request cannot be automatically merged and @nushio3 *does* appear to have a workaround. Can we close this?

@nushio3 is that still ok for you? I'm guessing since there's been no commentary here for a while this is probably something you've figured out how to deal with for yourself.","Comment:dcoutts:12/17/12 01:56:57 AM:

mentioned","Comment:edsko:12/17/12 01:56:57 AM:

mentioned","Comment:nushio3:12/17/12 01:56:57 AM:

mentioned","Comment:edsko:12/17/12 08:25:59 AM:

Closing it is fine with me.","Comment:nushio3:12/17/12 09:25:16 AM:

Sure, I respect the developers' decision, and am okay with an orphan instance. Sorry for not making a prompt response.","Comment:nushio3:12/17/12 09:25:17 AM:

closed","Comment:hyperthunk:12/17/12 11:24:38 AM:

Thanks everyone, much appreciated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
74,Make 'register' fail when name already registered,"(Thanks to Jeff Epstein for the below.)

The semantics of whereis and register differ between Erlang and Cloud Haskell. I don't know if this is intentional or not.

In particular, Erlang's documentation [1] says of register that will fail

> if Pid is not an existing, local process or port, if RegName is already in use, if the process or port is already registered (already has a name), or if RegName is the atom undefined.

This differs from CH's register, which will happily re-assign the same name to different processes. See the program in [2]. The value of the process name can be changed, whereas the equivalent program in Erlang will crash. (Erlang allows names to be re-assigned following an explicit unregister.) Furthermore, Cloud Haskell will let a name be assigned to an invalid PID (that is, the PID of a process that has died), whereas Erlang will consider that an error, and will unregister the name of a process when it dies.

So, this presents a possible answer to my question ""What is the Erlang way of starting a once-per-node process?"" In Erlang, one could just have the process call register on itself when it starts. If it succeeds, then it's the first such attempt; but if another process has taken that name before, the process will die harmlessly. (There may be more elegant ways to accomplish the same thing, but that's one variant that occurs to me.) If CH's registration had the same semantics, a similar solution would work there.

Since CH's process registration can't do this as it stands, it seems that the only option is to change it. Should we adopt the Erlang way, or something else?

[1] http://erlang.org/doc/man/erlang.html#register-2

[2]
```Haskell
test =
  do self <- getSelfPid
     b <- spawnLocal $ receiveWait []
     register ""name"" self
     whereis ""name"" >>= say . show    -- says pid://localhost:8080:0:3
     register ""name"" b
     whereis ""name"" >>= say . show    -- says pid://localhost:8080:0:4
```",11/18/12 08:08:56 AM,11/19/12 02:20:22 PM,Task,,Closed,Fixed,edsko,,"Comment:edsko:11/18/12 08:10:52 AM:

If we make this change it becomes very important that the node controller *removes* the registered name when the process dies. We don't currently do this.","Comment:jepst:11/18/12 10:42:37 PM:

I have a patch for this:

https://github.com/jepst/distributed-process/commit/24163c2acde95fd7ca4deff1f6b57a743e8a6d4b
https://github.com/jepst/distributed-process/commit/37b02637a73735c5164fc7a585782534f9533736

If you approve, I'll submit it.","Comment:edsko:11/19/12 09:03:39 AM:

Looks good. I presume you ran the unit tests?

Only one comment: there is no need to change the representation of the registry from

    !(Map String ProcessId)

to

    !(Map (String,NodeId) ProcessId)

because you can easily extract a `NodeId` from a `ProcessId`. If you change that make, please submit a pull request and I'll apply it.
","Comment:jepst:11/19/12 10:30:07 AM:

Actually, the change of the registry is necessary. Names can be registered at any node for any PID, even if that PID doesn't live at that node. When a process has a registered name on a different node, that remote node must be notified when the process dies. The NodeId in NCState's registry indicates the node where this process is registered, and thus who should be sent a Died message.

Thus:
>   registerRemoteAsync nodeA ""foo"" blah

will result in a value of registry:

>   fromList [((""foo"",nodeA),blah)]

in at least one and up to two places: on nodeA, and also on blah's node, and for two different purposes. On nodeA, registry acts as a name lookup. On blah's node, registry acts as a list of node to inform of blah's death.

I hope that's clear. It was a little tricky to write.

I also had to change TestCH to reflect the fact that names are now erased when the node of the process they refer to is disconnected.","Comment:edsko:11/19/12 10:43:37 AM:

Ah, I see. I missed the point, sorry. Similar to how monitors are recorded then: as you say, in at least one, and up to two nodes. I wonder if the intention would be clearer if we split the `registry` into two components

    registeredHere :: Map String ProcessId

and

    registeredOnNodes :: Map ProcessId [NodeId]

That would also make the indexing more efficient, wouldn't it? For local lookups we want to lookup by name, but for the purpose of notifying nodes when a process dies, we want to lookup by `ProcessId`. What do you think?","Comment:jepst:11/19/12 10:45:22 AM:

Yeah, it does make sense.","Comment:jepst:11/19/12 10:51:08 AM:

Except, we also need to know the name of the registered process, so we can remove the notification when it's erased. So we would really need:

> registeredOnNodes :: Map ProcessId [(NodeId, String)]","Comment:edsko:11/19/12 10:57:31 AM:

I'm probably being thick, but why is that necessary? Process with pid `PID` dies. The local node controller notices this, looks up the set of nodes where that `PID` is registered using `registeredOnNodes` (that might include itself, of course), sends those nodes a message that this process died and removes the entry from `registeredOnNodes`. When a node controller receives a message that a process died removes that process from `registeredHere`. Am I missing something?","Comment:jepst:11/19/12 11:05:05 AM:

The call

> unregisterRemoteAsync nid ""foo""

will remove the (nid,""foo"") entry from the registeredOnNodes of the node where ""foo"" is running (but it won't remove other registeredOnNodes entries for that node).","Comment:edsko:11/19/12 11:20:19 AM:

`registeredOnNodes` is just there so that we can notify nodes though right? Are you saying that because there can be multiple names for a single process, if we do an unregister we don't know if we can remove the entry from the `registeredOnNodes` table because there might still be other names? Would it be solved if we added a count to the table? (Not suggesting that we do that, just making sure I understand your point).","Comment:jepst:11/19/12 11:25:32 AM:

Yes, that's it. A per-node count would work, too, e.g.

> registeredOnNodes :: Map ProcessId [(NodeId, Int)]","Comment:edsko:11/19/12 11:36:02 AM:

Right. I think that perhaps that representation makes the intent the clearest, what do you think?","Comment:jepst:11/19/12 01:40:41 PM:

I agree. I've pushed a change implementing your proposal.","Comment:edsko:11/19/12 01:47:57 PM:

Ok, great. Please submit a pull request.","Comment:edsko:11/19/12 02:20:22 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
75,a variant of 'link' that only signals in case of abnormal termination,"CH's _link_ differs from Erlang's in that it is (a) unidirectional and (b) signals for all termination, not only abnormal termination. I understand and agree with the reasons for this design decision. However, I would like a version of _link_ in CH that signals only on abnormal termination (although it can remain unidirectional). Let's call this variant _linkOnFailure_.

Some arguments in favor of this feature:

1. I think it's a common situation for a service process to spawn worker processes, which may fail (possibly running user-provided code). The service process should die if any of the worker nodes fail, but not when they complete normally. A ""fire and forget"" solution would be ideal, which is possible with _linkOnFailure_. Implementing this behavior in terms of _monitor_ is arduous: the service process needs additional state to store the MonitorRefs of the worker process, and it needs to listen for ProcessMonitorNotifications and act accordingly. 
2. Handling this kind of state for managing MonitorRefs gets a lot harder as the service process gets more complicated. If the service process implements a state machine, such that _receiveWait_ is used with a _match_ body, every embedded message receiver must separately check for ProcessMonitorNotifications (and update the process's state on their basis), especially if one of the interior message receivers is waiting for results from a worker process. Otherwise the ProcessMonitorNotification will be unread and the process will deadlock rather than terminate. For hierarchical state machine, this can lead to a lot of repetitive, bug-prone code.
1. It's not possible to write _linkOnFailure_ on the basis of what's provided in CH, so it must be a primitive.
4. Erlangers expect _linkOnFailure_-type behavior, so the presence of that option will make transitioning to CH easier for them. Not having _linkOnFailure_ seems like a strange omission, that it's bound to raise questions.

There's a question of how _link_ would interact with _linkOnFailure_. I think the best option is to go with the behavior of the most inclusive form. Thus, if process A calls _link_ on process B and then calls _linkOnFailure_ on it, the result will be as if only _link_ was called (since _link_ implies _linkOnFailure_). The result would be the same if process A calls _linkOnFailure_ on process B and then calls _link_ on it. _unlink_ would remove all linking between the two processes. 

I understand a hesitance to add _linkOnFailure_: it isn't strictly speaking necessary, and it often isn't what you really want to do. But it makes common cases a lot easier to deal with. Ultimately, I think the programmer should be trusted to use powerful tools responsibly.",11/18/12 11:03:30 PM,01/04/13 03:59:02 PM,Task,,Closed,Fixed,jepst,,"Comment:jepst:11/19/12 12:33:13 AM:

The problem could also be solved if we had a function _unlinkOther_ that would remove the link between other processes.

unlinkOther :: ProcessId -> ProcessId -> Process ()","Comment:edsko:11/19/12 01:44:09 PM:

Discussing this with @dcoutts we are still not keen on implementing this; this seems a typical use case for monitoring. Note, however, that you *can* implement this on top of the existing infrastructure if you really wish:

```Haskell
linkOnFailure :: ProcessId -> Process ()
linkOnFailure them = do
  us <- getSelfPid
  tid <- liftIO $ myThreadId
  spawnLocal $ do
    link us
    monitor them
    ProcessMonitorNotification _ pid reason <- expect
    case reason of
      DeadNormal -> return ()
      _ -> throwTo tid (ProcessLinkException pid reason)
```","Comment:dcoutts:11/19/12 01:44:09 PM:

mentioned","Comment:hyperthunk:12/03/12 02:23:18 PM:

if/when issue DPLEGACY-69 gets merged this should be cleaner to implement too.","Comment:hyperthunk:12/17/12 12:50:30 PM:

I'm going to close this issue for now. I will implement `linkOnFailure` in [distributed-process-platform](https://github.com/haskell-distributed/distributed-process-platform) although it needs to use two-way monitors instead of links, due to a potential race wherein `us` dies before the spawned thread calls `link` but `link` always succeeds and therefore we end up with a potentially zombie process.

See [distributed-process-platform issue 35](https://github.com/haskell-distributed/distributed-process-platform/issues/35).
","Comment:hyperthunk:01/04/13 03:59:02 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
76,Align semantics of register/whereis with Erlang,"This patch matches the behavior of register/whereis with Erlang. Specifically, attempting to register an already-registered name will throw an exception, and names are automatically unregistered with the associated process dies.",11/19/12 02:10:16 PM,11/19/12 02:20:09 PM,Task,,Closed,Fixed,jepst,,"Comment:edsko:11/19/12 02:20:09 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/474909d2438ee13f850c1a9a28c1a7c3324909b3
","Comment:edsko:11/19/12 02:20:09 PM:

merged","Comment:edsko:11/19/12 02:20:09 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
77,Make it possible to terminate the logger process,"The logger process has no way to shut itself down on request, so for small examples that do a few `say` calls, some of the messages are not emitted because the program terminates before the logger process has processed the messages.

This adds a way to shut down the logger process.  I think in general we will need a better way to do this - it looks like `closeLocalNode` should be doing this, perhaps.",11/27/12 02:44:53 PM,11/29/12 03:54:18 PM,Task,,Closed,Fixed,simonmar,,"Comment:edsko:11/29/12 09:30:09 AM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/3e510840c7f4f046daad628ab85d272259b81e6f
","Comment:edsko:11/29/12 09:30:09 AM:

merged","Comment:edsko:11/29/12 09:30:09 AM:

closed","Comment:edsko:11/29/12 09:30:13 AM:

Thanks.","Comment:rrnewton:11/29/12 03:32:42 PM:

Does GHC not have a hook for doing cleanup on normal process exit?  Is there a reason that's a bad idea?

One reason I ask is that it's expensive to wait for monad-par worker threads when exiting runPar, and determinism would be maintained (though not locality of exceptions) as long as work eventually finishes before process exit.","Comment:simonmar:11/29/12 03:54:18 PM:

On 29/11/12 15:32, Ryan Newton wrote:
> Does GHC not have a hook for doing cleanup on normal process exit? Is
> there a reason that's a bad idea?

We like to let the programmer arrange this themselves, rather than 
build-in a particular policy.

> One reason I ask is that it's expensive to wait for monad-par worker
> threads when exiting runPar, and determinism would be maintained (though
> not locality of exceptions) as long as work eventually finishes before
> process exit.

I don't think you would get determinism that way: a runPar could return 
a value when it should have thrown an exception, and the program could 
go on to have side effects (e.g. launch missiles) that it shouldn't have 
done.

Cheers,
	Simon",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
78,Issue #69 - Killing processes,This will implement a new protocol for forcefully killing and gracefully exiting a process. ,12/03/12 03:19:48 PM,12/03/12 05:29:13 PM,Task,,Closed,Fixed,rodlogic,,"Comment:edsko:12/03/12 05:29:12 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/2074a01d7a501c1819ea279cdc833a609e3b7f47
","Comment:edsko:12/03/12 05:29:12 PM:

merged","Comment:edsko:12/03/12 05:29:12 PM:

closed","Comment:edsko:12/03/12 05:29:13 PM:

Many thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
79,add mkStaticClosure :: Name -> Q Exp,I'd like to have this (or something like it) to avoid needing the dummy () argument when I spawn a top-level Process () function.,12/03/12 04:13:36 PM,12/03/12 04:15:12 PM,Task,,Closed,Fixed,simonmar,,"Comment:edsko:12/03/12 04:15:12 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/8a6a6cae344fa02a0bbf64d78f74528d495131e6
","Comment:edsko:12/03/12 04:15:12 PM:

merged","Comment:edsko:12/03/12 04:15:12 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
80,Shut down the logger process before exiting.,.. otherwise some of the 'say' messages get lost.,12/03/12 04:29:27 PM,12/03/12 04:30:57 PM,Task,,Closed,Fixed,simonmar,,"Comment:edsko:12/03/12 04:30:56 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/8bc0d4a927df4112682121d86c447292a274ff64
","Comment:edsko:12/03/12 04:30:57 PM:

merged","Comment:edsko:12/03/12 04:30:57 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
81,how do we implement timers efficiently,"This is more of a question right now, and I'll happily attempt an implementation and send a pull *if* it turns into a feature request. ;)

In Erlang, we **frequently** want to handle *things that happen after some time interval* - here's a classic example from RabbitMQ's erlang AMQP client, where we ensure that the network driver **has** to be closed within a reasonable time frame once the connection has been torn down.

```erlang
do(#'connection.close_ok'{} = CloseOk, State) ->
    erlang:send_after(?SOCKET_CLOSING_MAX_TIMEOUT, self(), socket_closing_timeout),
    do2(CloseOk, State);
do(Method, #state{writer0 = Writer}) ->
    %% catching because it expects the {channel_exit, _} message on error
    catch rabbit_writer:send_command_sync(Writer, Method).

handle_message(socket_closing_timeout,
               State = #state{closing_reason = Reason}) ->
    {stop, {socket_closing_timeout, Reason}, State};
handle_message(socket_closed, State = #state{waiting_socket_close = true,
                                             closing_reason = Reason}) ->
    {stop, {shutdown, Reason}, State};
%% snip....
```

Now the key here is `erlang:send_after(Milliseconds, Pid, Message)` which schedules a message to be sent to `Pid` after the appropriate delay. There is very little performance impact on the run-time system from this call.

Now I can implement this quite easily in CH I think:

```haskell
module Control.Distributed.Platform.Timer where

import Control.Distributed.Process
import Control.Distributed.Process.Serializable
import Data.Binary
import Data.DeriveTH
import Data.Typeable                            (Typeable)
import Prelude                                  hiding (init)

-- | a reference to a timer
type TimerRef = ProcessId

-- | cancellation message sent to timers
data CancelTimer = CancelTimer
    deriving (Typeable)
$(derive makeBinary ''CancelTimer)

sendAfter :: (Serializable a) => Int -> ProcessId -> a -> Process TimerRef
sendAfter ms pid msg = spawnLocal $ do
    cancel <- expectTimeout ms
    case cancel of
        Nothing          -> send pid msg
        Just CancelTimer -> return ()
    return () 

cancelTimer :: TimerRef -> Process ()
cancelTimer = (flip send) CancelTimer
```   

Now I have two questions about this.

1. is this the most efficient approach?
2. does this kind of functionality belong in distributed-process or distributed-process-platform?

If 1 == true && 2 == (the latter) then I'll go ahead and implement this over in the platform APIs.
",12/05/12 11:40:17 AM,12/05/12 01:51:44 PM,Task,,Closed,Fixed,hyperthunk,,"Comment:edsko:12/05/12 11:47:10 AM:

Hmm, I was going to suggest to use standard Haskell timers for this, but I guess you cannot! Yeah, I suppose the above is a reasonable implementation. And yes, I don't think we need this in the core libraries.","Comment:edsko:12/05/12 11:47:11 AM:

closed","Comment:edsko:12/05/12 12:58:57 PM:

mentioned","Comment:hyperthunk:12/05/12 01:01:29 PM:

Thanks for the clarification @edsko - I'll put it into the platform APIs instead. Can you point me to the docs for standard Haskell timers please, as I've not used them before and would like to know more? Cheers!","Comment:edsko:12/05/12 01:01:29 PM:

mentioned","Comment:edsko:12/05/12 01:31:22 PM:

Actually, I lie that it's impossible. I was thinking of [System.Timeout](http://hackage.haskell.org/packages/archive/base/latest/doc/html/System-Timeout.html) but actually what you want is [threadDelay](http://hackage.haskell.org/packages/archive/base/latest/doc/html/Control-Concurrent.html_v:threadDelay). It would be possible to use `threadDelay` instead, but actually I rather like your solution!
","Comment:hyperthunk:12/05/12 01:33:48 PM:

@edsko - both those calls block the executing thread though don't they? ;-)","Comment:edsko:12/05/12 01:33:48 PM:

mentioned","Comment:edsko:12/05/12 01:38:05 PM:

Well, normally (in an IO context) you would do

```Haskell
forkIO $ do
  threadDelay <whatever delay>
  <timer functionality>
```","Comment:hyperthunk:12/05/12 01:51:44 PM:

> Well, normally (in an IO context) you would do

Yep I see what you mean. :)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
82,Re-export MonitorRef in the Process module,"I am not sure if this is as designed, but the Process module doesn't export MonitorRef , only monitor. Right now I have to import Process(monitor) and Types(MonitorRef). Wouldnt it be cleaner to re-export MonitorRef in the Module so keep the internal modules hidden?",12/08/12 09:36:00 PM,12/09/12 09:24:21 AM,Task,,Closed,Fixed,rodlogic,,"Comment:edsko:12/09/12 09:24:21 AM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/28d8a5f6671aa80d6f11a28838a248ff97a065af
","Comment:edsko:12/09/12 09:24:21 AM:

merged","Comment:edsko:12/09/12 09:24:21 AM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
83,Tweak reregister semantics,"Jeff's recent changes made it an error to reregister a name if the
name was *not* already registered.  This interacts badly with the way
that SimpleLocalNet handles logging:

 1. start up a slave
 2. start up the master
 3. terminate the master
 4. start the master again

after restarting the master, logging no longer works on the slave.
This is because in step 3 when the master dies, the slave's
registration for ""logger"" is erased, and in step 4 the master calls
reregisterRemoteAsync to register ""logger"", but this fails because
there is no existing registration for ""logger"" on the slave.

My fix is to change the semantics of reregister to make this work.  If
we need reregister with the old semantics, we could add a third
variant instead.",12/11/12 02:41:12 PM,12/12/12 01:42:36 PM,Task,,Closed,Fixed,simonmar,,"Comment:jepst:12/11/12 03:44:07 PM:

I would prefer to stay with Erlang semantics for the primitives. In this case, the obvious solution is to fix SimpleLocalNet, rather than to fix reregister. In apiRedirectLogsHere, it would be a simple matter to catch a negative response from reregisterRemoteAsync, and then try a registerRemoteAsync.","Comment:simonmar:12/11/12 03:50:58 PM:

Does Erlang have a reregister operation? I couldn't find one.","Comment:jepst:12/11/12 03:56:30 PM:

There is a global:re_register_name function, although it is a cluster registry operation, rather than a local registry operation.","Comment:simonmar:12/11/12 04:01:15 PM:

Right, that's part of the `global` module, which implements a separate global registry (as far as I understand it).  There's no reregister operation on the local registry.","Comment:simonmar:12/12/12 01:42:36 PM:

Ok, I'll do this a different way.","Comment:simonmar:12/12/12 01:42:36 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
84,"Various improvements, and fix restarting of master with simplelocalnet",,12/12/12 01:43:44 PM,12/12/12 02:10:09 PM,Task,,Closed,Fixed,simonmar,,"Comment:simonmar:12/12/12 02:10:09 PM:

merged","Comment:simonmar:12/12/12 02:10:09 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
85,"Various improvements, and fix restarting of master with simplelocalnet",,12/12/12 02:12:03 PM,12/12/12 02:16:03 PM,Task,,Closed,Fixed,simonmar,,"Comment:edsko:12/12/12 02:16:03 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/331215739dd49b6084fb0b30ee3951c2f67ac1e8
","Comment:edsko:12/12/12 02:16:03 PM:

merged","Comment:edsko:12/12/12 02:16:03 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
86,Implement matchChan,"Here is matchChan, as discussed.

I added a test case, but it needs some more work (see the commit log).

You might want to check whether there's any performance overhead worth worrying about here. I've been careful not to do repeated traversals of the mailbox, but there will be a constant factor overhead for each receiveWait due to splitting the matches into chunks, and an extra layer of constructor on each match.",12/14/12 12:40:56 PM,12/14/12 12:46:55 PM,Task,,Closed,Fixed,simonmar,,"Comment:edsko:12/14/12 12:46:30 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/38c90110d02dc05b346d3c6b5adaf6a19d80d627
","Comment:edsko:12/14/12 12:46:30 PM:

merged","Comment:edsko:12/14/12 12:46:31 PM:

closed","Comment:edsko:12/14/12 12:46:54 PM:

Many thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
87,Fix receiveWait test,See https://github.com/simonmar/distributed-process/commit/ad2d328d33bf92102f8d694886f2f44a12c68e3a.,12/14/12 12:48:38 PM,12/18/12 10:56:54 AM,Task,distributed-process-0.5.0,Open,Unresolved,edsko,distributed-process Bug,"Comment:hyperthunk:12/17/12 01:14:13 AM:

Maybe I'm missing something, but I don't get what exceptions you're talking about in https://github.com/simonmar/distributed-process/commit/ad2d328d33bf92102f8d694886f2f44a12c68e3a Simon. Are you talking about the `kill p ""BANG""` calls? Because that's an asynchronous primitive and you'll only see the results of it if you're monitoring the target process won't you?

If that is what we're looking for here, then we do similar stuff in the tests for distributed-process-platform's `Timer` module [here](https://github.com/haskell-distributed/distributed-process-platform/blob/master/tests/TestTimer.hs_L65) and the `Async` Process API [here](https://github.com/haskell-distributed/distributed-process-platform/blob/async-refactoring/tests/TestAsync.hs_L31). Perhaps we can borrow some of those ideas? e.g.,

```haskell
testAsyncCancel :: TestResult (AsyncResult ()) -> Process ()
testAsyncCancel result = do
    hAsync <- async $ runTestProcess $ say ""running"" >> return ()
    sleep $ milliseconds 100
    
    p <- poll hAsync
    case p of
        AsyncPending -> cancel hAsync >> wait hAsync >>= stash result
        _            -> say (show p) >> stash result p
```

```haskell
testCancelTimer :: TestResult Bool -> Process ()
testCancelTimer result = do
  let delay = milliseconds 50
  pid <- periodically delay noop
  ref <- monitor pid    
  
  sleep $ seconds 1      
  cancelTimer pid
      
  _ <- receiveWait [
        match (\(ProcessMonitorNotification ref' pid' _) ->
                stash result $ ref == ref' && pid == pid') ]
        
  return ()

testRunAfter :: TestResult Bool -> Process ()
testRunAfter result = do
  let delay = seconds 2  

  parentPid <- getSelfPid
  _ <- spawnLocal $ do
    _ <- runAfter delay $ send parentPid Ping
    return ()

  msg <- expectTimeout (intervalToMs delay * 4)
  case msg of
      Just Ping -> stash result True
      Nothing   -> stash result False
  return ()
```","Comment:simonmar:12/17/12 08:56:38 AM:

The problem I'm referring to is that if the test fails, test-framework still reports success.  You can see what happens by modifying the test so it fails.  This seems to be because the exception thrown by `@?=` is not propagated (it is not even reported to `stdout`). I haven't investigated why that is yet.
","Comment:hyperthunk:12/17/12 11:28:10 AM:

Hmn, that's annoying. Don't you want to stash the result somehow and/or split the test cases up into separate functions anyway - which makes using `assertBool` or some such easier - otherwise if you throw an exception and it does get propagated, won't that prevent the subsequent test cases from getting run?","Comment:simonmar:12/18/12 10:56:54 AM:

Yes, the tests should be split up, that's a separate issue though.","Comment:hyperthunk:01/09/13 05:12:05 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
","Comment:hyperthunk:01/17/13 11:55:06 AM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
88,Improve efficiency of local message passing,"I see no reason why we should copy data that's being passed between two processes on the same (local) node. So I've started experimenting with skipping some of the overhead of NT by sending directly via the node controller using `sendCtrlMsg` instead.

Initial commit is [here](https://github.com/hyperthunk/distributed-process/compare/local-send). With this branch installed locally, the distributed-process-platform tests still pass, but I have seen the test run bomb out with 130 exit code once, which is a bit worrying.

Following on from that commit, I'd like to see if we can skip the serialization step and just enqueue the data directly instead of creating a new `Message` in which to pass it. This shouldn't be too hard, but the matching operations will still need the type fingerprint in order to handle *selective receive* so I might create a type class that encapsulates the fingerprint and access to the payload, which we can use in the matching code. We can then have an instance for `Message` that calls decode and another for `Serializable a` that just returns the enclosed data directly.",12/14/12 04:48:07 PM,01/27/13 10:46:51 PM,Task,distributed-process-0.4.3,Open,Unresolved,hyperthunk,Feature-Request distributed-process In-Progress,"Comment:hyperthunk:12/14/12 04:48:07 PM:

assigned","Comment:simonmar:12/17/12 08:26:05 AM:

Note that sending must have the same strictness properties when sending locally as when sending remotely, so I think serialization will be necessary, although deserialization can be skipped (as long as we document the assumption that `encode . decode` should be the identity).
","Comment:hyperthunk:12/17/12 09:55:22 AM:

> Note that sending must have the same strictness properties when sending locally as when sending remotely

Naive question, but can we not fake that somehow without forcing serialization? It seems a massive overhead. I could not find anything suggestive of strictness constraints in ""Towards Haskell ..."" so I'm assuming that this has to do with the strictness properties of `ByteString` and its storage of vectors (as strict `Word8` arrays of bytes) or does this rather arise from the call to `Data.Binary.encode`?

How do the strictness properties of sending affect the semantics here? Presumably we need to ensure that `send` is evaluated strictly (so that the programmer knows this operation will not be deferred) but is serialization really the **only** way to guarantee that? There must be something we could do to provide the same semantics without actually converting the passed data structure to bytes.","Comment:edsko:12/17/12 10:03:28 AM:

Serializing most (but not necessarily all) data structures will force them to be fully evaluated (unless the corresponding 'encode' function skips part of the data structure). So, in order to maintain the semantics between sending messages remotely and locally you'd have to do the same thing. If you want to maintain this semantics precisely I don't see that you have an option not to serialize; how else are you going to get the same semantics? You could add an `NFData` construct and use `deepSeq`, but now you're changing the CH API, and moreover, there is no guarantee that the `deepSeq` will have the same affect as `encode`. 

Whether or not preserving this semantics is truly important is a separate issue.","Comment:hyperthunk:12/17/12 10:14:37 AM:

> Serializing most (but not necessarily all) data structures will force them to be fully evaluated

Of course. Sorry, that's my ML addled brain not fully thinking in terms of laziness yet - I'll get there eventually.

> You could add an NFData construct and use deepSeq, but now you're changing the CH API, and moreover, there is > no guarantee that the deepSeq will have the same affect as encode. 

That doesn't sound like the way to go, unless you tie that to identity. Even then I really don't like the idea of changing the top level API in this way. It feels way to invasive and creates a bigger conceptual overhead for people to grok before they can work productively.

> Whether or not preserving this semantics is truly important is a separate issue.

Indeed. When you're sending locally, I can't see why this matters *much* in practice. If we simply pass a pointer to the data structure being sent, then it will be fully evaluated when the consumer forces this. Evaluation could presumably have a space/time effect in the consuming process' code. We do know that no *unexpected* side effects will take place though don't we.

In my opinion this is a reasonable trade-off that could simply be documented in the API so that it is apparent to consumers. This would force the programmer to decide whether or not they should forcefully evaluate the whole data structure before sending, or suck up the consequences otherwise.

Another alternative, I suppose, would be to provide `sendLocal` as a different/alternative primitive. Personally I don't really like that idea as the *location transparency* of Erlang's `!` operator is a huge benefit to the programmer and I'd be reluctant to loose that.

... Or ... we could provide an alternative `sendLocal` that **does** serialize, for those who wish to deliberately do this. I'm struggling to imagine who would choose to use such a primitive or why, which seems to indicate that from a developers perspective, serializing local sends by default is not the intuitive behaviour one would expect.

","Comment:edsko:12/17/12 10:26:05 AM:

> Another alternative, I suppose, would be to provide sendLocal as a different/alternative primitive. Personally I don't really like that idea as the location transparency of Erlang's ! operator is a huge benefit to the programmer and I'd be reluctant to loose that.

The thing is that if you don't serialize the data structure you don't have local transparency, because the program might alter it's behaviour quite significantly if data structures get forced or not. Indeed, you already say this: ""This would force the programmer to decide whether or not they should forcefully evaluate the whole data structure before sending"" -- which is not something that's currently required and something's that's never required when the data *is* serialized (and we have no way around that). In other words, if you want the local transparency, you need to do the same thing in both cases.

In serious applications it is important to control when data structures get forced; it seems rather dangerous to have this depend on network topology. 

I honestly don't know what the right approach is here. ","Comment:hyperthunk:12/17/12 11:10:40 AM:

> In serious applications it is important to control when data structures get forced; it seems rather dangerous to have this depend on network topology.
> 
> I honestly don't know what the right approach is here. 

Hmn, this is an aspect of laziness that I hadn't fully considered. Because of this consideration, I guess that by default we **should** make sure that local sends go through serialization, though as @simonmar points out we don't actually have to deserialize. I'm guessing what he means by this is that we only have to call `encode` but we do not need to actually use the resulting `ByteString` - so we can just pass the original pointer we received because the call to `encode` will have walked the whole structure and forced any thunks to be evaluated.

Now in terms of actually passing a `Serializable a => a` instead of a `ByteString` I was experimenting and I came up with this so far.

```haskell
data LocalMessage :: * where
  LocalMessage :: forall a. Serializable a => a -> LocalMessage

class Deliverable a where
  typeFingerprint :: a -> Fingerprint
  payload         :: (Serializable b) => a -> Maybe b  

instance Deliverable Message where
  typeFingerprint = messageFingerprint
  payload         = reify

reify :: forall a. Serializable a => Message -> Maybe a
reify m = 
  case messageFingerprint m == fingerprint (undefined :: a) of
    True -> Just decoded
    _    -> Nothing
  where decoded :: a
        !decoded = decode (messageEncoding m)

instance Deliverable LocalMessage where
  typeFingerprint (LocalMessage m) = fingerprint m
  payload = reifyLocal
                                         
reifyLocal :: forall a. Serializable a => LocalMessage -> Maybe a
reifyLocal (LocalMessage m) =
  case fingerprint m == fingerprint (undefined :: a) of
    True -> Just unpack
    _    -> Nothing
  where unpack :: a
             !unpack = m
``` 

Presumably I'd have to add a call to `encode` in reifyLocal - that's easy enough.

I can't seem to figure out how to make the existential work so that the `Typeable` comes into scope. My *plan* was to change node to initialize `CQueue` to handle either `Message` or `LocalMessage` and use the `Deliverable` type class in the match implementations. Does that sound like a reasonable approach? And how can I make a concrete type of `CQueue a` when all I know about the type parameter is that it's either `Message` or `LocalMessage` - the latter being an existential? I'm afraid my understanding of existentials is coming up short here, which is frustrating because I can see how to implement this otherwise.","Comment:simonmar:12/17/12 11:10:40 AM:

mentioned","Comment:edsko:12/17/12 11:47:51 AM:

You like your type classes, don't you :-) I would consider changing `Message` itself. 

```Haskell
data Message = EncodedMessage Fingerprint ByteString | forall a. Typeable a => UnencodedMessage a
```

or something along those lines.","Comment:hyperthunk:12/17/12 11:50:49 AM:

Oooh - that's cool. I had *no idea* that was even possible. Don't really know much about existentials yet, so working on CH is turning out to be a bit of a baptism by fire. :)

I'll have a go at using that construct instead - it's **much** nicer than what I was attempting.

Cheers!","Comment:edsko:12/17/12 12:00:57 PM:

> I'm guessing what he means by this is that we only have to call encode but we do not need to actually use the resulting ByteString

Yes, but remember that you will need to force that `ByteString` or *still* nothing happens :)","Comment:hyperthunk:12/17/12 12:07:26 PM:

> Yes, but remember that you will need to force that ByteString or still nothing happens :)

Cripes yes, I would've forgotten that if you hadn't said something. Cheers! :D","Comment:edsko:12/17/12 12:09:57 PM:

And since this is a *lazy* bytestring, you will need to force the entire thing. Something like

```Haskell
let encoded = encode a in length encoded `seq` UnencodedMessage a
```

or something like that. This is the hardest part about writing serious Haskell applications -- I don't want to tell you how many subtle laziness bugs I've had to fix in Cloud Haskell so far, or you might lose all confidence in me :)","Comment:hyperthunk:12/17/12 12:24:00 PM:

> I don't want to tell you how many subtle laziness bugs I've had to fix in Cloud Haskell so far, or you might lose all confidence in me :)

He he - I'm just hoping to make sure none of my contributions cause any major problems! I think as we increase the test coverage we'll get more confidence about semantics affecting bugs, though whether we remain space/time efficient will require manual benchmarking I guess.

Thanks for the pointer anyway - I'll go with your suggestion for now. If there any way that in a test case, using GHC APIs we can determine whether or not something is fully evaluated on receipt?","Comment:hyperthunk:12/17/12 12:46:45 PM:

Note to self: need to update `postAsMessage` and other auxiliary capabilities in `Node.hs` before merging this.","Comment:hyperthunk:01/27/13 10:46:51 PM:

Hmn, there is some unpleasantness to deal with when implementing this. The channel send operations would also have to be modified in order to maintain the stronger ordering semantics we want. This probably needs a bit more thought.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
89,Accessing Process Stats,"Leading on from [issue 65 comment 2](https://github.com/haskell-distributed/distributed-process/issues/65#issuecomment-10147301), it would be nice to be able to get some per-process stats, such as message queue length.

Presumably we could look at the map of typed channels stored in the process state to get some useful metrics out of that too. Quite how efficient looking at the types would be I'm not sure, but it *does* sound useful.

Also, another assumption I'm making here is that this would need to be a primitive.",12/15/12 06:03:23 PM,01/25/13 11:43:02 AM,Task,distributed-process-0.4.2,Closed,Fixed,hyperthunk,Feature-Request,"Comment:hyperthunk:12/15/12 06:03:23 PM:

assigned","Comment:hyperthunk:12/16/12 01:12:00 PM:

Initial stab at this [here](https://github.com/hyperthunk/distributed-process/compare/process-info). I have indeed made it a primitive and it is dispatched to, and handled via the target process' node controller. This allows you to get process info for remote processes, which is something erlang's `process_info` does not. ","Comment:hyperthunk:12/18/12 01:18:51 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
","Comment:hyperthunk:12/18/12 01:55:28 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
","Comment:hyperthunk:12/20/12 03:47:27 PM:

Looks like we can't get the mailbox size easily without blocking the caller - this can **so easily** lead to deadlock, I'm loth to do it, so I've taken that lookup out for now and will open a separate issue to address it. https://github.com/hyperthunk/distributed-process/compare/process-info","Comment:hyperthunk:12/20/12 04:33:22 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
","Comment:hyperthunk:01/05/13 02:15:20 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
","Comment:hyperthunk:01/15/13 01:44:21 AM:

Merged in DPLEGACY-111 ","Comment:hyperthunk:01/15/13 01:44:21 AM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
90,how is spawn supposed to be called?,"I cannot get this simple program to work:
it should initialize slave nodes and the spawn a process on each.

http://dfa.imn.htwk-leipzig.de/cgi-bin/gitweb.cgi?p=skpp-ws12.git;a=tree;f=cloud;h=4c6f7b88a96688f9edf5b6ca8060b2bba183233f;hb=378d1a82df411f65d72fc1774b38a6cc3f6f8727

from the main program, the spawn looks OK (it returns with a valid Pid)
but I never get any indication that any slave would be doing any work.
""make test"" gives this, and then it hangs, with idle CPU (and then I kill it)

```
./Bitcount slave localhost 8081  &
./Bitcount slave localhost 8082  &
./Bitcount slave localhost 8083  &
./Bitcount slave localhost 8084  &
sleep 1
./Bitcount master localhost 8080 20
Sun Dec 16 21:56:41 UTC 2012 pid://localhost:8080:0:3: Slaves:
[nid://localhost:8081:0,nid://localhost:8082:0,nid://localhost:8083:0,nid://localhost:8084:0]
Sun Dec 16 21:56:41 UTC 2012 pid://localhost:8080:0:3: pid://localhost:8081:0:4
Sun Dec 16 21:56:41 UTC 2012 pid://localhost:8080:0:3: pid://localhost:8082:0:4
Sun Dec 16 21:56:41 UTC 2012 pid://localhost:8080:0:3: pid://localhost:8083:0:4
Sun Dec 16 21:56:41 UTC 2012 pid://localhost:8080:0:3: pid://localhost:8084:0:4
^Cmake: *** [start-master] Interrupt
```
I'm sure I'm missing something basic here about how this is supposed to work.

The API doc on ""spawn"" is sort of unhelpful: 
http://hackage.haskell.org/packages/archive/distributed-process/latest/doc/html/Control-Distributed-Process.html#v:spawn

* Spawn a process. - sure: ""spawn"" is in the name, and ""process"" is in the type
* For more information about Closure, see Control.Distributed.Process.Closure. - sure: this is already in the type.
* See also call. - I'd love to, indeed I am seeing it, since it is the following item, but it does not mention ""spawn"". Actually it does, in ""see also spawn"".

J.W.

",12/16/12 10:08:07 PM,12/17/12 11:49:15 AM,Task,,Closed,Fixed,jwaldmann,distributed-process-simplelocalnet Invalid,"Comment:hyperthunk:12/16/12 10:43:43 PM:

assigned","Comment:jwaldmann:12/16/12 11:55:21 PM:

It seems I forgot to make the correct remoteTable. So I put

```
-   let rtable = initRemoteTable 
+   let rtable = __remoteTable initRemoteTable 
```

http://dfa.imn.htwk-leipzig.de/cgi-bin/gitweb.cgi?p=skpp-ws12.git;a=tree;f=cloud;h=a49c94579cf92823b287975196df6a8a6fbd8b0a;hb=ea9e495f50dc8c97b6c2c34baf8c6bc53cd1707a

but even then the behaviour is strange:

```
make test
./Bitcount slave localhost 8081  &
./Bitcount slave localhost 8082  &
./Bitcount slave localhost 8083  &
./Bitcount slave localhost 8084  &
sleep 1
./Bitcount master localhost 8080 26 +RTS -N
Sun Dec 16 23:50:12 UTC 2012 pid://localhost:8080:0:3: Slaves: [nid://localhost:8081:0,nid://localhost:8082:0,nid://localhost:8083:0,nid://localhost:8084:0]
Sun Dec 16 23:50:13 UTC 2012 pid://localhost:8080:0:3: pid://localhost:8081:0:4
Sun Dec 16 23:50:13 UTC 2012 pid://localhost:8080:0:3: pid://localhost:8082:0:4
Sun Dec 16 23:50:13 UTC 2012 pid://localhost:8080:0:3: pid://localhost:8083:0:4
Sun Dec 16 23:50:13 UTC 2012 pid://localhost:8080:0:3: pid://localhost:8084:0:4
Sun Dec 16 23:50:13 UTC 2012 pid://localhost:8080:0:3: spawned all
Sun Dec 16 23:50:13 UTC 2012 pid://localhost:8081:0:4: slave here
Sun Dec 16 23:50:13 UTC 2012 pid://localhost:8081:0:4: (0,16777216)
Sun Dec 16 23:50:13 UTC 2012 pid://localhost:8081:0:4: 201326592
Sun Dec 16 23:50:13 UTC 2012 pid://localhost:8082:0:4: slave here
Sun Dec 16 23:50:13 UTC 2012 pid://localhost:8082:0:4: (16777216,33554432)
Sun Dec 16 23:50:13 UTC 2012 pid://localhost:8082:0:4: 218103808
Sun Dec 16 23:50:13 UTC 2012 pid://localhost:8084:0:4: slave here
Sun Dec 16 23:50:13 UTC 2012 pid://localhost:8084:0:4: (50331648,67108864)
Sun Dec 16 23:50:13 UTC 2012 pid://localhost:8084:0:4: 234881024
```

(only on 3 of the 4 slaves, processes are run, so the main program is blocked)
","Comment:hyperthunk:12/17/12 12:25:16 AM:

Hi there. That does seem odd - slave 3 appears not to have spawned properly now does it. I've tried to replicate this, but unfortunately I'm not able to run your code at all - it hangs before we even get to `say $ ""Slaves: "" ++ show slaves`. This *might* be a bug in distributed-process-simplelocalnet or more likely just some silly network configuration issue on my laptop.

The TCP backend works fine for me, in that I'm able to run all the tests. So..... might I suggest trying the TCP backend instead, and letting us know if you're still having problems?

In the meanwhile I'll file a bug about the issue I'm seeing with the simplelocalnet backend and will cross reference this issue, so hopefully we can get to the bottom of it.   ","Comment:hyperthunk:12/17/12 12:41:47 AM:

> In the meanwhile I'll file a bug about the issue I'm seeing with the simplelocalnet backend and will cross reference this issue, so hopefully we can get to the bottom of it. 

Looks like this could be an issue with UDP multicast on OSX, so I'm going to investigate a little more before submitting that issue. Whilst your test run is stuck like that, can you see slave node 3 running (using ps) and listening on the right port with netstat or something similar? Have you also checked your firewall/iptables/pf configuration to ensure that the port 8083 is accessible (and supports UDP multicast!) like the others?","Comment:jwaldmann:12/17/12 09:36:08 AM:

thanks for getting on this quickly.

Meanwhile I found that I can solve (work-around) the problem by waiting in main:

http://dfa.imn.htwk-leipzig.de/cgi-bin/gitweb.cgi?p=skpp-ws12.git;a=commitdiff;h=676198460c6052dbdafdaebe598fc9253f7171d7

I don't understand this - I assume receiveChan is blocking? But it looks like  startMaster  does something like forkIO, so the main program exits anway?

","Comment:edsko:12/17/12 09:48:06 AM:

I don't know if this is related, but the master process does not wait for the slave nodes to exit, so you might not see all log messages. In the version of `distributed-process-simplelocalnet` the master will wait for the slaves to print all their log messages.","Comment:jwaldmann:12/17/12 10:31:32 AM:

> the master process does not wait for the slave nodes to exit,

I dont' think that would be necessary in my example: the processes on the slaves do sendChan to the master, and the master should just wait for that (in receiveChan)? I was expecting the slaves keep running.","Comment:edsko:12/17/12 10:36:23 AM:

The logger process is a process running on the master node; when the slaves call `say`, they send a message to that logger process. When the master exits, however, that logger process might not yet have had a chance to print all log messages still in its mailbox. ","Comment:jwaldmann:12/17/12 10:45:36 AM:

> When the master exits,

so my question is: when exactly does the master exit? (can it exit while in receiveChan?)","Comment:edsko:12/17/12 10:51:50 AM:

No, `receiveChan` is blocking. However, your slaves finish with

    say $ show s
    sendChan input s

You might never see that `say` if the master exits before all log messages are printed.

This may be unrelated to your problem; but try with the `SimpleLocalnet` from github and see if it changes anything.","Comment:jwaldmann:12/17/12 11:08:32 AM:

> try with the SimpleLocalnet from github

I'd love to, but I don't dare to: install-all.sh gives me

```
Resolving dependencies...
In order, the following would be installed:
bytestring-0.9.2.1 (new version)
containers-0.4.2.1 (new version)
binary-0.6.4.0 (new version)
data-accessor-0.2.2.3 (reinstall) changes: containers-0.5.0.0 -> 0.4.2.1
...
cabal: The following packages are likely to be broken by the reinstalls:
...
```","Comment:edsko:12/17/12 11:14:13 AM:

Don't install all, just install `distributed-process-simplelocalnet`.","Comment:hyperthunk:12/17/12 11:20:01 AM:

> You might never see that say if the master exits before all log messages are printed.

This is really important to understand. The logger is running in another process, not the process that calls `say` so that printing to stdout is racing with the master. What @edsko is saying is that the master can exit before the logger process gets a chance to flush all IO. 

We *could* provide a `flush` call for the logger process that walks the mailbox until `expectTimeout 0 == Nothing`, though even that will be a bit racy because there's no guarantee that the logging infrastructure will have synchronised by that time either.

@jwaldmann  - if you replace `say` with a printf (or whatever) that writes to stdio synchronously (in the calling) thread then this might go away, though you've still got buffering to take into account, so I suspect a sleep/threadDelay at the end of main will *still* be required.

This, BTW, has cropped up on the erlang-questions mailing list gazillions of times. Making sure that I/O gets flushed in an concurrent environment is a surprisingly tricky business.","Comment:edsko:12/17/12 11:20:01 AM:

mentioned","Comment:jwaldmann:12/17/12 11:20:01 AM:

mentioned","Comment:jwaldmann:12/17/12 11:20:12 AM:

this still claims to be 0.2.0.8, right? agains distribute-process-0.4.1 from hackage I get:

```
src/Control/Distributed/Process/Backend/SimpleLocalnet.hs:138:5:
    Module `Control.Distributed.Process' does not export `try'
```","Comment:edsko:12/17/12 11:25:34 AM:

> What @edsko is saying is that the master can exit before the logger process gets a chance to flush all IO.

Not just flush -- it might even be terminated before it even gets to process all messages in its queue. @simonmar modified this in `SimpleLocalnet` however.","Comment:edsko:12/17/12 11:25:34 AM:

mentioned","Comment:simonmar:12/17/12 11:25:34 AM:

mentioned","Comment:jwaldmann:12/17/12 11:26:03 AM:

> if you replace say with a printf

yes, this works. - Can you please put a big warrning in the API doc for ""say"" then ....","Comment:edsko:12/17/12 11:27:20 AM:

> if you replace say with a printf

I think that if that works it's an ""accident"", similar to the `threadDelay`. See my earlier comment. 

","Comment:jwaldmann:12/17/12 11:30:11 AM:

ultimately the print in the master is useless (non-compositional) anyway, so the master process should export the value in a reliable manner (to the ""main programm"") - via an MVar I guess","Comment:hyperthunk:12/17/12 11:31:17 AM:

Yes good point @edsko. @jwaldmann - we need to add some blurb to `say` about the logger process implementation anyway. The real reason behind this probably has more to do with the way `SimpleLocalNet` relays logging messages back to the master.

> ultimately the print in the master is useless (non-compositional) anyway, so the master process should export the value in a reliable manner (to the ""main programm"") - via an MVar I guess

Right exactly! That's a much better approach and then you can print the result in main if that's required.","Comment:edsko:12/17/12 11:31:17 AM:

mentioned","Comment:jwaldmann:12/17/12 11:31:17 AM:

mentioned","Comment:edsko:12/17/12 11:32:39 AM:

To be honest, ""logging"" in in its current state is very fragile. What we really need is a better logging infrastructure on top of the core libraries. 

The problem with documenting `say` better is that it's behaviour will depend on what the specific backend does -- unlike in Erlang, where there is no such choice.","Comment:edsko:12/17/12 11:33:27 AM:

Anyway, with @simonmar's changes to `SimpleLocalnet` this behaviour will be less confusing.

> Module `Control.Distributed.Process' does not export `try'

Uh, this is confusing. Maybe also install `distributed-process`, might be some API changes.","Comment:simonmar:12/17/12 11:33:27 AM:

mentioned","Comment:hyperthunk:12/17/12 11:33:31 AM:

So guys ..... is there actually a bug here, in `SimpleLocalNet` or elsewhere? I don't think so, therefore I'm tempted to close this issue and open up a few new ones

* better documentation of how the logger process (i.e., `say`) works and its semantics
* figuring out what's wrong (if anything) with `SimpleLocalNet` on OSX - I suspect this is a name resolution issue

Does that sound reasonable?","Comment:edsko:12/17/12 11:34:17 AM:

I'm not convinced that there is a bug, other than documentation perhaps (esp given @simonmar 's changes).","Comment:simonmar:12/17/12 11:34:17 AM:

mentioned","Comment:jwaldmann:12/17/12 11:39:35 AM:

agree, it's more like missing documentation (or missing insight on my part). Thanks for the explanations.","Comment:hyperthunk:12/17/12 11:40:02 AM:

> To be honest, ""logging"" in in its current state is very fragile. What we really need is a better logging infrastructure on top of the core libraries.
> 
> The problem with documenting say better is that it's behaviour will depend on what the specific backend does -- unlike in Erlang, where there is no such choice.

Yes good points both. Although I notice that currently the logger process is implemented in `Node.hs` which doesn't seem right. Is that just something that gets overridden by the backends themselves or something?

Either way we'll need to say *something* in the `say` documentation about how this falls together. 

> I'm not convinced that there is a bug, other than documentation perhaps (esp given @simonmar 's changes).

>> agree, it's more like missing documentation (or missing insight on my part). Thanks for the explanations.

Agreed. Thanks guys.","Comment:simonmar:12/17/12 11:40:02 AM:

mentioned","Comment:hyperthunk:12/17/12 11:40:02 AM:

closed","Comment:edsko:12/17/12 11:42:42 AM:

> Although I notice that currently the logger process is implemented in Node.hs which doesn't seem right.

My preference would be not to offer `say` in the core library at all, it's there mostly for backwards compatibility. The implementation in `Node.hs` is there to be able to provide a default that does *something*. Backends override this behaviour by re-registering the logger process in the registry.","Comment:hyperthunk:12/17/12 11:49:15 AM:

**Right** - I'm going to file a bug to remove `say` altogether. I'll file another one to discuss whether we should have distributed-process-logging or whether the existing distributed-process-platform should provide a logging implementation/infrastructure like OTP does.

Do any of the existing backends apart from `SimpleLocalNet` currently do any overriding of the logger process? Obviously if we move the functionality out into another place, we'll need to continue to support customisations such as the relay functionality offered there. In OTP the logging infrastructure uses gen_event, which allows you to register/unregister multiple event handlers making it unnecessary to replace the entire logger process.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
91,"sending Int32, Int64 seems broken","When I replace Int by Data.Int.Int32 or Data.Int.Int64, I get  out-of-memory at runtime:

http://dfa.imn.htwk-leipzig.de/cgi-bin/gitweb.cgi?p=skpp-ws12.git;a=tree;f=cloud;h=be068913876db478e721cecf80947f451b63c354;hb=b3f8ba674bde08da7d2db7b477e0b7b55f13b560

```
Bitcount: Bitcount: Bitcount: Bitcount: out of memory (requested 1048576 bytes)out of memory (requested 1048576 bytes)
out of memory (requested 1048576 bytes)

out of memory (requested 1048576 bytes)
```
",12/17/12 11:47:13 AM,12/18/12 01:15:09 AM,Task,distributed-process-0.5.0,Closed,Fixed,jwaldmann,Wontfix Invalid,"Comment:hyperthunk:12/17/12 11:51:44 AM:

What does the `Binary` instance for `Data.Int.Int32` look like?","Comment:jwaldmann:12/17/12 02:22:49 PM:

binary-0.5.1.1 (comes with ghc-7.6.1)

```
-- Int64s are written as a 4 bytes in big endian format
instance Binary Int64 where
    put i   = put (fromIntegral i :: Word64)
    get     = liftM fromIntegral (get :: Get Word64)
```

seems to work in ghci
```
Prelude> import Data.Binary
Prelude Data.Binary> import Data.Int
Prelude Data.Binary Data.Int> decode ( encode (42 :: Int64) ) :: Int64
Loading package array-0.4.0.1 ... linking ... done.
Loading package deepseq-1.3.0.1 ... linking ... done.
Loading package bytestring-0.10.0.0 ... linking ... done.
Loading package containers-0.5.0.0 ... linking ... done.
Loading package binary-0.5.1.1 ... linking ... done.
42
```
","Comment:hyperthunk:12/17/12 02:45:33 PM:

Not sure about this, but certainly looks like a bug => updating label.","Comment:jwaldmann:12/18/12 12:02:29 AM:

OK, this is weird, but it is not the fault of C.D.P. I distilled this from my example program:

```
import Data.Int
import System.Environment

bitcount x = if x > 0 
    then let (d,m) = divMod x 2 in  bitcount d + m
    else 0

main = do
    [ arg ]  <- getArgs
    print $ sum  $ map bitcount
          [ 0 :: Int64 .. 2^read arg - 1 ]
```

this gives me Stack Overflow for arguments 18 ..24, and Out of memory for 25.

I guess some loop unrolling/fusion is not happening for Int64 (but it is, for Int).
","Comment:hyperthunk:12/18/12 01:15:09 AM:

Thanks for boiling it down and confirming it's not a CH issue - much appreciated.","Comment:hyperthunk:12/18/12 01:15:09 AM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
92,Move logging infrastructure out of the core,"I would propose that we provide the basic infrastructure to register logging handlers in distributed-process-platform and build on that to add additional capabilities. Another option is to create a new distributed-process-logging package, but I'd **really** like to build this on top of gen-server/gen-event stuff we're building into -platform personally.

The only **problem** I can see with that approach is for example with `SimpleLocalNet` which replaces the `logger` process on its slaves with one that relays the messages back to the master. Erlang does something similar in its [slave module](http://www.erlang.org/doc/man/slave.html) by redirecting all stdout back to the master.

The obvious solution I can see to this, is to go ahead and move the logging infrastructure, then take the master/slave stuff *out* of `SimpleLocalNet` and put it into another package that depends on `SimpleLocalNet` **and** on -platform. That's still not quite a clean API design though, because there's an implicit dependency on a specific backend which seems unusual to me.

If someone can come up with an alternative that still allows backends to provide this kind of functionality without creating a tangled mess of dependencies then I'm all ears. Currently the backends do not seem to depend on distributed-process at all, which makes sense. I can't see how we'd do CH logging without the `Process` monad so I'm questioning whether it really makes sense for backends to have this capability at all?

Perhaps this requires some thought about the packaging requirements for CH and how we go about making it easy for developers to work with.",12/17/12 12:19:35 PM,01/27/13 08:57:28 PM,Task,distributed-process-0.5.0,Open,Unresolved,hyperthunk,Feature-Request distributed-process,"Comment:hyperthunk:12/17/12 12:20:05 PM:

BTW - I'm quite happy to pick up the grunt work on this once we've made some decisions about how it should work.","Comment:edsko:12/17/12 12:22:35 PM:

I'm perfectly happy to for you to take stuff out of `SimpleLocalnet`. It's a quick-and-dirty backend to get people going, it should not influence design decisions.","Comment:hyperthunk:12/17/12 12:32:28 PM:

> I'm perfectly happy to for you to take stuff out of SimpleLocalnet. It's a quick-and-dirty backend to get people going, it > should not influence design decisions.

Cool, so what remains to be decided is whether we're comfortable taking away the ability of backends to override the logging. If that **is** the case, then we can simply strip things out of the core library (and simplelocalnet) and then open up the relevant ticket(s) in distributed-process-platform.

If we *do* decide to do that, then I wonder whether we could/should consider what the role of `startServiceProcesses` should be. There's an [open ticket](https://github.com/haskell-distributed/distributed-process-platform/issues/31) in -platform to look at the [OTP Application](http://www.erlang.org/doc/design_principles/applications.html) concept. The role of an application controller is very tightly integrated into the behaviour of an Erlang node, but I'm inclined to keep that in -platform and remove the 'service process' concept from CH altogether.","Comment:hyperthunk:12/17/12 12:33:56 PM:

We should also consider the impact that removing `say`, the built-in logger process and the start{Slave,Master} APIs from `SimpleLocalNet` will have on example code and blog posts. That should be factored in, even though it shouldn't necessarily be the basis for decision making either.","Comment:edsko:12/17/12 12:36:35 PM:

I would say ideally logging is independent of the backend.","Comment:hyperthunk:12/17/12 12:43:49 PM:

> I would say ideally logging is independent of the backend.

That's my opinion too. Unless @dcoutts and/or @simonmar have major objections then, I suggest we go ahead with this change (in a branch for now, of course!) and then before closing this issue, we will need to

1. remove the logging relay code for start{Slave, Master} from `SimpleLocalNet`
2. break the `say` API
3. remove the `serviceProcess` code from `Node.hs`
4. put something in the forthcoming release notes
5. notify the parallel-haskell mailing list

Then after we've set up a new logging API, we can pull start{Slave, Master} out of `SimpleLocalNet` and re-implement the master/slave logging relay on top of the new API in -platform.","Comment:dcoutts:12/17/12 12:43:49 PM:

mentioned","Comment:simonmar:12/17/12 12:43:49 PM:

mentioned","Comment:simonmar:12/18/12 11:05:09 AM:

@hyperthunk the reason I've been making changes and getting involved in the development here is that I'm writing a chapter about distributed-process for the O'Reilly book on Parallel and Concurrent Haskell. The first thing I encountered with my examples was that `say` messages got lost, so I fixed that.

I agree that the right thing to do is to move the logging functionality out of d-p and simplelocalnet. However, my main concern is that I don't have to completely rewrite the examples - as long as it is easy to get hold of `say` from somewhere, and have it just work, that will be fine with me. We have a few weeks before the book is due to be ready, so you have until then to decide on the API :)","Comment:hyperthunk:12/18/12 11:05:09 AM:

mentioned","Comment:hyperthunk:12/18/12 11:41:42 AM:

> We have a few weeks before the book is due to be ready, so you have until then to decide on the API :)

He he he - ok no problem. I'm going to suggest to the guys that we either make `master` the stable/live branch or use it as the development branch and branch from the last release (tag) to create a line called 'live' or 'stable'. Any API breaking changes (i.e., anything that's not a bugfix) will get merged into 'master' (or development if we go that way instead) and so you'll still have `say` sticking around for many months to come before we do the next major release.

In the future, we'll provide `say` in distributed-process-platform, though obviously there will be a bit more available in that API too but we'll keep the basic *log this message to whatever default (i.e., console) setup is available). So *even then* hopefully your examples will continue to work, though obviously at that stage the -platform library will need to be installed too.

**Really** looking forward to the new book BTW! :D","Comment:hyperthunk:01/27/13 08:57:28 PM:

So, I've come to the conclusion that `say` should remain, as I think any *logging infrastructure* that does get built up will reside in d-p-platform. We should have *some* kind of basic logging support in the core d-p library though, and `say` and its registered `logger` process is good enough for that. It's also possible to configure the new `trace` API to write to the logger process, which is quite handy. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
93,Development Process and Branching Model,"I want to start a *hacking* guide on the wiki. First and foremost I'd like to be able to give some sensible direction about where to branch new features and/or bug fixes from.

Currently it looks like 'dev' branch has been forgotten and all changes are being merged into master. I'm comfortable using master as a development branch, but we should have an alternative live/stable branch in that case, so that bugfixes can be branched off without pulling in lots of experimental and possibly API breaking fixes.

What we're doing for distributed-process-platform is

* master = stable/live, bugfixes branch from here
* development = next-release, features branch from here

I realise there's a bit more overhead in maintaining an additional stable branch, but personally I think it's worth it. We could, of course, leave this change until after our next release to hackage, giving us a bit of breathing space to stabilize everything.

I'd also like to set up some milestones on the issue tracker, without specifying any dates. This gives us a chance to collaborate around which bugs/features go into the next planned release and gives users at least a vague sense of our roadmap.   ",12/17/12 12:39:29 PM,01/27/13 03:54:53 PM,Task,Misc/Admin/Support,Closed,Fixed,hyperthunk,distributed-process In-Progress,"Comment:edsko:12/17/12 12:41:38 PM:

I think it makes sense to unify the two workflows for `distributed-process` and `-platform`, I'm happy with the above.","Comment:hyperthunk:12/17/12 12:44:40 PM:

Good - thanks for the quick response @edsko. Let's give the others a few days to shout before making any changes. I'll work up some wiki pages in the meantime.","Comment:edsko:12/17/12 12:44:40 PM:

mentioned","Comment:hyperthunk:01/04/13 02:06:01 PM:

Ok this change will hopefully happen over the next week or so - I'm aiming to do it over the wkend but we'll see. I will move both repos to the same model and update the wiki here with the contributor guide, linking to it from d-p-platform.","Comment:hyperthunk:01/26/13 11:27:22 PM:

assigned","Comment:hyperthunk:01/26/13 11:27:51 PM:

I'm going to push 0.4.2 out tomorrow - unless someone shouts in the meanwhile - and I plan to put this in place thereafter.","Comment:hyperthunk:01/27/13 03:54:53 PM:

Done. We've not got a maintainers guide on the wiki too, outlining how this works.","Comment:hyperthunk:01/27/13 03:54:53 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
94,Contributors Guide,"I've set one up for distributed-process-platform already. I'd like to use the same here. Perhaps they should both link to a single resource instead, either moving that contributors guide here and linking from -platform *or* putting the guide on the web site and linking from both locations to there.",12/17/12 12:52:21 PM,01/27/13 08:55:40 PM,Task,Misc/Admin/Support,Closed,Fixed,hyperthunk,website Documentation,"Comment:hyperthunk:01/04/13 05:41:27 PM:

Style-guide has been added. I want to adapt the [contributor's guide from d-p-platform](https://github.com/haskell-distributed/distributed-process-platform/wiki/Contributing) so there will be some duplication I'm afraid.","Comment:hyperthunk:01/27/13 08:55:40 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
95,Website,"It would be nice if the haskell-distributed organisation had a sexy looking web site that links off to CH and -platform and so on.

If we use Jekyll and github pages for this, then we'd have the benefit a nice blog engine to boot. I'm inclined to use the same theme as http://lens.github.com/, for which the Jekyll sources are at https://github.com/lens/lens.github.com. I'll have to contact the author to check for copyright details on the theme and strip out any images and such things that aren't in the public domain.

If we do this, then I'll subsume http://haskell-distributed.github.com/distributed-process-platform/ into the new site (and just provide a link) and maintain all CH related stuff on the new site.

I'm not sure, BTW, what permissions are required to configure a github pages site for this organisation. I suspect I don't have the access rights, although I **do** have permission to fork the lens website repo to this org, though I don't really want to do that as I'd rather hack out the theme and set up the initial content first.",12/17/12 12:58:17 PM,01/04/13 04:03:39 PM,Task,Misc/Admin/Support,Open,Unresolved,hyperthunk,website,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
96,please improve API doc for send/receiveChan (capacity? order?),"A ""channel"" can be one of many things, and the API docs don't say what's happening:

* does it have a capacity? (can it store messages?)
* if yes, what guarantees are there for message ordering? (is it FIFO in some concurrent sense, that is FIFO between a fixed pair of sender and receiver process?)

and the same for send/expect - with the extra difficulty that the underlying channel is untyped? What does this imply for message ordering (e.g., FIFO-by-type)?

You have some ""INTERNAL NOTES"" in C.D.Process - please make them visible in the Haddocks.",12/17/12 06:33:46 PM,01/08/13 03:41:05 PM,Task,distributed-process-0.5.0,Open,Unresolved,jwaldmann,Documentation,"Comment:hyperthunk:12/17/12 06:53:14 PM:

> A ""channel"" can be one of many things, and the API docs don't say what's happening:

You're right, and we are going to improve the documentation for all of CH. A *channel* is backed by a module implementing a subset of `Control.Concurrent.STM.TQueue` features.

> does it have a capacity? (can it store messages?)

Yes, see above.

> if yes, what guarantees are there for message ordering? (is it FIFO in some concurrent sense, that is FIFO between > a fixed pair of sender and receiver process?)

That's a bit tougher to answer. My understanding is that this is a FIFO queue but there are no ordering guarantees between sender and receiver that I can see, because incoming requests to the node controller to enqueue messages in the channel will race.

> and the same for send/expect

This is a bit easier. Messages between two processes should arrive in FIFO order, if at all. Beyond that there are no guarantees about ordering (where other processes are sending messages as well).

> with the extra difficulty that the underlying channel is untyped

That doesn't really make any difference AFAIK.

> What does this imply for message ordering (e.g., FIFO-by-type)?

The thing to bare in mind, with the guarantees given thus far, is that the receive can *choose* to `receive` some messages before others, filtering them by type or by some arbitrary predicate using `matchIf`. One can also filter to receive messages from *either* the primary mailbox (i.e., the untyped channel) or one or more typed channels.

I will look at improving the documentation in this area and others as we work on getting CH production ready!","Comment:jwaldmann:12/17/12 08:04:04 PM:

> FIFO between a fixed pair of sender and receiver 

I meant the following: for one channel: if P1 .. Pm are writers, and Q1 .. Qn are readers, and if Pi sends A and then  B, and Qj reads both A and B, then it will read them in that order. 

> untyped .. no difference

well - say A :: T and B :: S, but the receiver expects type S, then expexts type T. Then it could read B before A? Or does this just deadlock? (I don't think so, and a simple experiment seems to confirm.)

can we say in summary: the messages are held in a queue, and any receive/expect removes the first message that fulfils the criterion? (the API docs often say: read ""any"" message. But it is really ""first"" here?)","Comment:hyperthunk:12/17/12 09:20:43 PM:

> well - say A :: T and B :: S, but the receiver expects type S, then expexts type T. Then it could read B before A? Or > does this just deadlock? (I don't think so, and a simple experiment seems to confirm.)

Yes, the selective receive construct will remove B before A. The other message (A) will remain in the mailbox and the ordering should be preserved.

> the API docs often say: read ""any"" message. But it is really ""first"" here?

If your mailbox is `Cons a (Cons b (Cons c Nil))` and you do a selective receive that matches on either `b` or `c` then you will get `b` as the result - the first match - then a second receive will return `c`. The remaining mailbox will contain `a` as you would expect. Does that make sense? How might the wording of the API docs better describe that? And @edsko - can you confirm I'm telling the truth here!? :)

Cheers ","Comment:edsko:12/17/12 09:20:43 PM:

mentioned","Comment:hyperthunk:12/19/12 10:10:44 PM:

Actually I should correct myself on something here. In a sender receiver pair there *is* a FIFO ordering guarantee. But between more than two processes there aren't any such guarantees.","Comment:edsko:12/20/12 09:25:42 AM:

This is documented in great detail in the [semantics](https://github.com/haskell-distributed/distributed-process/blob/master/doc/semantics/CloudHaskellSemantics.pdf). The basic rule is: 

> Messages from `Proc` to `Id` are delivered in order (with `Proc` a process and `Id` either process, a channel (sendport), or a node). *No* ordering guarantees exist for messages sent by dfiferent senders or sent to different receivers.

The fact that you can extract messages in different order from how they are delivered to the mailbox or the channel is orthogonal to that definition.

Note that we will probably want to strengthen this rule so that

> All messages from process `P` to process `Q` are ordered (whether they are sent directly to `Q`s mailbox or to one of `Q`s channel)

Without this guarantee it becomes very difficult to write robust algorithms (for instance, a monitor message might ""overtake"" a message sent on a channel). This is still under debate. However, if you are using the TCP transport (which, currently, is everybody) then we *do* have the stronger guarantee.

By the way, although @hyperthunk is right that a channel is currently backed by STM, I don't think that this is part of its API per say (in other words, I'm not sure whether this is just an implementation detail or whether we want to make this part of the spec).","Comment:hyperthunk:12/20/12 09:25:42 AM:

mentioned","Comment:hyperthunk:12/20/12 10:23:13 AM:

> By the way, although @hyperthunk is right that a channel is currently backed by STM, I don't think that this is part of its API per say (in other words, I'm not sure whether this is just an implementation detail or whether we want to make this part of the spec).

Good point. It's almost certainly an implementation detail I guess - I should've probably stuck to describing the semantics but explain how it was implemented was easy (i.e., lazy). I'll try not to do that in future! :)","Comment:hyperthunk:12/20/12 10:23:13 AM:

mentioned","Comment:hyperthunk:01/08/13 12:45:26 PM:

>>    All messages from process P to process Q are ordered (whether they are sent directly to Qs mailbox or to one of Qs channel)
>
> Without this guarantee it becomes very difficult to write robust algorithms (for instance, a monitor message might ""overtake"" a message sent on a channel). This is still under debate. However, if you are using the TCP transport (which, currently, is everybody) then we do have the stronger guarantee.

I'm not entirely convinced this is worth worrying about unless you're using @simonmar's new `matchChan` primitive. How else would you receive channel messages in conjunction with regular messages, given that channel messages are handed to their `ReceivePort` completely independently of the regular (untyped) mailbox? Of course your point about TCP preserving the ordering is correct, but that is almost irrelevant when talking about ordering over channels *and* mailbox entries: the order in which you `receive` will be determined by the order in which the `match h, matchChan h2` list of rules is provided will it not? And if you do not grab messages using `receive [ match a, matchChan b, match c ]` then there is no correlation between the process mailbox and channels whatsoever.

Unless we were to handle incoming channel messages differently it would be difficult to make this property hold. I can see the argument for wanting to have channel messages ordered along with *regular* messages, but it's going to make the mailbox implementation even more complicated and hamper things like issue DPLEGACY-105 and more importantly DPLEGACY-43 . And will we preserve ordering over multiple channels without using mergePorts et al in that case, given that we're trying to preserve total ordering over multiple input streams?

I think the simple solution here is to recognise that ordering is preserved for the process mailbox and ordering on channels is preserved *per channel* but that's it. If you are using channels for input data *and* using monitors, then you've got to manage the potential races yourself. Better to keep processes which use monitors focussed on the mailbox and use typed channels where you have a specific fixed domain of inputs across one or more channels and manage these yourself. Of course I'm willing to be persuaded otherwise, but currently I suspect the costs outweigh the benefit.","Comment:simonmar:01/08/13 12:45:26 PM:

mentioned","Comment:edsko:01/08/13 12:48:08 PM:

No, I disagree. Ordering is very important. For instance, I might want to wait for a reply value from a server on some typed channel, but at the same monitor that server so that I don't wait indefinitely when the server dies or I lose my connection to the server. This requires `matchChan`, granted, but that's precisely what it's for. But now ordering becomes very important: if the server terminates after replying to me, then I want to be sure that the monitor notification that the server is now dead does not overtake its reply to me.","Comment:hyperthunk:01/08/13 02:03:24 PM:

@edsko - you make a compelling argument for ordering there. I'd be inclined to use a middleman for the monitoring and make the reply channel typed as `data Reply = ActualReply T | NoReply DiedReason` or some such. Whilst this introduces some non-determinism, it simplifies the caller somewhat and is no less dependent on the NT layer's guarantees than what we have using `receiveWait [ match m, matchChan c ]` afaics.

> I want to be sure that the monitor notification that the server is now dead does not overtake its reply to me.

My reading of `CQueue` right now is that the order in which the matches are given has priority if there is data already in the mailbox. Once we go into blocking mode waiting for the incoming channel we still fold over the matches once we've received something. The use of STM means that each match that fails causes a retry of the remaining/next matcher until we've actually received and handled something.

Using the current implementation, I don't see how we could hold the guarantee for channels too without reliable ordering in the delivery mechanism. Right now we're completely relying on the system queue receiving messages in the expected order. If the system queue delivered the monitor signal first then the code in `waitIncoming` would pick that up and hand it over as the result of `dequeue`. There's nothing in `CQueue` that attempts to preserve ordering over typed channels and the process mailbox and you'd need to merge all the typed channels input streams into the `incoming` stream in order to have that afaict. Even then the network layer could break things if it was unreliable. That doesn't sound like a fun task, though I suppose it's plausible.

I guess this is the reason for your comment:

> However, if you are using the TCP transport (which, currently, is everybody) then we do have the stronger guarantee.","Comment:edsko:01/08/13 02:03:24 PM:

mentioned","Comment:edsko:01/08/13 02:08:56 PM:

Yes, the network.transport layer guarantees ordering per lightweight channel. At the moment CH uses a lightweight channel for the main mailbox, and then other lightweight channels for each typed channel that is created. The TCP transport multiplexes all these lightweight channels to the same TCP connection, which is why we have stronger ordering guarantees. Other transports (such as a UDP transport) may do this differently.

What needs to happen (once it is agreed that these ordering guarantees are important) is that CH uses a single NT lightweight channel for each destination endpoint. This doesn't change the NT layer at all, just how it's used at the CH layer. ","Comment:hyperthunk:01/08/13 03:41:05 PM:

Ah ok I see. I hadn't picked up on that aspect of the wiring between the two, but now you mention it I can see that from the node controller code. This should not then, affect the `CQueue` implementation at all, as it'll be dealt with at the node controller by switching to a single lightweight channel for all comms! Nice.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
97,Threadscope integration,"Not sure what this would look like but it was originally logged for -platform and I think if it has legs it's better suited here.

Presumably there's not much to do, as a process is implemented using forkIO already. Do we want to provide stats (such as issue #89 provides) there as well, or would it be better to expose them in some other bespoke tool?",12/18/12 01:18:51 PM,01/23/13 10:04:35 PM,Task,Horizon,Open,Unresolved,hyperthunk,Feature-Request distributed-process,"Comment:hyperthunk:12/18/12 01:55:28 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
","Comment:hyperthunk:01/23/13 10:04:35 PM:

I don't think we need to do anything special here. I suppose we could associate process identifiers with thread ids, but most of what Threadscope does will just work, as a process is nothing more than a forkIO thread at heart.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
98,Statistics for Network.Transport,"These would obviously be implementation dependent, so the API is likely to be rather loosely defined with `String` keys and so on. Making statistics available is likely to be very useful to administrators, particularly when trying to optimise the network topology for a distributed application. ",12/18/12 01:44:13 PM,01/04/13 03:58:15 PM,Task,Horizon,Open,Unresolved,hyperthunk,Feature-Request network-transport,"Comment:hyperthunk:12/18/12 01:48:16 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
99,Network.Transport.Management,"With any external resource, chances are things are going to go wrong sometimes. Following on from issue #98 there are various situations which might require manual intervention. Shutting down a bundle of connections/channels between two specific endpoints, forcibly terminating an endpoint or even shutting down a whole transport might be required if connectivity drops in such a way that things get stuck.

Having an API for this would enable us to write tools (command line, web based, etc) to assist with manual administration of a distributed system. The API needs to support the primary use case where an administrator connects to a running node from an external location and can take actions from there.

There needs to be some *entry point* to which the external client connects, and of course there is no such concept in `Network.Transport`. I can see two ways to go about this, though there may be other possibilities too.

1. couple the functionality with the node controller
2. provide a service registry as part of `Network.Transport` itself

The point here is that in any running executable, we need some means by which we can connect in order to query for this information. As the node controller already provides this, it seems a sensible choice at first glance. The node controller is initialised with a `Transport` so it can use the `Network.Transport` APIs to handle requested interactions. 

So does it make sense to force all the interactions to go through the node controller? Another alternative would be to have a registered *service process* that gets booted with each node controller, and use `nsend` to talk to this process instead. Either way, the API data needs to reside in core CH so that the nodes can communicate effectively without sharing the same image.

Providing some kind of service registry for the `Network.Transport` itself is probably wrong. We'd need to provide an access point to the outside world and it seems crazy not to use the node controller(s) for this. I suppose one way of doing this would be to have the backends open up an additional management port and use a separate control channel for *management messages* - not sure what I think about forcing that on all backends though, and as @edsko mentioned elsewhere we're trying to keep actual functionality out of the `Network.Transport` layer and push it to the implementations. Forcing each implementation to write code to handle management requests seems wrong.

One *problem* with using the node controller as the entry point for a management (and/or stats gathering) API is that you need to know which backend is in use. As an administrator I guess you should know that anyway, so maybe it's not a problem.

I also think that we should put a secure HTTP based API around this, so that you can open up the management capabilities without having to make connectivity possible. For example, you might not *want* to expose the node outside your LAN, but allow administration to take place over the internet providing TLS is in play. *That* probably belongs either in a separate top-level project, or in -platform, possibly bundled with other functionality into a single management web interface.",12/18/12 01:48:16 PM,01/07/13 07:01:05 PM,Task,Horizon,Open,Unresolved,hyperthunk,Feature-Request network-transport,"Comment:robstewart57:12/18/12 02:31:21 PM:

I'm not sure if this sits in this issue...

I'd like to further this by suggesting that some autonomous management should be done within the transport layer, to deal with massive scalability. As I discovered [http://www.haskell.org/pipermail/haskell-cafe/2012-February/099212.html] , there are operating system limits on the number of open TCP connections --- by default 1024. If we are thinking at scales of 1,000's of nodes, then connecting every endpoint to every other endpoint is problematic.

There is a case to say that this concern is not one for the programmer to deal with. A solution would be have the transport on each node manage the connections on each of its endpoints. Heuristics might include killing heavyweight connections to remote endpoints that haven't been used in a time limit, killing old connections when new connection requests are made and a connection limit (e.g. `ulimit`) isn't far from being breached etc.. 

I'm not sure whether this is a parameter set when creating transports or endpoints, but such connection management should probably not be a concern of the programmer.. (?)","Comment:hyperthunk:12/18/12 04:05:30 PM:

Hi @robstewart57 - first of all, thanks for taking an interest in Cloud Haskell. It's very important that we build a strong community around CH and getting feedback as well as community participation is vitally important to us.

> I'm not sure if this sits in this issue...

Indeed I think this issue deserves a separate thread all of its own. I'm going to address your comments and start the discussion going over in issue DPLEGACY-101. Please do come and join in over there!","Comment:robstewart57:12/18/12 04:05:30 PM:

mentioned","Comment:hyperthunk:12/18/12 04:31:35 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
","Comment:edsko:12/19/12 01:23:35 PM:

mentioned","Comment:edsko:01/07/13 06:41:18 PM:

Without having thought about this too hard (so I could be wrong) I would recommend to do as much as you can at the level of CH and as little as possible at the level of NT. Transports are tricky to implement and should be kept as focused as possible. ","Comment:hyperthunk:01/07/13 07:01:05 PM:

Yes I agree with that in principle. I suspect we can do all of this in the node controller or with a service process next door to it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
100,Distributed.Process.Management Toolset,"In line with issue #97 there are times when you'd like to get an overview of running processes in your system. Issue #89 is already starting to provide some of the primitives required to achieve something like this. There are other things a system administrator might want to do, such as manually running `exit` to `terminate` on a given process, set up links/monitors or even manually injecting messages (although that last one's a bit far-out).

Whilst the idea behind `Network.Transport.Management` is just about providing an API so we can build tools, the tools for **this** feature already exist by and large (viz getProcessInfo, exit, kill, link, etc) so this issue is really about providing the *actual* tools themselves.

I suspect that a command line admin interface would be best suited to this, at least to begin with. Whether or not that is something that takes parameters and runs specific commands, or something more interactive is up for debate. Again, this is the kind of thing that makes administrators and tech support *love* rather than *hate* your application. 

I suspect this should be a top level project. Whether it belongs here or bundled in with -platform is another thing to consider.",12/18/12 01:55:28 PM,12/18/12 04:34:57 PM,Task,,Closed,Fixed,hyperthunk,Feature-Request distributed-process,"Comment:hyperthunk:12/18/12 04:33:44 PM:

> I suspect this should be a top level project. Whether it belongs here or bundled in with -platform is another thing to consider.

I'm going to jump in before anyone else! Thinking about it, this would sit better in -platform land, and might even warrant being in a separate repo by itself. I'm a bit dubious about creating zillions of separate repositories, because of the overhead of managing all the individual version numbers and dependencies. But I think this is definitely not core CH, so I'm going to pull it over to distributed-process-platform. Closing...","Comment:hyperthunk:12/18/12 04:34:57 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
101,Support for large distributed networks,"This was first raised as a question on issue #99. This is a common problem in distributed systems. 

Distributed Erlang systems require a fully connected network of nodes, which places considerable strain on system resources. As a professional Erlang programmer, I've seen anecdotal reports of distributed Erlang systems with thousands of nodes, though these apparently start to come unstuck beyond 1000 or so nodes.

In issue #99 @robstewart57 mentioned operating system limits on open TCP connections, and I'd like to discuss that in a a bit more detail.

> As I discovered [http://www.haskell.org/pipermail/haskell-cafe/2012-February/099212.html] , there are operating system limits on > the number of open TCP connections --- by default 1024. If we are thinking at scales of 1,000's of nodes, then connecting every > endpoint to every other endpoint is problematic.

As the respondent in that thread points out, the 1024 file descriptor limit is an artefact of the unix `select` system call, which can handle only that many descriptors at a time. Modern non-blocking I/O system calls such as `poll` and `epoll` on linux, `kqueue` on BSD variants and AIO capabilities on other operating systems can support a **much** higher number of open file descriptors in practice.

There **are** still limits however. Forcing all nodes to be fully interconnected carries a significant overhead, especially in a connection oriented transport layer protocol such as TCP.

> There is a case to say that this concern is not one for the programmer to deal with.

I think that's both true and false at the same time. It depends, as the saying goes. Let's address the problem area a little, then we can come back to areas of responsibility.

### Handling Connections Efficiently

> A solution would be have the transport on each node manage the connections on each of its endpoints. Heuristics might include killing heavyweight connections to remote endpoints that haven't been used in a time limit, killing old connections when > new connection requests are made and a connection limit (e.g. ulimit) isn't far from being breached etc..

The classic way to deal with this is to use a kind of 'heart beat' sensor, that periodically sends a ping to the other end to ensure it's alive, and closes the connection if it doesn't receive a response in a timely fashion. The definition of 'timely' is up for debate and, of course, this might benefit from being configurable.

Erlang certainly **does** support this notion, and the time limit for the heartbeat that determines whether or not a connection should be considered dead and torn down is set using the `net_kernel.tick_time` parameter to the virtual machine.

These however, are the concerns of handling connections efficiently and carefully in a fully connected system. They do not actually address the scalability concerns that you've raised, because if the system is fully operational - i.e., all the nodes stay online and are able to remain connected to one another - there is **still** a limit to how many nodes you can interconnect before you begin to exhaust system limits on each node.

Personally I **do** think that connection management should be configurable and exposed via the *management* APIs, but I do not think it should be magic. We should simply pick some sensible defaults.

Another issue to be aware of, with all this, is that Cloud Haskell deliberately makes the programmer handle `reconnect` explicitly. This is a **good thing** because, unlike Erlang, it forces the programmer to be aware that message ordering and/or delivery guarantees might not hold after a reconnection has occurred.

There is, BTW, some open issues to look at this in detail: 

* https://github.com/haskell-distributed/distributed-process/issues/66
* https://github.com/haskell-distributed/distributed-process/issues/32

### Scalable Distributed Computing

Ha ha - the title says it all! This is a **massive** area of distributed systems research and we'd do well to absorb as much of that as makes sense before pushing forward with any particular implementation.

One fairly common solution to the problem of needing fully connected networks is to create independent federated clusters. In this architecture, you have several *clusters* of nodes, which are fully inter-connected (like a *normal* CH system). These clusters are then connected to one another via a quorum of members, which act as intermediaries.

> I'm not sure whether this is a parameter set when creating transports or endpoints, but such connection management should probably not be a concern of the programmer.. (?)

Connection management is very much the concern of the programmer, but we should not make it a barrier to entry. You should

* be able to build a scalable, high performance distributed system on CH using the (sensible) defaults
* be able to configure connection management (such as heartbeat timeouts) if you wish
* be able to manage the network layer at runtime, once your application is in production
* be able to choose the right strategy for your own needs, where this is appropriate [*]

On that last point, the example I gave of federated clusters is a good one. We should not be doing something like that automatically IMO, but if that kind of capability *does* exist then we should make it easy and transparent for the application developer to take advantage of it. From the system architects point of view however, this is almost certainly something that should be turned on or off explicitly.

Finally, I do need to point out that this is way off our radar right now. That doesn't mean it's unimportant, but the attention it receives will increase over time - I see the massive scalability support as low (ish) priority, versus the connection management which is of medium importance - less so than, for example, being able to send messages efficiently to local processes, but perhaps more so than providing some gorgeous web UI to do cluster/grid administration.",12/18/12 04:04:53 PM,01/16/13 09:58:22 PM,Task,Horizon,Open,Unresolved,hyperthunk,network-transport-tcp network-transport distributed-process-azure,"Comment:hyperthunk:12/18/12 04:05:30 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
","Comment:robstewart57:12/18/12 04:31:35 PM:

mentioned","Comment:cartazio:01/16/13 08:12:07 PM:

One thing that seems like it would help here and in a few other areas is having a reliable message oriented transport layer. 

I'm playing with some engineering relating to such transport layers (as some of the more interesting protocols that are relevant, like SCTP, which don't have kernel versions on every major OS)

I'm not sure if this is quite relevant or not (i'm still learning a lot in this space), but thought I'd throw that thought out there :) 
","Comment:hyperthunk:01/16/13 09:58:22 PM:

> One thing that seems like it would help here and in a few other areas is having a reliable message oriented transport layer.

@cartazio - that's what [network-transport-tcp](https://github.com/haskell-distributed/distributed-process/tree/master/network-transport-tcp) provides. I also think that reliable ordering, whilst important for Cloud Haskell (given that we rely on this to make the semantics work), is orthogonal to the issue of scaling up to handle very large grid/cluster deployments.

Whilst we're talking about transport layers though, I would certainly be interested in seeing an SCTP network transport layer - it would need to conform to the [Network.Transport API](https://github.com/haskell-distributed/distributed-process/blob/master/network-transport/src/Network/Transport.hs).

","Comment:cartazio:01/16/13 09:58:22 PM:

mentioned",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
102,simplelocalnet: how to use on master with several interfaces?,"(I'm sorry for this being not exactly Haskell)

I wan to run C.H. on a cluster where the main node has several network interfaces:

```
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
192.168.6.0     0.0.0.0         255.255.255.0   U     0      0        0 eth1
192.168.5.0     0.0.0.0         255.255.255.0   U     0      0        0 eth0
141.X.X.0      0.0.0.0         255.255.252.0   U     0      0        0 eth2
0.0.0.0         141.X.X.X   0.0.0.0         UG    0      0        0 eth2
```
client nodes have two interfaces in the local network (192.168.5/6.X) only

I can run C.H. processes on several of the client nodes, but I don't seem to be able to use the main node. Are the broadcasts going to the wrong places? ",12/18/12 09:59:52 PM,01/04/13 03:58:02 PM,Task,Misc/Admin/Support,Open,Unresolved,jwaldmann,,"Comment:hyperthunk:12/23/12 11:13:22 AM:

It's possible your os isn't configured to multicast to the interface you're trying to use. What does netstat -nr look like?","Comment:jwaldmann:12/27/12 07:27:36 PM:

```
Kernel IP routing table
Destination     Gateway         Genmask         Flags   MSS Window  irtt Iface
192.168.6.0     0.0.0.0         255.255.255.0   U         0 0          0 eth1
192.168.5.0     0.0.0.0         255.255.255.0   U         0 0          0 eth0
141.X.X.0       0.0.0.0         255.255.252.0   U         0 0          0 eth2
0.0.0.0       141.X.X.X         0.0.0.0         UG        0 0          0 eth2
``` ","Comment:hyperthunk:12/28/12 02:26:59 AM:

So I don't see a multicast range IP address in the routing table there. You need to add the route so that all the servers know about the group, something like `route -nv add -net 228.0.0.4 -interface eth0` or whatever your flavour of *nix requires. I'd suggest using something like emcast or mnc to test that the machines are happily communicating in the first instance. Or just `ip igmp join-group <ip-addr>` and ping from another location or whatever. ","Comment:hyperthunk:01/04/13 03:58:02 PM:

Did you have any joy with this @jwaldmann?","Comment:jwaldmann:01/04/13 03:58:02 PM:

mentioned",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
103,spawnChannel doesn't look right,"```haskell
-- | Spawn a new process, supplying it with a new 'ReceivePort' and return
-- the corresponding 'SendPort'.
spawnChannel :: forall a. Typeable a => Static (SerializableDict a)
             -> NodeId
             -> Closure (ReceivePort a -> Process ())
             -> Process (SendPort a)
spawnChannel dict nid proc = do
    us <- getSelfPid
    spawn nid (go us)
    expect
  where
    go :: ProcessId -> Closure (Process ())
    go pid = cpNewChan dict
           `bindCP`
             (cpSend (sdictSendPort dict) pid `splitCP` proc)
           `bindCP`
             (idCP `closureCompose` staticClosure sndStatic)
```

That `expect` might pick up the wrong `SendPort` - I think we need to create a channel for the reply first.",12/20/12 03:52:31 PM,01/27/13 08:55:04 PM,Task,distributed-process-0.4.3,Open,Unresolved,simonmar,distributed-process Bug,"Comment:hyperthunk:12/20/12 04:11:51 PM:

assigned","Comment:hyperthunk:12/20/12 04:12:31 PM:

Thanks for spotting that @simonmar - it looks like a simple (ish) fix, so I'll have a look at it over the next day or so.","Comment:simonmar:12/20/12 04:12:31 PM:

mentioned",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
104,Process death is very quiet,"When a process dies due to an exception, we don't see anything on `stdout` or `stderr` at all. This makes debugging very difficult - typically the result is a deadlock.  The `remote` framework used to issue a SYS message to the logger, which was much more helpful.  I've been doing a lot of printf debugging lately that would have been helped by some built-in error reporting.

I know there are plans to move the logger out of the core, which is good, but we should think about what to do here.
",12/20/12 03:56:45 PM,01/26/13 05:44:07 PM,Task,distributed-process-0.4.2,Closed,Fixed,simonmar,Feature-Request distributed-process,"Comment:hyperthunk:12/20/12 04:25:57 PM:

> I know there are plans to move the logger out of the core, which is good, but we should think about what to do here.

It sounds like the node controller (and maybe one or two other places) should perform internal logging under some circumstances. Is this something that the backends should have access to as well, or is that a separate concern?

I'm not overly excited about using a `Process` based logger for this. Is there anything wrong with using something that simply calls `putStrLn` or uses `Debug.Trace` in those places? I do realise buffering is out of our hands, but there's little we can do about that anyway. Or should we be looking at `System.Log.Logger` perhaps?","Comment:hyperthunk:12/20/12 04:28:05 PM:

Also, come to think of it.... In OTP, you tend to see process deaths logged by supervisors, but ordinarily process death will go unnoticed (unless you're linked to it of course!) - that's fine for something that's running in production for the most part. So perhaps printing process death info to stdio should be something that is configured on a per-node basis, so you can turn it on when you're developing/debugging only.","Comment:simonmar:12/20/12 04:43:46 PM:

Optional debug tracing would be fine by me.","Comment:hyperthunk:01/23/13 10:02:41 PM:

So what I've done is put some traceEvent calls into the node controller. This can be printed to stdio right? Though I've not figured out what flags to put in the cabal test definitions to make it work. -eventlog -Dx doesn't appear to work..... ","Comment:simonmar:01/24/13 09:26:41 AM:

Yes, `traceEvent` should work.  You need to compile with `-eventlog` and run with `+RTS -v`. 

If `traceEvent` is going to be the standard way to do debug logging in `distributed-process`, then that should probably be documented somewhere...","Comment:hyperthunk:01/24/13 02:15:21 PM:

> If traceEvent is going to be the standard way to do debug logging in distributed-process, then that should probably be documented somewhere...

Well I think it's probably a good option, but that *does* introduce a lot of other noise that has to be filtered out, so.... What I'm thinking of doing is introducing a trace option for node creation that provides a plug-able API for tracing. This would be fairly simple, disabled by default (to avoid evaluating when we're *not* in debug mode) and only dependent on `-eventlog` if `traceEvent` was the chosen trace function. A choice of pre-built *tracers* will be available in the node creation/setup API so you can choose an existing one or wire in your own. 

You're right of course that this will need to be documented clearly. I'll push these changes later on tonight if I get time to finish them off.","Comment:hyperthunk:01/24/13 05:44:03 PM:

Right. I haven't wired this into the node controller yet - that requires adding a field for the tracer to the `LocalNodeState` which is easy enough - but the initial stab at providing either console or eventlog based tracing is comitted here: https://github.com/haskell-distributed/distributed-process/compare/debug-trace.

I'll tidy this up and integrate it tonight (or tomorrow morning) so if we *do* get a release out soon, it'll be included.","Comment:hyperthunk:01/25/13 03:56:04 AM:

assigned","Comment:hyperthunk:01/25/13 04:01:30 AM:

Ok so that branch is a bit more complete now. I will look at documenting it a bit more clearly, and pointing out that it's not a *logging* feature and shouldn't be used in production, yada yada.","Comment:hyperthunk:01/25/13 04:27:56 AM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
","Comment:simonmar:01/25/13 08:29:40 AM:

Looks good - how does this interact with the stuff in simplelocalnet that redirects log messages to the master node?
","Comment:hyperthunk:01/25/13 09:58:54 AM:

> Looks good - 

Cool, I'll roll it into 0.4.1 then.

> how does this interact with the stuff in simplelocalnet that redirects log messages to the master node?

That's a good question. The event-log based logging should *just work*, as based on the 'traceEventIO' documentation (and the GHC 'eventlog' docs too) it should as though writing to the event log is thread safe, with the caveat that on SMP enabled systems you might get some duplication.

If code in (or using) simplelocalnet uses 'say' and you're printing the event log to stdout with @+RTS -v@ then the output is going to interleave. I guess folks will need to write to a binary log and/or use ghc-events or threadscope to alleviate that problem.

In terms of logging to a file, that's a little different. I'm not overly familiar with the GHC runtime's IO subsystem, but the documentation for `openFile` seems to indicate that opening a handle to a file that is ""already in use"" will raise an error. Quite how that works isn't clear to me, but if that happens when the slave nodes begin to start up then the `catch` in `defaultTracer` will revert to event-log anyway. I **think** this means that the only *sane* way to use the debug tracing facility with simplelocalnet is to stick to the event log.

We *could* add another option to use `say` for tracing. That would make life easier for simplelocalnet users, **but** I don't want to add any code to `Node.hs` for *configuring tracing* on node start (as that would make the API messy and overly complicated) and **especially** not after starting - I tried that using the `sendCtlMsg` primitive and the only way to do it safely is to store the `Tracer` in an `(T)MVar` which is going to hurt performance every time we trace something. Plus with the current approach, a regular function in the `Process` monad can use the `trace` primitive without having to synchronise with other threads, which seems quite handy.","Comment:hyperthunk:01/25/13 11:07:58 AM:

Ok, so I've just pushed an update to the debug-trace branch that supports doing tracing to the system logger process if the `DISTRIBUTED_PROCESS_TRACE_FILE` environment variable is not given (or the operation to open the relevant file handle fails) and the `DISTRIBUTED_PROCESS_TRACE_CONSOLE` is non-empty, then trace messages are sent transparently to the registered `logger` process. Otherwise they fall back to `traceEventIO` which of course will do nothing if event logging isn't enabled.

Take a quick look and let me know if that'll do the trick for simplelocalnet, then I'll merge - all the tests still pass with all three tracing options enabled (or not). I haven't checked the benchmarks to see what the performance implications are - this laptop wouldn't do much for the figures anyway.","Comment:hyperthunk:01/26/13 05:44:07 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
105,How can we get the size of a process' mailbox,"Following on from issue #89 - it's pretty touch to implement this properly because if the process in question is stuck in a blocking receive, you'll block the caller until the `MVar` containing the *arrived* messages put back. Of course it's very easy to deadlock in that kind of situation too.

How can we go about making this state readable without introducing lots of contention or worse affecting the semantics? Would it make sense to keep an `IORef Int` in the `CQueue` state? After all, we would presumably only *write* to that inside of `dequeue` and **that** routine acts like a critical section (because it calls `takeMVar` prior to doing anything).",12/20/12 04:33:22 PM,01/04/13 03:56:48 PM,Task,distributed-process-0.5.0,Open,Unresolved,hyperthunk,Feature-Request distributed-process,"Comment:hyperthunk:12/20/12 08:08:20 PM:

> Would it make sense to keep an `IORef Int` in the `CQueue` state? After all, we would presumably only *write* to that > inside of `dequeue` and **that** routine acts like a critical section (because it calls `takeMVar` prior to doing anything).

That is nonsense! I don't think it's possible for `dequeue` to be called by two threads at once, as the `LocalProcess` is effectively thread-local data. We only need to update the count in `dequeue` and we **do** know that only the process to which this mailbox belongs will get into `dequeue` - only one `forkIO` thread at a time. So we have 1 reader and multiple writers. Sounds like a `TVar` that gets updated `atomically` to me. As long as the readers only call `readTVar` then we should be fine.","Comment:hyperthunk:01/08/13 12:45:26 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
106,Live Server Upgrades,"
  So... Cloud Haskell isn't likely to support the same kind of *hot code upgrade* as Erlang any time soon, and for good reason. That model simply doesn't make sense for a strongly typed language like Haskell, and there's nothing in the RTS to support it either - one of our aims is to avoid changing the RTS and to keep CH as a library.

After looking at http://hackage.haskell.org/packages/archive/plugins/1.5.3.0/doc/html/System-Plugins-Load.html, I can see that it's possible to dynamically load new modules, but there's really nothing in that approach that fits the concept of upgrading an existing (i.e., already loaded) module cleanly.

I *do* however, think that we can approximate many of the benefits of Erlang's *rolling upgrades* without actually doing any runtime code changes. At a very high level, my idea is that we provide a mechanism for seamlessly migrating *process state* from one node to another, and enable many of the benefits of *rolling upgrades* by transparently moving *managed processes* from one node to another, where the latter node is *running a different image to the former/original node.*

First let's look at the mechanical aspects of this, just to see if it's plausible. Then we'll consider the pros and cons. From an implementation standpoint, I think we can achieve this via several steps:

1. define an API for processes to manage their intrinsic ongoing state
2. make it possible to migrate `Serializable` data to a node running another binary/image
3. provide an API for processes to handle data transformations when migrating to the upgraded node
4. figure out how to synchronise all of this properly

Let's talk about (1) for now. Firstly, it's important to realise that for (1) and (2) I'm **not** proposing that the v1 node sends a `Closure (Process a)` to the v2 node. That *clearly* wouldn't work, because the nodes are not running the same image and the closure environment (viz function pointers/free variables) won't be the same on the destination node! Instead, what I'm proposing is that certain kinds of *special process* which are willing to manage their ongoing state in a specific way, could be allowed to *relocate* that state to another node providing that it is `Serializable`. *How* this state is decoded on the other side is a combination of built in Cloud Haskell support **and** user defined transformations. We'll cover the latter part shortly(ish).

Consider, for example, the *generic process* implementation that we've started working on for -platform. In that module, we use the state monad to track the *server state* which is `Serializable` and make this available to the user defined implementation code. The `GenServer` module does something similar.

```haskell
type Process s = ST.StateT s BaseProcess.Process

-- [snip for brevity]

getState :: Process s s
getState = ST.get

putState :: s -> Process s ()
putState = ST.put

modifyState :: (s -> s) -> Process s ()
modifyState = ST.modify

-- [snip for brevity]

loop :: Behaviour s -> Timeout -> Process s TerminateReason
loop s t = do
    s' <- processReceive (dispatchers s) t
    nextAction s s'
    where nextAction :: Behaviour s -> ProcessAction ->
                            Process s TerminateReason
          nextAction b ProcessContinue = loop b t
          nextAction b (ProcessTimeout t') = loop b t'
          nextAction _ (ProcessStop r) = return (TerminateReason r)

processReceive :: [Dispatcher s] -> Timeout -> Process s ProcessAction
processReceive ds timeout = do
    s <- getState
    let ms = map (matchMessage s) ds
    -- TODO: should we drain the message queue to avoid selective receive here?
    case timeout of
        Infinity -> do
            (s', r) <- ST.lift $ BaseProcess.receiveWait ms
            putState s'
            return r
        Timeout t -> do
            result <- ST.lift $ BaseProcess.receiveTimeout (intervalToMs t) ms
            case result of
                Just (s', r) -> do
                  putState s'
                  return r
                Nothing -> do
                  return $ ProcessStop ""timed out""
```

Now let us imagine that a new exception type is defined which can be thrown to a process (lightweight thread) - let's call it `ForceUpgradeException`. When this exception is delivered asynchronously to a process using `throwTo`, it will of course terminate the process. Any process however, can install an exception handler to deal with this. A *special* process, in my proposed model then, is a process (like `GenProcess`) that installs an exception handler for `ForceUpgradeException` and in the exception handler calls a Cloud Haskell API for *migrating the process state to another node*. Let's imagine that there is a primitive we can call to *relocate* the process state to the node we're failing over to - I realise that we're not actually failing so this is really a take-over.

```haskell
-- assuming that we have something like
data ForceUpgradeException = ForceUpgradeException !NodeId
    deriving (Typeable, Show)
instance Exception ForceUpgradeException

relocate :: (Serializable a) => NodeId -> a
relocate nid state = mask $ sendCtrlMsg Nothing $ RelocateSignal nid state
```

The `GenProcess` code can easily add support for this, because it can mandate that the *state* is `Serializable` - we can therefore automate the relocation logic ourselves, but using `try` and handling `ForceUpgradeException` by calling `relocate targetNode state'`.

Now looking at `relocate` - going via the local node controller may be a good idea, as we can immediately track the fact that this process has been *moved* to another node (more specifically, to another running haskell program). We'll consider some of the implications of this later on too.

So before we get into the muddy details of (2) I want to clarify that we're saying....

1. When upgrades are started, a `ForceUpgradeException` will be thrown (asynchronously) to the target process
2. If a `ForceUpgradeException` is caught, then there is an API that can be used to move the process state to the other node as part of the take-over
3. The `relocate` API is useless (or will end up terminating the calling process?) if called otherwise

A corollary issue we ought to consider is that of maintaining state invariants whilst a process is transitioning from one state to another. Consider a process that, for example, is in the middle of writing to a file. The implementer may wish to handle normal state changes by reading the contents of the file and writing some changes out before replying (or continuing) but this will break in the face of asynchronous exceptions, such that if the `relocate` call tries to read the file and serialise its contents for reconstitution on the take-over/v2 node, the content of the file may have become inconsistent due to reads/writes being terminated unceremoniously.

The obvious solution to this is to call `mask` before entering any such *critical path* and a generic process management API such as `GenProcess` should provide explicit, declarative support for doing this.

---------
 
Migrating `Serializable` state to another node isn't impossibly hard. There are two core problems that need to be solved - 'addressability' and decoding. The latter problem is very simple to outline - we cannot use `Typeable` to identify the type and thence `Binary` to decode the incoming data; We might not have an instance for it on the target node, or if we do it may have changed between versions and decoding might fail, both of which would prove catastrophic. In either case, the `Fingerprint` that `Typeable` gives us may or may not provide enough information to identify the type, but CH currently sends a pointer to it, which is unlikely to be useful on a target node running a different image.

This use of a pointer is, of course, a very sensible optimisation. We wouldn't want to do away with that in the normal case, nor is it clear to me whether or not there is any other way to handle type identification on remote nodes without user intervention.

### Handing Decoding

My proposal for handling this latter difficulty is very simple, and probably quite unpopular - make it the user's problem, at least to some extent. If the definition hasn't changed then *they* know this and can use an API call in `Primitives.hs` to `decodeFromVersion` or some such, which would just defer to binary's `decode`. We would basically have to ignore the fingerprint in this situation.

If the expected state *has* changed, then the user needs to deal with this themselves. If the *type* which the *server loop* is working with has changed, then they should simply write a transform from `a -> b` and use `myTransform . decodeFromVersion` to handle it. If there are changes to the data types involved, then the user should handle these themselves, and we can support *both idioms* simply by making sure that relocated process state is given to the destination (upgraded) node as a `ByteString`. The user can write code using `ByteString` and `Binary` to grab the various chunks of the original (prior) state and transform it however they wish.

This might sound complicated, but it's only a bit more complicated than what you've got to do in Erlang in practise. Erlang **can** do *magical* upgrades because modules can change on the fly, but more often than not record/tuple definitions have changed and the *managed process APIs* in OTP such as gen_server actually provide an explicit callback for code upgrades, just so that the developer can write these kinds of transformations themselves. Admittedly they've not got to bother with encoding/decoding, but I think that's a reasonable price to pay for a beautiful type system.

Before we can move on to 'addressability' and points (3) and (4), we have another set of issues to handle. When relocating a process, we have mandated that the `relocate` API call is provided with `Serializable` data representing the process' current view of its state. As mentioned previously, we cannot magically take the free variables and transmit them, as we know next to nothing about the image that the v2 node is running. It would be very unfortunate however, if the process running on node-v1 had some quantity of messages in its mailbox and these disappeared when the process was relocated to node-v2! As much as I'm willing to move away from Erlang's model for release upgrades, I'm not willing to loose my process' message queue!

### Implementation Challanges

This is where it gets a bit more complicated....

All the messages in a process' mailbox are `Serializable` and so can be copied to node-v2 in theory. We do, however, have the same problem here as we did with the process state migration, viz what to do if there are messages in the mailbox which will not be recognised by code running on node-v2 due to API changes and so on. I suspect the only solution here is to ensure that the user has provided a means for decoding them properly. If we go back to the principle we had chosen for handling the process state translation, we gave the user a `ByteString` and made them decode it themselves. This isn't going to scale for the contents of the whole message queue, but we *might* be able to get away with this to some extent.

GHC precomputes the MD5 hash for the TyCon so I'm guessing that we will know if a type is not *recognisable* when we try to examine its fingerprint. Quite how this works in practise is a little hazy for me - I've not delved into the GHC sources enough to fully understand it - I suspect that the location of the type's pointer in the data section of the image might be used along with the MD5, so we **may** need a little runtime support in order to identify that? Maybe not though.... 

Clearly functions like `decodeFingerprint` in Cloud Haskell are going to fail regardless, because they basically take the fingerprint binary that is created with `encodeFingerprint` and pass it to `toForeignPtr` - because `encodeFingerprint` simply grabs a pointer to the type def (TyCon?) this will not work when the layout of the image (viz the code and data sections) has changed; Even if the `Fingerprint` was for the same type, this wouldn't work for identifying the type when running another image. And if not, is that the end of the world? I think not, as we can still serialise the data to a `ByteString` to send it and therefore we can come up with some means of identifying the types on another node running a different version of the image. It might be that the user needs to tag all their remote-able data types with a `Word8` or some such, so that we can figure out how to convert them via a lookup table. Or perhaps just using the MD5 is sufficient?

I'm not 100% clear on how to do this, but it doesn't seem *impossible* anyway. In fact, one advantage of working via some kind of lookup table would be that we could handle transformations and straight decodes identically. The developer writing the upgrade **will** need to decide what to do with messages of types that no longer exist, as well as types for which the `TypeCon` has fundamentally changed anyway. The code that handles `relocate nid state` instructions in the node controller would basically also require a table mapping `(Serializable a) => a -> ByteString` and the user would then annotate the output of `Data.Binary.encode` somehow and on the other side, there would be another table from `(Serializable a) => ByteString -> Message a` wherein the user would figure out the mapping between types themselves. If it was necessary to go down that path, we could provide some build/compile time support for generating the lookup/mapping tables I'm sure.

Of course for now, I'm leaving all the lovely details of how to handle that to the reader! ;)

There is also the problem of moving the process' typed channels to the new node. We would need to re-establish the `ReceivePort` for each of these in the new node controller, so fair amount of internal node controller code will be required to support this properly. In addition to the challenge of handling the types and their transformations onto the remote node, there is a not insignificant amount of work required to handle draining the message queue and transactional channels cleanly due to the complex nature of the `CQueue` implementation. Whilst this *is* tricky to get right, because the writes go via the node controller, we should be able to avoid getting deadlocked whilst doing so.

Again, I'm going to leave this as another item for further discussion.

### Handing 'Addressability'

After worrying about how to transfer typed messages from one node to another, when both nodes are running different executables, this ought to be a breeze.

Processes are 'addressable' via their `ProcessId`, which encapsulates the `NodeId` for the node on which they reside as well as a node-local identifier for the process' thread. If processes are to be *re-locatable* then we must ensure that wherever the `ProcessId` has escaped to, we inform the actor possessing it of this change. This isn't actually too big a problem. When a process decides to `relocate` to another node, we must send a control message to *all* other nodes in the system informing them of this fact. All process message passing interactions happen via the local node controller, and it is there that we should deal with this fact. Because a `ProcessId` is `Serialisable` and can be sent to arbitrary receivers, it would be necessary to hold a map from the old process identifiers to the new ones if we allowed processes to arbitrarily call `relocate`. If we have to keep these mappings in all the node controllers throughout the system's lifetime, we're going to leak space like nobody's business. I propose therefore, that we keep the existing semantics which are that once node-v1 goes away, messages sent to the original `ProcessId` pointing to that node will be silently dropped. This is a bit nasty compared to Erlang's upgrade mechanism, but I don't think we can scale otherwise - I'm certainly open to suggestions about how to better manage this.

At a minimum however, a process migration should generate a `Monitor` signal, thus I propose that we add a new `DiedReason` indicating that a process has moved.

```haskell
data DiedReason = 
     -- | Normal termination
     DiedNormal
  |  DiedProcessRelocated !ProcessId
  |  DiedNodeRelocated !NodeId
  |  -- snip...
```

This at least allows remote processes to monitor the fact that the process/node they were interacting with has moved, so that code which holds on to one or more `ProcessId` items - or `NodeId` for that matter - can update itself when the move is complete.

Talking of monitors, when migrating a process to node-v2, we will need to re-establish all monitors and links, and any node controller that receives the `relocation-in-progress` signal should do the same. Nodes which are disconnected when the take-over happens will basically be stuck, and attempting to track all the peers we've seen over time so as to handle this sounds unworkable to me.

### Dealing with synchronisation

While the -v1 node controller is merrily attempting to migrate a process' state and mailbox to another node, and dealing with incoming and outgoing links and monitors (!), it's highly likely that other nodes are attempting to interact with node-v1. Ignoring connection requests and other infrastructure level calls from `Network.Transport` is almost certainly the right thing to do, but what should we do about messages that other processes have asked us to deliver? And how do we ensure that all the processes in an application's supervision trees are upgraded seamlessly? Should we enforce some specific ordering, or should the user be able to specify this, or both? If a process is linked to another process that we wish to `relocate` - what semantics do we require? And to what extent do we need/want to attempt to synchronise with other nodes whilst this is going on?

And.... Should we communicate something about this at the `Network.Transport` level? I would expect this to impact significantly there, not least because we'd want to automatically establish a connection to node-v2 if we were previously connected to node-v1. And of course the semantics of doing that are flaky for distributed platform based on asynchronous messaging, as evidenced by the fact that `reconnect` is explicit. We won't even talk about what Erlang does <shudder>.

What occurs to me about this, is that we *do* have to force the user to get involved in the migration. Not only is this normal even for Erlang release upgrades (when data types change for example), it is reasonable IMO and will still be simpler in practice than having to build some complex beast that handles down time. The architectural complexity would be primarily in CH, but the user would have to do some work to make the migration happen.

Another thing that we must consider here is that some kinds of process will not be able to magically migrate their state. A process which has, for example, opened a socket and started listening (or sending) will not be able to do this. The same would be true for a distributed erlang application that was failing-over to another node, though code upgrades would not cause that problem there. In practise, this is also not a huge difficulty, as a load balancer in front of the application isn't a hugely complex thing to have to configure. There *is* a bit of complexity leakage here, in that a process which is maintaining some kind of session, is going to have to make sure they can re-establish that state on node-v2. Again, this should be do-able because the load balancing mechanism should hide the fail-over from the client and the state migration can simply rely on the `relocate` and migration/translation mechanism. Of course, it's entirely up to the application developer if they want to handle session state in this way, but the point is that we can support doing so when required.


So......

Thoughts/Ideas on a postcard please!!! :)",01/02/13 11:03:18 PM,01/17/13 01:48:02 AM,Task,Horizon,Open,Unresolved,hyperthunk,Feature-Request distributed-process,"Comment:hyperthunk:01/03/13 11:45:26 AM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
","Comment:simonpj:01/03/13 10:06:14 PM:

Some quick thoughts.

First, it's an ambitious project. I rather think there may be other things that need doing first.  But perhaps you are motivated by a particular application.

Second, the type of relocate looks very strange.  Shouldn't it take the state as input (to be serialised) not return it?  And what code should be run at the other end?  You need some way to specify which function to call in the new image.

Third, if the old process (OldP) has state of type OldP.T,  the new process (NewP) has to parse that state somehow.  Imagine that we serialised it as a string with Show.  Then OldP's 'show' would generate the string, and NewP's 'read' would parse it. They had better agree!

One possibility is to insist that OldP's state type and the type that NewP parses are identical. (After parsing, NewP might then translate that old state into a new richer state, but that's a separate matter.)  What does ""identical"" mean?  One simple possiblity would be ""the same all the way down"" which is what the MD5 finger print does.

And yet it may be too much to insist that the types are identical.  An alternative would be use use a self-describing format, much as happens for web services. I'm out of my depth here, but I'd look at JSON and Protocol Buffers and that kind of stuff.  No point in re-inventing the wheel.","Comment:hyperthunk:01/04/13 02:04:25 PM:

HI Simon - thanks for the feedback!

> First, it's an ambitious project. I rather think there may be other things that need doing first.

I quite agree. This is very low priority for me, but I thought that others (such as Pankaj) might be interested, and I like to capture the conversation around these ideas on the issue tracker.

> Second, the type of relocate looks very strange

That's just me typing too late at night.

> And what code should be run at the other end? You need some way to specify which function to call in the new image.

Yes indeed. I was thinking there would be a structured API for this, but you're right that this information needs to come from the 'user' as it were. Over email Jeff suggested that the code calling `relocate` should provide a `Closure (Process ())` which makes sense. 

> And yet it may be too much to insist that the types are identical

Indeed this is one of the big challenges. Again over email, Jeff suggested solving this first:

""It might be nice to use a system to ensure that types with different fingerprints can be automatically converted. This is an issue that comes up a lot (also in sending messages between different versions, in the case of partial upgrades) and might be worth solving first."" - Jeff Epstein

I might open a separate bug for that at some point, but I want to see if someone *bites* and takes an interest in owning this first. :)","Comment:basvandijk:01/16/13 11:16:06 PM:

Have you looked at [safecopy](http://hackage.haskell.org/package/safecopy)? It solves a very related problem.","Comment:hyperthunk:01/17/13 01:48:02 AM:

@basvandijk that's completely awesome - I didn't know it existed. It looks **very** similar to what we need - if we adapt that approach to work with lazy bytestrings and the one or two other differences - the automatic derivation would need to be done a bit differently and so on - but yes, this looks like an ideal approach. Thanks for pointing it out!","Comment:basvandijk:01/17/13 01:48:02 AM:

mentioned",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
107,Support for testing process failures and netsplits,"When testing distributed systems, it's pretty useful to be able to simulate communication errors. I propose that we write some extensions to network-transport-inmemory that allow a test coordinator to *mess with* the connectivity between nodes, introducing arbitrary delays, forced disconnects and so on.

I would also like to have something like https://github.com/dluna/chaos_monkey that kills arbitrary processes on demand, but stays away from *system processes* and supervisors. Quite how one should identify that a process *is* a supervisor I don't know. Erlang uses the process dictionary for this (storing the initial_call and such things in there) but we might want to think about other approaches.",01/03/13 12:57:42 PM,01/07/13 12:59:42 PM,Task,Horizon,Open,Unresolved,hyperthunk,Feature-Request distributed-process network-transport-inmemory,"Comment:pankajmore:01/03/13 04:43:23 PM:

@hyperthunk  Do you suggest that we build chaos_monkey in haskell. I have not tried chaos_monkey but it seems like it can be used to kill haskell processes too? 

I will have a look at network-transport-inmemmort and see how to see simulate communication errors but first I need to get upto speed with the deployement. It seems that my 7.6 ghc upgrade broke things. seems like distributed-process is not compatible with ghc-7.6.1 yet.","Comment:hyperthunk:01/03/13 04:43:23 PM:

mentioned","Comment:hyperthunk:01/03/13 05:19:58 PM:

@pankajmore 

> It seems that my 7.6 ghc upgrade broke things. seems like distributed-process is not compatible with ghc-7.6.1 yet.

Please file a bug with a complete log of the build failure so we can fix this.","Comment:pankajmore:01/03/13 05:19:58 PM:

mentioned","Comment:hyperthunk:01/07/13 12:59:42 PM:

Note here that n-t-tcp uses a script based approach to this which might be better or at least provide an alternative",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
108,network-transport-inmemory test failures,"In the current HEAD network-transport-inmemory tests fail. Attached is the output. 

This is using haskell platform 2012.04.0.0 on Mac OSX 10.8.2. 


Test suite TestInMemory: RUNNING...
Running ""PingPong"": ok
Running ""EndPoints"": ok
Running ""Connections"": ok
Running ""CloseOneConnection"": ok
Running ""CloseOneDirection"": ok
Running ""CloseReopen"": ok
Running ""ParallelConnects"": ok
Running ""SendAfterClose"": failed (exception: user error (Pattern match failure in do expression at src/Network/Transport/Tests.hs:413:7-40)
Trace:
0       Left (TransportError SendFailed ""Connection closed"")
1       ""15""
)
Running ""Crossing"": ok
Running ""CloseTwice"": ok
Running ""ConnectToSelf"": ok
Running ""ConnectToSelfTwice"": ok
Running ""CloseSelf"": failed (exception: user error (Pattern match failure in do expression at src/Network/Transport/Tests.hs:560:3-36)
Trace:
0       Left (TransportError SendFailed ""Connection closed"")
)
Running ""CloseEndPoint"": failed (exception: user error (closeEndPoint not implemented)
Trace:
0       ""23""
)
Running ""CloseTransport"": failed (timeout)
Running ""ConnectClosedEndPoint"": failed (exception: user error (closeEndPoint not implemented))
Running ""ExceptionOnReceive"": failed (exception: user error (closeEndPoint not implemented))
Running ""SendException"": failed (exception: user error (Pattern match failure in do expression at src/Network/Transport/Tests.hs:813:3-36)
Trace:
0       ConnectionOpened 1 ReliableOrdered 0
)
Running ""Kill"": TestInMemory: thread blocked indefinitely in an MVar operation
Test suite TestInMemory: FAIL
Test suite logged to: dist/test/network-transport-inmemory-0.3.0-TestInMemory.log",01/03/13 03:41:54 PM,01/27/13 08:53:14 PM,Task,Misc/Admin/Support,Open,Unresolved,ericbmerritt,distributed-process network-transport-inmemory,"Comment:hyperthunk:01/03/13 04:38:07 PM:

Thanks for reporting this Eric - either @edsko or I will take a look when we get a chance.","Comment:edsko:01/03/13 04:38:07 PM:

mentioned","Comment:edsko:01/03/13 04:40:17 PM:

The in-memory transport is incomplete; see https://github.com/haskell-distributed/distributed-process/issues/35.","Comment:edsko:01/03/13 04:40:17 PM:

closed","Comment:ericbmerritt:01/03/13 04:43:19 PM:

I did not realize that. I assumed that if it was on the master branch then tests should pass. Is there a branch that does have that property?","Comment:edsko:01/03/13 04:44:26 PM:

Nope, not at the moment. @hyperthunk is working on getting that sorted.","Comment:hyperthunk:01/03/13 04:44:26 PM:

mentioned","Comment:ericbmerritt:01/03/13 04:45:56 PM:

No worries, thanks.
On Jan 3, 2013, at 11:44 AM, Edsko de Vries <notifications@github.com> wrote:

> Nope, not at the moment. @hyperthunk is working on getting that sorted.
> 
> —
> Reply to this email directly or view it on GitHub.
> 
> ","Comment:hyperthunk:01/03/13 04:45:56 PM:

mentioned","Comment:hyperthunk:01/03/13 05:21:29 PM:

Whoops - sorry about the oversight guys. I'll look to get the various branches into the right state over the next two weeks. There will be an announcement on the parallel-haskell mailing list once that's done.

Cheers","Comment:hyperthunk:01/27/13 08:53:11 PM:

assigned","Comment:hyperthunk:01/27/13 08:53:14 PM:

reopened",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
109,build failure on ghc-7.6.1 ,"Glasgow Haskell Compiler, Version 7.6.1, stage 2 booted by GHC version 7.4.2

Running ./install.sh, it looks that it tries to reinstall template-haskell and unix on which ghc depends and may possibly break it.

Output of ./install.sh at https://gist.github.com/4446186",01/03/13 07:23:22 PM,01/25/13 11:42:56 AM,Task,distributed-process-0.4.2,Closed,Fixed,pankajmore,,"Comment:hyperthunk:01/03/13 11:36:16 PM:

We might need to widen the possible dependencies. Did you try editing the relevant .cabal file(s) and allowing for a newer version of template-haskell?","Comment:hyperthunk:01/06/13 03:32:14 PM:

closed","Comment:hyperthunk:01/24/13 06:34:36 PM:

Does widening the dependencies require a major or a minor version bump? If minor then we can put this out in 0.1.1 shortly","Comment:pankajmore:01/25/13 04:44:39 AM:

http://www.haskell.org/haskellwiki/Package_versioning_policy_Version_numbers
seems to suggest that a minor version bump should be fine.","Comment:hyperthunk:01/25/13 09:45:21 AM:

@pankajmore yes, I agree. I will include this fix in the 0.4.1 release.","Comment:pankajmore:01/25/13 09:45:21 AM:

mentioned",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
110,Add variants of exit and kill for the current process,"From an email conversation with Jeff (@jepst) - Currently exit/kill, unlike throw/error, will return, and so the calling function still needs to produce a value. Jeff has proposed adding a variant that sends the control message but whose return type is `Process a`.",01/04/13 03:14:07 PM,01/25/13 11:42:51 AM,Task,distributed-process-0.4.2,Closed,Fixed,hyperthunk,Feature-Request distributed-process,"Comment:jepst:01/04/13 03:14:07 PM:

mentioned","Comment:hyperthunk:01/04/13 03:14:07 PM:

assigned","Comment:jepst:01/04/13 03:23:31 PM:

Specifically when the target of exit/kill is the self process. Thus the behavior is analogous to terminate, but with a reason that can be logged. That is:

```die :: String -> Process a```","Comment:hyperthunk:01/17/13 01:35:33 AM:

closed","Comment:hyperthunk:01/17/13 01:38:14 AM:

@jepst - I opted to skip having a local version of `kill` as it seemed somewhat over-complicated. The new primitive throws `ProcessExitException` and therefore is `forall a. (Serializable a) => a -> Process b` rather than being limited to using `String` for the reason. I did extend the `Show` instance for that exception to decode and pretty print the message payload if it is a `String` however, which should make logging a bit prettier.","Comment:jepst:01/17/13 01:38:14 AM:

mentioned","Comment:hyperthunk:01/26/13 11:15:37 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
111,Provide an API for accessing Process stats,"Resolves issue #89. The message queue length is *not* included in this patch as doing that is quite complicated. This works locally or remotely and provides

* the node
* a list of known registered names
* a list of registered monitors
* a list of registered links",01/05/13 02:15:20 PM,01/13/13 12:21:20 AM,Task,,Closed,Fixed,hyperthunk,,"Comment:hyperthunk:01/05/13 02:16:19 PM:

I'm going to let this brew for a couple of days and see if there's any feedback before merging.","Comment:hyperthunk:01/13/13 12:21:21 AM:

closed","Comment:hyperthunk:01/15/13 01:44:21 AM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
112,Successfully build on ghc-7.6.1,just increased the dependency for bytestring and containers in network-transport-inmemory so that those dependencies don't conflict with those used by ghc-7.6.1,01/05/13 06:09:00 PM,01/06/13 03:32:13 PM,Task,,Closed,Fixed,pankajmore,,"Comment:hyperthunk:01/06/13 03:32:13 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/d95866ccfaa45d74989e9ffa260679053aebd138
","Comment:hyperthunk:01/06/13 03:32:13 PM:

merged","Comment:hyperthunk:01/06/13 03:32:13 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
113,support current versions of test-platfrom-*,"This patch changes the version constraints to for test-platform and test-platform-hunit to support the current versions of those packages. The system compiles and the tests run with those changes. 

The original version of this code explicitly excluded version versions 0.7 and 0.3 respectively. These changes continue explicitly excluding those versions. 

I think a better patch would be to move the constraints from a whitelist to a blacklist. That is, instead of listing known good versions in the constraints explicitly disqualify known-bad versions. ",01/09/13 02:06:23 PM,01/09/13 05:21:45 PM,Task,,Closed,Fixed,ericbmerritt,,"Comment:hyperthunk:01/09/13 05:12:05 PM:

I assume *platform* is a type and we **are** in fact talking about `test-framework` here?

> I think a better patch would be to move the constraints from a whitelist to a blacklist. That is, instead of listing known > good versions in the constraints explicitly disqualify known-bad versions. 

If we can get @edsko or @dcoutts to specify why 0.8* was excluded then that might help. For now I'm tempted to merge this and deal with that later. First I need to test it on a couple of platforms.

One thing that *does* concern me is the potential correlation with issue DPLEGACY-87 if any. That may be unrelated however.","Comment:edsko:01/09/13 05:12:05 PM:

mentioned","Comment:dcoutts:01/09/13 05:12:05 PM:

mentioned","Comment:edsko:01/09/13 05:18:58 PM:

It wasn't excluded, it was just above the upper bound. I would say just increase the upper bound to 0.8, including 0.7; and similar for 0.3. Generally I've set upper bounds to be the package that I was working with and adjust when necessary. ","Comment:hyperthunk:01/09/13 05:20:47 PM:

Great, thanks for the confirmation @edsko. This also works for me on OSX and Linux => Merging.","Comment:edsko:01/09/13 05:20:47 PM:

mentioned","Comment:hyperthunk:01/09/13 05:21:45 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/390b9b8c002a327653eca954ca268467667a3256
","Comment:hyperthunk:01/09/13 05:21:45 PM:

merged","Comment:hyperthunk:01/09/13 05:21:45 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
114,"Given a RemoteTable, is it possible to get the static lable for a Message","I may not be framing this question quite right, but I've been so busy learning all the rest of cloud haskell, that I've not really had the time to fully understand how the static stuff works.

What I'm trying to achieve is this. Following on from issue #30 - I've built up the infrastructure for a generic server to handle arbitrary messages using `maybeHandleMessage (m :: AbstractMessage)` and this works nicely. When a message is *not* matched by any handlers then it can be dropped, forwarded (i.e., dead-lettered) or cause the server to halt. What I'd like to do is dead-letter these 'unhandled' messages to an error logger and print out some debugging info about them. I was hoping that it's possible to use the remote table to get the static label for the message, even though I can't necessarily decode it fully. That would at least provide a bit of info about *what* is arriving in the mailbox that is unexpected. 

Is this even possible? I started poking around in Closure/BuiltIn and distributed-static but I don't want to disappear down a rabbit-hole for days only to find out I'm trying to do the impossible.",01/16/13 01:10:52 PM,01/20/13 07:42:06 PM,Task,,Closed,Fixed,hyperthunk,distributed-process Invalid,"Comment:hyperthunk:01/16/13 01:10:52 PM:

assigned","Comment:hyperthunk:01/20/13 07:42:06 PM:

Actually I can just use `show $ typeOf m` to get enough info about what arrived without being able to decode it. This is therefore invalid.","Comment:hyperthunk:01/20/13 07:42:06 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
115,Intermittent test failures,"```
t4@iske:distributed-process $ ./dist/build/TestCH/TestCH
Basic features:
  Ping: [OK]
  Math: [OK]
  Timeout: [OK]
  Timeout0: [OK]
  SendToTerminated: [OK]
  TypedChannnels: [OK]
  MergeChannels: [Failed]
ERROR: thread blocked indefinitely in an MVar operation
```
This could be related to similar problems as issue #87 but I've not had time to investigate yet. 
",01/17/13 11:55:06 AM,01/25/13 02:50:15 AM,Task,distributed-process-0.5.0,Open,Unresolved,hyperthunk,distributed-process Bug,"Comment:hyperthunk:01/25/13 02:50:15 AM:

And observed this evening (looking at HEAD) - ./dist/build/TestStats/TestStats (which does *barely anything* at all) can lock up **if** you run it in a tight loop for long enough. Around 20 iterations is usually enough for me on a dual core I5. Looking at the event log output, some thread is getting stuck in a foreign call - further investigation to follow. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
116,Extend AbstractMessage with deferred type checking - fixes #71,,01/17/13 12:04:32 PM,01/18/13 05:53:55 PM,Task,distributed-process-0.5.0,Closed,Fixed,hyperthunk,,"Comment:hyperthunk:01/17/13 12:04:57 PM:

As usual I'll let this brew for a couple of days before merging.","Comment:hyperthunk:01/17/13 12:06:02 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
","Comment:hyperthunk:01/17/13 12:08:03 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
","Comment:hyperthunk:01/18/13 05:39:32 PM:

assigned","Comment:hyperthunk:01/18/13 05:40:15 PM:

I'm going to merge this, as I think it represents a common pattern and it clearly enables a number of other useful features. ","Comment:hyperthunk:01/18/13 05:53:55 PM:

merged","Comment:hyperthunk:01/18/13 05:53:55 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
117,Release distributed-process?,Could we have a release of distributed-static please? I need to depend on `mkStaticClosure`.,01/24/13 01:50:52 PM,01/29/13 08:48:02 AM,Task,distributed-process-0.4.2,Closed,Fixed,simonmar,distributed-process Release,"Comment:hyperthunk:01/24/13 02:01:25 PM:

I'm not able to do that. Would you mind poking @dcoutts and/or @edsko and asking if (a) they can assist and (b) set me up to do this in future.

On a side note, we should tag the release and because master is currently diverged from stable (as we've not moved to a stable/dev branch setup yet) we probably need to branch from the previous tagged release and rebase the distributed-static changes onto that for tagging. That part I can and will do.","Comment:dcoutts:01/24/13 02:15:41 PM:

mentioned","Comment:edsko:01/24/13 02:15:41 PM:

mentioned","Comment:simonmar:01/24/13 02:51:01 PM:

Sorry, I meant `distributed-process` (got confused about where `mkStaticClosure` was defined).  It's an addition to the API, so only a minor version bump is needed, unless there are other API changes pending.","Comment:hyperthunk:01/24/13 02:56:16 PM:

Hi @simonmar 

The point I was making earlier is that I have no idea how to do a release to hackage, having never done that before, nor whether or not I have the appropriate permissions and so on. Perhaps I'm all set and I just don't know it!?","Comment:simonmar:01/24/13 02:56:16 PM:

mentioned","Comment:hyperthunk:01/25/13 04:27:56 AM:

It seems all I need to do a release is to get an account on hackage! I've emailed the relevant person and once that happens I'll tag distributed-process-0.4.2 and upload it.

If you wouldn't mind glancing at the [debug-trace](https://github.com/haskell-distributed/distributed-process/compare/debug-trace) change-set for issue DPLEGACY-104? If it looks reasonable I can merge that before the release too.","Comment:simonmar:01/25/13 04:27:56 AM:

mentioned","Comment:hyperthunk:01/25/13 04:28:56 AM:

assigned","Comment:hyperthunk:01/25/13 11:42:08 AM:

Urgh - it seems that 0.4.1 was already tagged and released. Bumping milestone to 0.4.2.","Comment:hyperthunk:01/25/13 11:46:45 AM:

And that leaves **another** question as well, which comes in two parts:

1. has anything in the **other** packages (i.e., NT, d-static, rank1dynamic, etc) changed since the last release
2. does any features in d-process/simplelocalnet/etc rely on the output of (1)

In other words - **can** I make a release of just d-process or do I need to release several packages in order to get this done?","Comment:hyperthunk:01/25/13 01:29:58 PM:

I've not *completely* finished going through this, but it looks like only distributed-process and simplelocalnet actually have any changes that need to be released. There are some other changes but just in 883830fff4746d51571d09cd944f829f7e296a87 which appears to be a cosmetic change-set only.","Comment:hyperthunk:01/26/13 05:49:47 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process/
","Comment:hyperthunk:01/26/13 05:54:50 PM:

Ok @simonmar - if you agree with the fix I merged to resolve issue DPLEGACY-104 then we're good to go and I'll tag and release distributed-process-0.4.2. I think that means we **do** need to release simplelocalnet-0.2.1 (right version?) and I've tested that and it *seems* ok with the latest raft of changes. Do your smoke tests (or samples for your book chapter) work with the HEAD revisions for both packages?","Comment:simonmar:01/26/13 05:54:50 PM:

mentioned","Comment:hyperthunk:01/27/13 03:22:43 PM:

Done. I haven't released distributed-process-simplelocalnet though. Do I need to!?","Comment:hyperthunk:01/27/13 03:22:43 PM:

closed","Comment:simonmar:01/28/13 03:36:53 PM:

Sorry for being unresponsive.  I tested my examples with the released packages and everything seems to be ok.  Thanks!","Comment:hyperthunk:01/28/13 04:17:27 PM:

> Sorry for being unresponsive

Sure no worries. I'll be posting updates on planned releases to the parallel-haskell mailing list in future and you might like to follow 'CloudHaskell' on twitter. :)  ","Comment:edsko:01/29/13 08:48:02 AM:

Thanks for the release! ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
118,Don't expose **everything** - some modules should be private,"Hardly urgent, but at some point we should begin to hide things.",01/25/13 02:53:16 AM,01/27/13 08:54:43 PM,Task,distributed-process-0.4.3,Open,Unresolved,hyperthunk,Feature-Request distributed-process,"Comment:hyperthunk:01/25/13 02:53:16 AM:

assigned",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
119,Should Node Controller and Network Listener be linked?,"If the forkIO thread running `handleIncomingMessages` terminates, nothing happens to the thread in `runNodeController`. Isn't this wrong? The node is now cut off from the outside world and there's no means to start `handleIncomingMessages` - or is there some magic in the Network.Transport layer that handles this? I can't see anything obvious.",01/25/13 10:51:02 AM,01/25/13 10:51:02 AM,Task,,Open,Unresolved,hyperthunk,distributed-process Bug,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
120,Should we split up the umbrella repository?,"It seems a bit awkward figuring out what's changed since the last tag for project-XYZ when we have multiple packages bundled into the same repository. On the other hand, I can see that there are some administrative advantages and, of course, it's easier when hacking on more than one project.

If it was up to me, I'd be inclined to split up the different projects into their own repositories. I am **not** however going to do anything radical without getting agreement for the other maintainers. So, comments on a postcard please....",01/25/13 11:21:56 AM,01/29/13 08:46:37 AM,Task,Misc/Admin/Support,Closed,Fixed,hyperthunk,Question,"Comment:edsko:01/25/13 11:21:56 AM:

assigned","Comment:hyperthunk:01/28/13 11:52:15 PM:

Bounce - it'd be pretty handy to do this, as I've set up an on-demand JIRA and Bamboo account at http://cloud-haskell.atlassian.net/. CI would be much simpler if we had separate repos, and JIRA makes collecting issues across multiple projects pretty simple too, though it is admittedly a bit more complicated than github's issue tracker. ","Comment:edsko:01/29/13 08:46:37 AM:

See email.","Comment:edsko:01/29/13 08:46:37 AM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
121,Release simplelocalnet-0.2.x.y,"@simonmar - I can't quite tell if this is a dot-dot release or not. Is 0.2.1 the right bump for the simplelocalnet version number? If so, I think we're ready to push out a release alongside d-p-0.4.2 (issue #117).",01/26/13 05:49:47 PM,01/27/13 03:52:23 PM,Task,,Closed,Fixed,hyperthunk,Feature-Request distributed-process-simplelocalnet Release,"Comment:simonmar:01/26/13 05:49:47 PM:

mentioned","Comment:hyperthunk:01/26/13 05:49:47 PM:

assigned","Comment:simonmar:01/27/13 03:37:06 PM:

mentioned","Comment:hyperthunk:01/27/13 03:52:23 PM:

Done - 0.2.0.9 is on hackage.","Comment:hyperthunk:01/27/13 03:52:23 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
122,Refactor use of asynchronous exceptions,"### This is a breaking change to the API!

Since we resolved issues #69 and #110, d-p has an **exit signalling/handling mechanism** much closer to Erlang's than before. Most of the exceptions thrown by d-p are *still* exported and can therefore be caught and handled however! This is **not** what we want at all, at least not by my reading of the unified semantics paper. The impression I've walked away with is that links should be uni-directional **and cannot be ignored/trapped.** I propose that we do not export the linking (and registration) failure exceptions at all, making them impossible to catch without resorting to 'catch-all' which will hopefully put people off. As mentioned in earlier issues (and in the unified semantics paper), if the user wants to *respond* to the death of another entity then they should be using monitors, not links.

I have already done this (hiding) for `ProcessKilledException`.

I do not think that `terminate` should throw a different kind of exception to `die` - it should throw `ProcessExitException` with a `Terminated` type or the string ""terminated"" or some such. Calling `terminate` should just be a means for saying ""exit now, and I don't care what reason you give"".",01/26/13 11:15:37 PM,01/26/13 11:15:37 PM,Task,distributed-process-0.5.0,Open,Unresolved,hyperthunk,Feature-Request distributed-process,"Comment:hyperthunk:01/26/13 11:15:37 PM:

assigned",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
123,Post 0.4.2 branch cleanup,Merged branches can be deleted. Maintainers guide needs to be in place first.,01/26/13 11:28:35 PM,01/27/13 03:26:39 PM,Task,Misc/Admin/Support,Open,Unresolved,hyperthunk,distributed-process In-Progress,"Comment:hyperthunk:01/26/13 11:28:35 PM:

assigned","Comment:hyperthunk:01/27/13 03:26:39 PM:

I want to delete the 'dev' branch and other merged branches. Objections anyone!?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
124,Expose Message and broaden the scope of polymorphic expect,"Remove AbstractMessage and replace its forward and maybeHandleMessage
API with versions that operate directly on Message. Create Typeable
and Binary instances for Message and export the type from Process.hs
(though not the constructor). Provide functions for matching, wrapping
and unwrapping Message.

Fixes issue #30.",01/28/13 01:01:36 AM,01/28/13 01:02:33 AM,Task,distributed-process-0.5.0,Open,Unresolved,hyperthunk,,"Comment:hyperthunk:01/28/13 01:02:33 AM:

assigned",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
125,Network.Transport.Chan is named incorrectly,"The project name is right: `network-transport-inmemory`. To use `Chan` as the module is to let an implementation detail leak into the public API. I suggest we move to either

* `Network.Transport.Numa`
* `Network.Transport.Memory`
* `Network.Transport.InMemory`

We will pick one of these (probably `Memory` IMO) for the next major release.",01/28/13 10:43:49 AM,01/28/13 10:45:25 AM,Task,network-transport-inmemory-0.4.0,Open,Unresolved,hyperthunk,network-transport-inmemory,"Comment:hyperthunk:01/28/13 10:43:49 AM:

assigned",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
