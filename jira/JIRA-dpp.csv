Issue Id,Summary,Description,Date Created,Date Modified,Issue type,Milestone,Status,Resolution,Reporter,Labels,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments,Comments
1,supervision trees,"policy control

- sibling management (1-4-1, 1-4-all, left/right neighbours)
- seq/par shutdown (per branch)
- timeout/kill (per process?)
- exit policy (i.e., transient, temp, perm, etc)
- restart intensity (limits)
- alternating restart policies (exit type based, global, etc)
- backoff/timeout in response to restart limits

questions...

- how do we define the startup procedure?
- what type does a start spec have???
- how can we cleanly represent error signals in a generic way?
",10/19/12 05:35:53 PM,01/14/13 10:03:49 PM,Task,Initial-Public-Release,Open,Unresolved,hyperthunk,enhancement in-progress,"Comment:hyperthunk:10/19/12 05:35:53 PM:

assigned","Comment:hyperthunk:10/22/12 12:46:51 AM:

On the point about start specs: the Supervisor process needs to spawn these other Processes, and must be able to restart them. It therefore needs to have the clousure information available for a start - is this enough? And do we force all code that starts a supervisor (or asks for another Process to be supervised) to be a Process too? More to the point, what about code that provides the child specifications themselves - does that have to be in the Process monad?","Comment:rodlogic:10/24/12 12:54:06 PM:

I don't have answers to your questions so far, but found this link useful (I am new to Erlang): 

http://www.aosabook.org/en/riak.html

Would creating a set of data types describing a supervision tree, including a supervision and child specifications, be a good start?

Are supervision tree usually static (static wrt to supervisors not workers)?","Comment:hyperthunk:10/24/12 05:34:15 PM:

Yes, starting with the type definitions is exactly what I want to do. One of the challenges here is knowing how to distinguish between a normal and an abnormal exit condition. In erlang's supervisor, if a process exits with the atom 'normal' then it is considered a happy exit and is only restarted if the process restart policy is set to 'permanent'. Cloud Haskell's concept of exits appears somewhat different. Also, in Cloud Haskell, a link between the child and supervisor will cause both processes to exit, which isn't what we want. So, we need to use spawnMonitor, but the question is, should that be down to the start function the child spec points to, or down to the supervisor? I want to make this mechanism as user friendly as possible.

On the subject of types, how best to represent the child spec? Should it just be a thunk, with the last few arguments left to be filled in when it is started? Or should it provide complete transitive closure over all the arguments used to start the server each time, as passed by the initial caller?","Comment:rodlogic:10/29/12 04:05:19 AM:

After reading a bit about Erlang and supervision trees/supervisor module, I have a few questions/observations I would like to share (note that I don't have much real Haskell and Erlang experience, so take this with a grain of salt):

- (general) behaviors + (user-defined) callbacks - how should we translate this notion to Haskell?

First of all, each behavior could be in it's own module. This makes sense since it's one API per module used potentially by all the other modules. So, we should have a Control.Distributed.Platform.Supervisor module exporting at least the following functions:
  - startLink
  - startChild
  - terminateChild
  - deleteChild
  - restartChild
  - whichChildren
  - countChildren
  - checkChildSpecs

Second, we could use type classes to represent callbacks. A type class specifies a well-defined interface that user-defined modules can implement. E.g.:

```haskell
module Control.Distributed.Platform.Supervisor where
    
class Supervisor s where
    toSpec :: s -> Maybe (SupervisorSpec, [ChildSpec]) -- ^ toSpec is a rename of init
```

Here, s would be a typed version of Args. The problem here is that each different Supervisor will require a different s, but for now I'll ignore this.

Then, a user defined module could define a supervisor:

```haskell
module MySupervisor where
    
newtype MySuper = MySuper () deriving Show

instance Supervisor MySuper where
    toSpec _ =  Just (SupervisorSpec OneForOne 1 60, [
                                       ChildSpec ""Child1"" () Permanent (Shutdown 60) Worker
                                      ])
```

And the generic Supervisor module would define startLink as:

    startLink :: (Supervisor s) => s -> Process ()

I am not sure about Process () here, but it seems like a good choice since startLink would (should?) be called in the context of a Process. In any case, startLink would be implemented along these lines:

```haskell
startLink s = do
    let spec = toSpec s
    (sid, sname) <- processSpec spec
    ....
    return ()
```

- It seems that an Erlang module can implement more than one behavior (?). This also works well with type classes.

- Should a behavior translate to monad a wraps the Process monad (this could get out of hand if multiple behaviors are needed)? Or should there be a single, parameterized monad for behaviors that wraps Process monad and works together with per behavior type classes?

- Supervisor Data types.
This seems to be most straightforward aside from specific refinements.

```haskell
-- |  
data RestartStrategy
    = OneForOne 
    | RestForOne 
    | OneForAll
    | SimpleOneForOne
      deriving Show

-- | 
type AllowedRestarts = Int

-- |
type MaxSeconds = Int

-- | 
data SupervisorSpec = SupervisorSpec { 
      restartStrategy :: RestartStrategy, 
      allowedRestarts :: AllowedRestarts,
      maxSeconds :: MaxSeconds
    } deriving Show
                    
-- |
type Id = String

-- |
data Shutdown 
    = Shutdown Int 
    | Infinity
      deriving (Show)

-- |
data Restart
    = Transient 
    | Temporary 
    | Permanent
      deriving Show
      
-- |
data ChildType 
    = Worker
    | Supervisor
      deriving Show
      
type ChildName = String

-- |
data ChildSpec = ChildSpec { 
      childName :: ChildName, 
      childCallback :: (), 
      childRestart :: Restart, 
      childShutdown :: Shutdown, 
      childType :: ChildType
    } deriving Show
```

- how can we test these modules in isolation?","Comment:hyperthunk:10/29/12 10:55:36 AM:

Hi! Thanks for that feedback, it's very useful. Some thoughts inline:

> 
> First of all, each behavior could be in it's own module. This makes sense since it's one API per module used
> potentially by all the other modules. So, we should have a Control.Distributed.Platform.Supervisor module exporting at least the following functions:
>   - startLink

That's pretty much what I was thinking.

> Second, we could use type classes to represent callbacks. A type class specifies a well-defined interface that user-defined modules can implement. E.g.:

```haskell
module Control.Distributed.Platform.Supervisor where
    
class Supervisor s where
    toSpec :: s -> Maybe (SupervisorSpec, [ChildSpec]) -- ^ toSpec is a rename of init
```

> Here, s would be a typed version of Args. The problem here is that each different Supervisor will require a different > s, but for now I'll ignore this.

Yes, that's a bit of a bind. It's very likely that you'll want to supervise numerous different `Process` instances that are started in different ways (i.e., using different args). For example, consider the network layer supervisor for RabbitMQ:

```erlang
init({IPAddress, Port, SocketOpts, OnStartup, OnShutdown,
      AcceptCallback, ConcurrentAcceptorCount, Label}) ->
    Name = rabbit_misc:tcp_name(tcp_acceptor_sup, IPAddress, Port),
    {ok, {{one_for_all, 10, 10},
          [{tcp_acceptor_sup, {tcp_acceptor_sup, start_link,
                               [Name, AcceptCallback]},
            transient, infinity, supervisor, [tcp_acceptor_sup]},
           {tcp_listener, {tcp_listener, start_link,
                           [IPAddress, Port, SocketOpts,
                            ConcurrentAcceptorCount, Name,
                            OnStartup, OnShutdown, Label]},
            transient, 16_ffffffff, worker, [tcp_listener]}]}}.
```

If the elements in the `Spec` for the children that need to conform to a single type, then that is much harder.
Perhaps we can work around this by making the user pass a ready made `Closure`? Whilst this seems a bit annoying, including the start args using `(Serializable t1) => t1` isn't enough as we don't know what function to call from that alone.

> And the generic Supervisor module would define startLink as:

    startLink :: (Supervisor s) => s -> Process ()

> I am not sure about Process () here, but it seems like a good choice since startLink would (should?) be called in the context of a Process. 

Well `startLink` should **definitely** be called in the context of a process - the supervisor must have started up before it spawns any children, so that the link/monitor actions remain atomic and we avoid any nasty races in the (re)start sequence.

> In any case, startLink would be implemented along these lines:

```haskell
startLink s = do
    let spec = toSpec s
    (sid, sname) <- processSpec spec
    ....
    return ()
```

Surely we need to carry the `s` around with us in our process state, so we'd actually want something like `startLink :: (Supervisor s) => s -> Process s` instead?

> - It seems that an Erlang module can implement more than one behavior (?). This also works well with type classes.

I'm happy with the idea of using modules and type classes where we actually need something akin to behaviours, like the supervisor case.

> - Supervisor Data types.
> This seems to be most straightforward aside from specific refinements.

snip....

Yes, this is exactly right, bar (as you say) a few refinements.","Comment:edsko:11/05/12 10:15:28 AM:

Just a few brief remarks:

* You mention that ""Also, in Cloud Haskell, a link between the child and supervisor will cause both processes to exit"". That is not the case. Linking in Cloud Haskell is unidirectional. So, in a supervisor tree, it makes sense for the child to link to the supervisor, and the supervisor to monitor the child.

* Termination: If the supervisor is monitoring the child then it receives a monitor notification when the child exists; the monitor notification will inform the supervisor if this was a normal or abnormal termination, just like in Erlang.

* I am not convinced that type classes are the way to go here. Perhaps consider packaging up the relevant functions in a record instead.

* Leading on from the previous point, one thing one might consider if having *pure* ""processes"" instead, rather like the Map Reduce skeleton in my [http://www.well-typed.com/blog/73](blog post). This fits with the intended usage of Erlang behaviours, which are designed to separate out the ""pure"" and the ""concurrent"" parts; and of course, purity can be enforced by Haskell's type system. This leads to something akin to the ""Task"" layer in the original Cloud Haskell prototype.","Comment:hyperthunk:11/05/12 11:19:36 AM:

@edsko - thanks for this! 

> Just a few brief remarks:

> * You mention that ""Also, in Cloud Haskell, a link between the child and supervisor will cause both processes to exit"". That is not > the case. Linking in Cloud Haskell is unidirectional. So, in a supervisor tree, it makes sense for the child to link to the supervisor, > and the supervisor to monitor the child.

Ok that's a good point, but thankfully I was just being slow there. I had already assumed that startSupervised was doing that, so it looks like I'm on the right track there, at least in my head! :) 

> * Termination: If the supervisor is monitoring the child then it receives a monitor notification when the child exists; the monitor > notification will inform the supervisor if this was a normal or abnormal termination, just like in Erlang.

Cool - I just noticed something in the tests to that effect, so I'll build on what I can see there.

> * I am not convinced that type classes are the way to go here. Perhaps consider packaging up the relevant functions in a record > instead.

I did think about doing that, but I wondered whether the proper type signature would be hard to figure out. Having said that, **if** we make the init/startup data a Closure then I guess this isn't a problem.

> * Leading on from the previous point, one thing one might consider if having *pure* ""processes"" instead, rather like the Map Reduce skeleton in my [http://www.well-typed.com/blog/73](blog post). This fits with the intended usage of Erlang behaviours, which are designed to separate out the ""pure"" and the ""concurrent"" parts; and of course, purity can be enforced by Haskell's type system. This leads to something akin to the ""Task"" layer in the original Cloud Haskell prototype.

Thanks for pointing that out. I'll go through and read it as well as looking at the *Task* layer ideas too.
","Comment:edsko:11/05/12 11:19:36 AM:

mentioned","Comment:rodlogic:11/05/12 01:00:36 PM:

>
> Just a few brief remarks:
>
>
>    - I am not convinced that type classes are the way to go here. Perhaps
>    consider packaging up the relevant functions in a record > instead.
>
>  I did think about doing that, but I wondered whether the proper type
> signature would be hard to figure out. Having said that, *if* we make the
> init/startup data a Clojure then I guess this isn't a problem.
>
I also hit another issue with using type classes as the formal contract
between the ""pure"" and ""concurrent"" layers: type inference can't figure out
the proper message type. There is a chicken and egg problem here trying to
infer the type.

My main motivation for considering type classes as the pure-concurrent
contract was that it would become dead simple for anyone to implement a
server/task since even the signatures are figured out for you: just plug
your own types and provide an implementation.

>
>    - Leading on from the previous point, one thing one might consider if
>    having *pure* ""processes"" instead, rather like the Map Reduce skeleton
>    in my http://www.well-typed.com/blog/73 <http://blog%20post>. This
>    fits with the intended usage of Erlang behaviours, which are designed to
>    separate out the ""pure"" and the ""concurrent"" parts; and of course, purity
>    can be enforced by Haskell's type system. This leads to something akin to
>    the ""Task"" layer in the original Cloud Haskell prototype.
>
>  Thanks for pointing that out. I'll go through and read it as well as
> looking at the *Task* layer ideas too.
>
While looking at Erlang's Supervisor code this weekend, I noticed that it
is implemented in terms of the gen_server behavior, which enforces a
well-defined client/server protocol between the pure and concurrent layer
in a server (would a server in Erlang's world be the same as what you refer
to as a 'Task'?). This seems to be a critical area that we should be
focusing first, no?

The gen_server behavior in Erlang:
 * a server has a well-defined state-machine it goes through: it is spawn,
initialized, loop and terminate
 * a server can return a timeout on initialization that will shutdown the
process if no message is received with the specified time.
 * each server is implemented in a single Erlang module (convention)
 * a server module contains a public API part and internal callbacks for
each of the phases above
 * a server has/can have state maintained across callbacks for the duration
of the process.
 * a server supports one-way messages (simple send, no reply), a standard,
synchronous request/reply, and a request/reply were the reply is either performed
by another process or at another time.","Comment:hyperthunk:11/05/12 02:12:10 PM:

@rodlogic - you make a good point about the gen_server basis for OTP's supervisor. But....

The gen_server concept is quite different to implement in Haskell. Differentiating between for example handle_call, handle_cast and handle_info is done in OTP by the callback module providing implementations of these functions of the same name. Now how do you do this in Haskell - what is the type signature of those functions? Are you saying that the type signature will be `handleCast :: (Typeable a, StateTransition b) => a -> b` and what does the signature for `handleCall` look like, when we've got to return the state transition and the reply? What type does the `reply` have - is this just `Typeable a` or something? Because we know *nothing* about the messages that the server will receive in the general case, nor about the responses we're expected to handle sending back to the client.

So whilst you're right in saying that the gen_server concept of abstracting the concurrency model out from the implementation (how to do blocking versus non-blocking calls, timeouts, hibernation, etc), it's not at all clear to me what this would look like in Cloud Haskell.

Perhaps once I've read the links that @edsko posted it'll be clearer, and in general, yes I agree that if there is something like gen_server that would *fit the bill* for Cloud Haskell then we should build supervisors on top of this.   ","Comment:rodlogic:11/05/12 02:12:10 PM:

mentioned","Comment:edsko:11/05/12 02:12:10 PM:

mentioned","Comment:edsko:11/05/12 03:11:06 PM:

I think an approach based on a record tells people exactly what they need to implement; I really don't see the advantage of a type class here. For instance, consider the map reduce skeleton from my blog post that I mentioned earlier:

    data MapReduce k1 v1 k2 v2 v3 = MapReduce {
        mrMap    :: k1 -> v1 -> [(k2, v2)]
      , mrReduce :: k2 -> [v2] -> v3
     }

Now when people give an instance of a map-reduce skeleton they simply say

    example :: MapReduce <type variables>
    example = MapReduce { mrMap = .. ; mrReduce = .. }

and the types are clear (in the map-reduce example is probably more generic than `gen_server` needs to be because the types of the internal results can be different from the input and output types).

It's not obvious to me what the main problem is that makes it difficult to port Erlang's ideas to Cloud Haskell; is it the problem that we need to specify the type of the messages? In that case a new feature of Cloud Haskell might be useful:

    matchAny :: forall b. (AbstractMessage -> Process b) -> Match b

This can be used a process (such as the implementation of the `gen_server` equivalent) to match against a message of any type; at the moment, the only operation on `AbstractMessage` is `forward`, but other operations can easily be added (for instance, one might envision that we could `match` against an `AbstractMessage`). Does that help, or do I misunderstand the problem?","Comment:hyperthunk:11/05/12 05:04:25 PM:

> It's not obvious to me what the main problem is that makes it difficult to port Erlang's ideas to Cloud Haskell; is it the > problem that we need to specify the type of the messages? In that case a new feature of Cloud Haskell might be useful:

> matchAny :: forall b. (AbstractMessage -> Process b) -> Match b

> This can be used a process (such as the implementation of the gen_server equivalent) to match against a message > of any type; at the moment, the only operation on AbstractMessage is forward, but other operations can easily be added (for instance, one might envision that we could match against an AbstractMessage). Does that help, or do I misunderstand the problem?

No, that's precisely what we need. At the moment, it's awkward to design an API that processes *any* message in the abstract, whereas having matchAny would make it simple enough to pull them off the mailbox and shove them into the callback(s). That let's us get arbitrary values out of our inbox.  

Now please forgive me for being excruciatingly slow with the use of the type system here - it's not intentional! Given the pseudocode type/record definitions below, what is the return type of handle{Call,Cast} going to be? **That** is the bit I'm struggling with. Do we just have another type class which allows us to deal with the reply and/or stopping reason, or is there some other really obvious way to do this that I'm missing?

```haskell

class ServerStop a where
    reason   :: a -> String
    isNormal :: a -> Bool

-- pseudo code, just pointing out the use of ServerStop
data ServerReply (ServerStop b) => a = Reply a | Stop b | Hibernate Int

-- pseudocode
data ServerNoReply (ServerStop a) => a = Ok | Stop a | Hibernate Int

data Server = Server {
    handleCall  :: (AbstractMessage m) => m -> ServerReply ?T
  , handleCast :: (AbstractMessage m) => m -> ServerNoReply ?T
 }
```
","Comment:rodlogic:11/05/12 06:25:12 PM:

I have committed a few files I have been working on this weekend for your reference.

Here is one file that offers nothing new, but is a naive implementation of a simple Erlang server without any abstractions (i.e. using CloudHaskell directly and no reuse). I figure it is better to start bottom up to ground the progress in meaningful ways.

https://github.com/rodlogic/distributed-process-platform/blob/master/src/Control/Distributed/Naive/Kitty.hs

Note that I am ignoring completely some of the more advanced concepts such as timeouts, hibernation or multi-hop req/responses here.

And here is my first attempt at slicing Kitty.hs into a a generic Server.hs using type classes:

https://github.com/rodlogic/distributed-process-platform/blob/master/src/Control/Distributed/Naive/Server.hs

However, the more I think about this, the more I begin to appreciate even more Edsko's suggestion of sticking to a record representation. Type inference issues aside, I am not so sure a type class solution will ""scale"" to other behaviors we will need to address (e.g. gen_event or gen_fsm).


What about a different approach altogether (at least different to me coming from an imperative world): a functional reactive programming approach to implementing CouldHaskell servers/processes? I.e. a way declaratively and sequentially wire together 1 or more input event streams (ReceivePort's or expect msgs) and 0 or more event sinks (SendPort's or send msgs) in a pure do block. The same abstraction could be used to implement different kinds of processes from a simple one-way or req/resp server to much more complicated state machines. I am not even suggesting we use one of the FRP libraries out there: it could even be a simple design tailored to CloudHaskell. 

I will explore this FRP idea a bit more in the next few days, but if you have any feedback about such direction let me know. Note that part of my difficulties here has less to do with the current CloudHaskell API or abstractions but more about Haskell in general as I don't have a whole lot of experience with it compared to the experience I have in the OO world. I am, though, fully aware that my initial instincts tend to take me into a OO route, which is not a good thing here :-)
","Comment:edsko:11/06/12 03:36:15 PM:

> Now please forgive me for being excruciatingly slow with the use of the type system here - it's not intentional! Given the pseudocode type/record definitions below, what is the return type of handle{Call,Cast} going to be? That is the bit  I'm struggling with. Do we just have another type class which allows us to deal with the reply and/or stopping reason,  or is there some other really obvious way to do this that I'm missing?

I'm still really unsure what the benefit is of all these type classes that you guys are proposing. A type class is useful when you want some behaviour (in the generic, non-Erlang-technical, sense) to be implicit. ""Objects of this type are comparable, and I don't want to have to tell you how to compare them all the time"". In this case we are describing a process as a collection of callbacks; that calls for a record, not for a type class. You would need to invent a data type (like you are doing above) simply so that you can define a type class instance for it. What's the point?

So, AbstractMessage isn't a type class either; it's another record, so the type would be

    data Server = Server {
      handleCall  :: AbstractMessage -> ServerReply ?T
     }

instead. As regards the `?T` bit: I'm not sure. Again, why is ServerReply a type class? What would you normally return from handleCall in Erlang?","Comment:edsko:11/06/12 03:39:15 PM:

As regards FRP: I think that's too ambitious. You are not the first person to consider it -- there is a [recent blog post](http://awelonblue.wordpress.com/2011/05/21/comparing-frp-to-rdp/) about it -- but I think it would take us too far into the realm of academic research and too far from the realm of actually getting stuff done today.","Comment:rodlogic:11/06/12 03:45:01 PM:

Agreed. That is the same conclusion I came yesterday night after reading a bit on it. That kind of challenge is not really up my alley and a big distraction at this point.
","Comment:hyperthunk:11/06/12 03:59:22 PM:

@edsko this is good - now we're getting somewhere. I'm not interested in using type classes for this at all - I just can't figure out how to make the return type of the various handle_* functions general enough to hold *anything* that can be sent back to the user. 

>  As regards the ?T bit: I'm not sure. Again, why is ServerReply a type class? What would you normally return from handleCall in Erlang?

Well that's exactly the point - you would return any bit of data that you wanted sent back to the client, of any type. You'd also need to indicate whether you want to reply, stop or continue without replying - we'll need a type with the appropriate constructors for that presumably.

The type of the data being sent back can change between invocations of handle_call as well of course, because what the generic server does is something (conceptually in Erlang) like this:

```erlang

loop(Mod, State) ->
    receive
        {'$gen', call, {From, Msg}} ->
            case Mod:handle_call(Msg, From, State) of
                {stop, Reason} ->
                    terminate(Mod, State, Reason);
                {reply, Reply, State2} ->
                    erlang:send(From, Reply),
                    loop(Mod, State2);
                {noreply, State3} ->
                    loop(Mod, State3)
            end
    end
``` 

So the return value of the callback functions is very open - something like:

```erlang

-type sender :: {pid(), ref()}.
-type state :: any().
-spec handle_call(Msg::any(), From::sender(), State::state()) -> {'reply', any(), state()} | {noreply, state()} | {stop, any()}.
```

Does this mean that we should have a record that looks something like this?

```haskell
data Server = Server {
  handleCall :: AbstractMessage -> Typeable  -- or should it be Serialisable?
 }
```

And also, where would the server state come into this function's type? Presumably we need a type parameter somewhere, which maps to the `a` in `Process a` right? 


I hope that makes a bit more sense now. We need to go from reading an AbstractMessage to sending *something* back. The `data ServerReply` is just a convenience so we can handle the various `stop, reply, noreply, hibernate` state changes as well as the response data.","Comment:edsko:11/06/12 03:59:22 PM:

mentioned","Comment:edsko:11/06/12 04:47:19 PM:

Have a look at https://gist.github.com/4025934. Very simplistic, doesn't do any error handling or timeouts or anything at all really, but at least it explains the kind of structure I'd expect.","Comment:hyperthunk:11/06/12 05:21:20 PM:

Thanks @edsko that makes a bit more sense now. It was the use of Serializable that wasn't clear to me, and now I see the record doesn't need to specify too much type information which helps.","Comment:edsko:11/06/12 05:21:21 PM:

mentioned","Comment:edsko:11/06/12 05:25:41 PM:

Yeah. With a few minor additions to the AbstractMessage interface you could remove more type information; I've thought a bit about this and summarized it at https://github.com/haskell-distributed/distributed-process/issues/30_issuecomment-10119000. But, as I say in that comment, I actually think that the semi-typed version in that gist is better. After all, this is Haskell -- we should strive for *more*, not fewer, types :)","Comment:rodlogic:11/07/12 03:44:07 AM:

The above code sample gives us a good direction, indeed. A good basis for moving forward with the genserver design.

I have a few additional questions (related/unrelated to genserver/platform): 
* What is the logging strategy in distributed-haskell (and haskell in general)? Ideally with log levels, etc.
* Related to the above question: are there specific strategies you use for debugging/tracing process messages, mailbox queues, etc?
* How expensive are typed channels in distributed-haskell? Could they be used to create throw away/ad-hoc reply channels in situations where the client is either not a full-fledged process or doesnt have a permanent relation to the server?
","Comment:edsko:11/07/12 08:43:14 AM:

> What is the logging strategy in distributed-haskell (and haskell in general)? Ideally with log levels, etc.

There isn't any baked in. Cloud Haskell backends (or indeed `distributed-process-platform`) add this on top of the basic infrastructure. Good distributed logging is not an easy problem, so we didn't feel that it was appropriate to make it part of the core library (indeed, there may be many alternative approaches).

> Related to the above question: are there specific strategies you use for debugging/tracing process messages, mailbox queues, etc?

Hehe. My strategy currently is: modify the core Cloud Haskell library and stick print statements in the right place :-P I would agree that this is non-optimal! :) Please feel free to open an issue about this on the distributed-process wiki, and list some things that you'd like to have.

> How expensive are typed channels in distributed-haskell? Could they be used to create throw away/ad-hoc reply channels in situations where the client is either not a full-fledged process or doesnt have a permanent relation to the server?

They are cheap, and are certainly intended to be useful in scenarios such as the one you describe. ","Comment:hyperthunk:11/07/12 11:10:29 AM:

@edsko 

> Yeah. With a few minor additions to the AbstractMessage interface you could remove more type information; I've thought a bit about this and summarized it at haskell-distributed/distributed-processDPP-30. But, as I say in that comment, > I actually think that the semi-typed version in that gist is better. After all, this is Haskell -- we should strive for more, not fewer, types :)

Yes I totally agree, I just couldn't grok how to do the 'halfway typed' thing but I see now and it makes good sense as it is.

>  Good distributed logging is not an easy problem, so we didn't feel that it was appropriate to make it part of the core library (indeed, there may be many alternative approaches).

OTP doesn't do distributed logging either, but the System Architecture Support Libraries (badly named as SASL has other meanings) do provide a configurable logging subsystem. @rodlogic I'm inclined to just ignore logging for now and revisit it later.

> > How expensive are typed channels in distributed-haskell? Could they be used to create throw away/ad-hoc reply > > channels in situations where the client is either not a full-fledged process or doesnt have a permanent relation to > > the server?
>
> They are cheap, and are certainly intended to be useful in scenarios such as the one you describe. 

Could one of you guys gist an example of how that would work? I think it sounds like a very useful optional model for transient/one-off GenServer clients that aren't inside the Process monad.","Comment:edsko:11/07/12 11:10:29 AM:

mentioned","Comment:rodlogic:11/07/12 11:10:29 AM:

mentioned","Comment:hyperthunk:11/07/12 11:12:59 AM:

Oh and BTW @edsko presumably any exception thrown by a process is propagated as an exit signal to any process that is monitoring it right? I ask because as I'm playing around with your GenServer example the one thing I **don't** want to do is handle errors explicitly, as that's going to be the supervisor's job! :)","Comment:edsko:11/07/12 11:12:59 AM:

mentioned","Comment:hyperthunk:11/07/12 11:22:33 AM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/
","Comment:edsko:11/07/12 11:23:47 AM:

> Could one of you guys gist an example of how that would work? I think it sounds like a very useful optional model for transient/one-off GenServer clients that aren't inside the Process monad.

So in my `GenServer` example I send the server the Process ID of the client to reply. An alternative would be something like

    call :: (Serializable a, Serializable b) => Name -> a -> Process b
    call name request = do
      (sport, rport) <- newChan
      nsend name (sport, request)
      receiveChan rport

where the client creates a new typed channel, sends the server the send port, and waits on the receive port. This kind of use-once typed channels are very commonplace in process calculi.","Comment:rodlogic:11/07/12 11:24:04 AM:

@edsko Yes, a distributed logging would be nice but it is not an easy problem not to mention that it could be done in many different ways. I think that both distributed-process (dp) and distributed-process-platform (dpt) would benefit from a simple std logging facility even if that means that each OS process performs logging independently to stdout or a file. I will add a ticket for a basic logging facility with a few requirements that seem important.

@hyperthunk Considering that distributed logging is more of a platform feature, what about a ticket here so we can track it and collect comments over time?","Comment:edsko:11/07/12 11:24:04 AM:

mentioned","Comment:hyperthunk:11/07/12 11:24:04 AM:

mentioned","Comment:edsko:11/07/12 11:25:28 AM:

> > Oh and BTW @edsko presumably any exception thrown by a process is propagated as an exit signal to any process that is monitoring it right? I ask because as I'm playing around with your GenServer example the one thing I don't want to do is handle errors explicitly, as that's going to be the supervisor's job! :)

> If you are monitoring a process and that process throws an exception you receive a monitor notification, a message like another other, so I'm not sure what you mean by ""propagate"". If you want that an exception gets thrown in the ""monitoring"" process you should be using linking instead.

And how does the supervisor kill the children politely (i.e., when we're **not** opting for brutal_kill as the termination option) - I see the usual suspects in the exception handling API but not throwTo - nor does there appear to be an `exit(Pid, reason)` corollary.","Comment:edsko:11/07/12 11:25:28 AM:

mentioned","Comment:edsko:11/07/12 11:26:51 AM:

As I mentioned before, I don't think that logging should be added to the core Cloud Haskell infrastructure. However, a standard `distributed-process-logging` package, independent of `distributed-process-platform` might be something to consider.","Comment:Jiansen:11/16/12 02:28:40 PM:

Hi Tim,

Following are decisions made in akka (http://akka.io/   and    http://doc.akka.io/api/akka/2.0.4/)

-- how do we define the startup procedure? 
   Using the actorOf method, each ActorSystem (a supervision tree) has a specification, and actors are started inside an ActorSystem according to that specification.
 
-- what type does a start spec have???
   Akka defines a set of elements for Actor System Specification.  In Akka, the specification is parsed from a String.   Obviously, we could give each element a type and let the type of the specification be a list of the supertype of all element types.
   
-- how can we cleanly represent error signals in a generic way?
   Using subclasses of the PossiblyHarmful trait.


As the type system of Haskell is stronger than the type system of Scala, I think you need to do more work than what the akka team did, but the akka design may help your initial design.


Cheers
Jiansen
","Comment:hyperthunk:11/18/12 12:29:10 PM:

@Jiansen thanks for pointing that out - I am taking some inspiration from akka. You're right about the type system requiring a bit more *work* but that'll pay for itself later on. :)","Comment:Jiansen:11/18/12 12:29:10 PM:

mentioned","Comment:hyperthunk:12/04/12 10:17:07 PM:

So in supervisor, **are** we actually able to figure out whether the exit succeeded in the expected way or not? Since @rodlogic implemented *exit/kill* signals in Cloud Haskell (see [issue 69](https://github.com/haskell-distributed/distributed-process/issues/69)) we **should** be able to do something akin to this:

```erlang
shutdown(Pid, brutal_kill) ->
    case monitor_child(Pid) of
	ok ->
	    exit(Pid, kill),
	    receive
		{'DOWN', _MRef, process, Pid, killed} ->
		    ok;
		{'DOWN', _MRef, process, Pid, OtherReason} ->
		    {error, OtherReason}
	    end;
	{error, Reason} ->
	    {error, Reason}
    end.
```","Comment:rodlogic:12/04/12 10:17:07 PM:

mentioned",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2,what are OTP ideas that probably aren't a good fit?,"(i'm super green / known nothing on the erlang  & OTP side, and that'd be handy for understanding and looking at the OTP libs for ideas)",10/21/12 02:28:11 AM,01/14/13 10:07:51 PM,Task,Misc/Admin,Closed,Fixed,cartazio,misc/admin,"Comment:hyperthunk:10/22/12 12:20:33 AM:

Yes I totally agree. The **main** thing we need to get right to start with IMO is the supervisor concept. Once we've figured that out then a lot of the *good stuff* in OTP is about making it easy to write a process that deals with your application logic only, but can participate in a supervision tree in a *well behaved* fashion.

I suspect the primary area where our approaches will differ significantly from Erlang is that of *behaviours* - see the OTP [Design Principles](http://www.erlang.org/doc/design_principles/users_guide.html) documentation for an overview.

Many key features in OTP are built on the idea of a 'behavior' - perhaps best known is the [gen_server](http://www.erlang.org/doc/man/gen_server.html), which implements a kind of generic client/server mechanism where the user supplies a callback module to deal with various incoming messages.

There's not really anything analogous to this in Haskell, apart from session types perhaps, which are probably a bit too complicated to be useful at this stage. Whilst the `gen_server` concept makes a good attempt at abstracting some of the non-functional parts of the client server relationship away, it does little to support defining a contract between the two which goes beyond dealing with concurrency and the server life-cycle. Segregating the interface proper (as supported by the handle_{call, cast} from direct receipt (in handle_info) for example is quite useful, as is the provision of a standardised start and stop phase. But it all relies heavily on the fact that user supplied callback functions (which implement the 'functional aspects' of the process) can take *any data*, what with Erlang being dynamically typed.

So, the challenge would be to think about what types the gen server interface would provide. But I'm not even convinced this concept is hugely useful to us in Cloud Haskell. I suspect we'd be better off stealing the idea of a generic process that deals with a few non-functionals, such as

* handling the reply channel used to communicate with clients
* handling sync versus async traffic
* handling the start up
* handling the shut down
* dealing with *system messages* (i.e., for debugging purposes)

If we were to couple these with a nice API that hides the details from client code as well, then that would probably be quite useful. And furthermore I don't think it's nice that *every* call site wishing to interact with a 'generic process' should have to be pulled up into the Process monad and/or become a process itself. So having an API that allows you to spawn off a process and them provide a data exchange mechanism to unwrap return values would be very useful too IMO.

I also think that we should look at providing a generic process that handles certain pre-defined (of user configurable) process interactions, such as

* request/reply channels
* process pools
* work stealing

and so on.","Comment:hyperthunk:01/14/13 10:07:34 PM:

assigned","Comment:hyperthunk:01/14/13 10:07:51 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3,"Process Groups, Group Leaders",,11/07/12 11:19:12 AM,01/04/13 03:52:46 PM,Task,Initial-Public-Release,Open,Unresolved,hyperthunk,API,"Comment:hyperthunk:01/03/13 11:54:31 AM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4,GenServer,See issue #1 and https://gist.github.com/4025934,11/07/12 11:22:33 AM,01/20/13 01:47:36 AM,Task,Initial-Public-Release,Open,Unresolved,hyperthunk,enhancement API in-progress,"Comment:rodlogic:11/07/12 01:04:32 PM:

Continuing over here since it is purely genserver related...

How do I register a process name given I have a SendPort? Is there an easy way to extract the ProcessId from it?

I also could not find a timeout version of receiveChan.","Comment:edsko:11/07/12 01:10:07 PM:

> How do I register a process name given I have a SendPort? Is there an easy way to extract the ProcessId from it?

Yes, `sendPortProcessId . sendPortId`

> I also could not find a timeout version of receiveChan.

This is available in HEAD.","Comment:rodlogic:11/07/12 01:21:29 PM:

I am not 100% if this is an issue with the prototype code I have or an issue with spawnChannelLocal. Basically, spawnChannelLocal returns a SendPort a, but spawnChannelLocal is called from GenServer which is a step removed from the concrete types in Counter? Is this something we will have to defer to the Server callbacks too to make sure it is fully typed?

This is the compilation error in line 132 of GenServer.hs (https://github.com/rodlogic/distributed-process-platform/blob/master/src/Control/Distributed/Platform/GenServer.hs).
```
  Couldn't match type `rq' with `(ProcessId, rq)'
      `rq' is a rigid type variable bound by
           the type signature for
             serverStart :: (Serializable rq, Serializable rs) =>
                            Name -> Process (Server rq rs) -> Process (SendPort rq)
           at src/Control/Distributed/Platform/GenServer.hs:96:1
    Expected type: SendPort rq
      Actual type: SendPort (ProcessId, rq)
    In the first argument of `return', namely `sreq'
    In a stmt of a 'do' block: return sreq
```

@hyperthunk I am committing my experiments in the above fork, btw. Still putting my fingers on the different areas of distributed-process.","Comment:hyperthunk:11/07/12 01:21:29 PM:

mentioned","Comment:rodlogic:11/07/12 01:24:39 PM:

@edsko Is there a way to register the SendPort by name instead? If the server is using channels, the pid is a bit useless.","Comment:edsko:11/07/12 01:24:39 PM:

mentioned","Comment:edsko:11/07/12 02:12:51 PM:

> @edsko Is there a way to register the SendPort by name instead? If the server is using channels, the pid is a bit useless.

No. You could add your own registry for sendports if you wish.","Comment:edsko:11/07/12 02:12:51 PM:

mentioned","Comment:edsko:11/07/12 02:46:26 PM:

>  Is this something we will have to defer to the Server callbacks too to make sure it is fully typed?

No, that's not the issue; the logic in your code doesn't quite add up. On [line 123](https://github.com/rodlogic/distributed-process-platform/blob/master/src/Control/Distributed/Platform/GenServer.hs_L123) you ask for a message from the channel taht containts both the request and a process ID, but yet you claim in the type of the function that the channel only carries requests.","Comment:hyperthunk:11/08/12 10:16:52 AM:

@rodlogic - I like these ideas and think we should start to collaboratively work on GenServer now. I have some feedback I'd like to make so once you've got this compiling, please send me a pull request so I can comment on individual lines of code in the diff. Once we're merged I'll start writing some HUnit tests for it and we can split up some more fine grained tasks to work on individually.

I quite like the setup you've got now, bar some stylistic questions, especially the use of temporary channels which is really nice. Most of the things I'd like to suggest are renaming, and a bit of splitting up 'where' into separate functions. Some things we can add quite quickly once we've merged this (apart from tests!) include:

___ Tagging internal messages

OTP's gen_server differentiates between messages sent via call/cast and 'info' messages by sending the former in a tagged tuple `{'$gen', Msg}` and we can do the same easily. That way the server 'loop' can use `receiveWait` to drain the message queue and match on these easily, with something like

```haskell
type ReplyTo = ReceivePort
data CallRequest = CallReq Serialisable ReplyTo
data CastRequest = CastReq Serialisable
``` 
and instead of `expect` we can use

```haskell
receiveWait [ match (\(CallReq m replyTo) -> handleCall server m replyTo)
    , match (\(CastReq m) -> handleCast server m)
    , match (\(Serializable m) -> handleInfo server m)
    ]
```

When the callbacks return a `Timeout` in the reply/noreply data, we can defer to `receiveTimeout` instead.

___ An explicit asynchronous call API

As well as providing a `call` implementation, we might consider `beginCall` and `endCall` as well, with `beginCall` returning the `ReceivePort` and `endCall` doing the blocking receive (with an optional timeout). That way clients can choose to defer getting the result until they want it, whilst `call` can be defined in terms of `endCall . beginCall` or some such with a bit of glue around it.
 ","Comment:rodlogic:11/08/12 10:16:52 AM:

mentioned","Comment:rodlogic:11/08/12 12:55:31 PM:

@hyperthunk Sounds good. Let me get it into a running state again and then I'll send a pull request. I have paid no attention to the code structure so far and was more focused on the actual call interaction with expect/channels/etc. I am/was expecting we'll iterate this and refactor the code as necessary.

__ Tagging internal messages
If we go the typed-channels route, which imo we should unless we have specific reason not to, we will have to use receiveChan instead of expect/receiveWait and to handle multiple types of messages we will have to ""merge"" the different ports with mergePort{RR,Biased}. Aside from the order in which the ports are selected, I am assuming that the underlying semantics is the same as receiveWait's with multiple matches. Is that so?

Now, what about info messages? They seem to be used for 'all the other kinds of messages', but what kind of messages are expected there in Erlang? The gen_server.erl has simple examples like:

```erlang
handle_info(Info, State) Info is e.g. {'EXIT', P, R}, {nodedown, N}, ...
```

And from the documentation:
```
If the gen_server should be able to receive other messages than requests, the callback function handle_info(Info, State) must be implemented to handle them. Examples of other messages are exit messages, if the gen_server is linked to other processes (than the supervisor) and trapping exit signals.

handle_info({'EXIT', Pid, Reason}, State) ->
    ..code to handle exits here..
    {noreply, State1}.
```
Are there other specific examples of how info messages are used in Erlang?

__ An explicit asynchronous call API
Definitely. In the Java world is common to see APIs like: 
```
public interface Counter {
   int count();
   Future<Integer> countAsync();
}
```
And the Future interface, which I think is what you are more or less referring to with the difference that it hide the internals from the client process.
```
public interface Future<V> {
    boolean cancel(boolean mayInterruptIfRunning);
    boolean isCancelled();
    boolean isDone();
    V get() throws InterruptedException, ExecutionException;
    V get(long timeout, TimeUnit unit)
        throws InterruptedException, ExecutionException, TimeoutException;
}
```
I am sure Haskell has similar patterns. In it's simplest form, this Future value (the reply) will wrap the reply's receive port completely from the call. The only limitation here is that it won't be possible to send future values to other processes, but unless we have a clear and valuable use case it is not worth solving that.

Another point for us to consider and somewhat related to the async API for calls, is the handling of deferred reply's by the server process, which in Erlang I think happens when you return {noreply, State} from the handle_call callback. It would be nice to have a dual of Future that would represent the promise to send a typed reply back by any process and only once since there is a caller waiting for this reply. The server process's implementation could decide to send this promise to another process, which would then finally reply to the caller. For instance, instead of something like:
```
handleCall      :: req -> Process (CallResult reply)
```
it would look like: 
```
handleCall      :: req -> Promise reply -> Process (CallResult)
```
where Promise would be a simple function/API such as:
```
serverReply :: Promise reply -> reply -> Process ()
```

The server implemenation would have the option of simply calling serverReply and returning CallOk or sending the Promise to another process and returning CallDeferred. The promise wrapper will guarantee that serverReply can be called only once.

","Comment:hyperthunk:11/08/12 12:55:31 PM:

mentioned","Comment:rodlogic:11/08/12 03:01:15 PM:

Created a pull request: https://github.com/hyperthunk/distributed-process-platform/pull/5

","Comment:hyperthunk:11/08/12 03:57:28 PM:

@rodlogic good, I think we're on the same track here. Some thoughts...

> If we go the typed-channels route, which imo we should unless we have specific reason not to, we will have to use receiveChan instead of expect/receiveWait and to handle multiple types of messages we will have to ""merge"" the different ports with mergePort{RR,Biased}. Aside from the order in which the ports are selected, I am assuming that the underlying semantics is the same as receiveWait's with multiple matches. Is that so?

I **do** like the typed channels route, but it kind of kills off the handle_info concept, which is for handling 'unexpected' messages or (in other words) messages that aren't a primary part of this particular server's protocol, but are *possible* such as system messages (for dynamic debugging) and monitor signals and so on.

Also, we can't monitor other processes very easily in this way, because monitor signals come via expect/receiveWait rather than on channels. Basically the typical use of handle_info in OTP is to deal with processes that are *interested* in other processes, though perhaps not supervising them. Consider this example from RabbitMQ:

```erlang
handle_info({'DOWN', _MRef, process, {rabbit, Node}, _Reason},
            State = _state{monitors = Monitors}) ->
    rabbit_log:info(""rabbit on node ~p down~n"", [Node]),
    {AllNodes, DiscNodes, RunningNodes} = read_cluster_status(),
    write_cluster_status({AllNodes, DiscNodes, del_node(Node, RunningNodes)}),
    ok = handle_dead_rabbit(Node),
    {noreply, State_state{monitors = pmon:erase({rabbit, Node}, Monitors)}};
```

This is a classic example of receiving a monitor notification and doing some hidden internal state change in response without replying.

Personally, I think we should actually have two kinds of gen-server: one using channels and one using bare messages. They can probably share a lot of infrastructure, and you'll pick the bare messages one only if you need to do things like monitor and/or link handling.

___ Killing Processes (also needed for supervision)

Unless I'm *really* missing something, there doesn't appear to be a corollary to `erlang:exit(Pid, Term)` in the API, which means that in order to kill a process you have to dream up some protocol to allow (for example) supervisors to instruct their children to terminate and brutally kill them if they don't respond in a timely fashion by exiting.

One way to do this is to pass some `Exit reason` structure to the process, another approach would be to spawn a 'live' process to which all the children link, and then kill it when you need to terminate them abruptly. Anyway, we need to be able to kill processes in various ways.

The point is partly that we should prioritize 'shutdown' messages. If the Channel based GenServer is going to support this, then need to check we understand the semantics of `mergePort_` properly.

___ Futures

I like the things you've said about this: There is a similar set of concepts in the .NET world (BeginInvoke, EndInvoke).

> Another point for us to consider and somewhat related to the async API for calls, is the handling of deferred reply's by the server process, which in Erlang I think happens when you return {noreply, State} from the handle_call callback. It would be nice to have a dual of Future that would represent the promise to send a typed reply back by any process and only once since there is a caller waiting for this reply. The server process's implementation could decide to send this promise to another process, which would then finally reply to the caller. [snip]

Yes the 'reply later' option is very useful. I do wonder whether this will work more cleanly for a Channels based server API wise though.

Another thing Promises/Futures enable is the ability to construct higher order channels, dealing with things like delegation and proxying, which is very very useful (and important!) in building fault tolerant systems. One of the **key** things about OTP's gen_server is that the implementation is no longer in control of their own mailbox. This can be vitally important. Consider this wrapping code, for example, which deals with the common situation where you need fine grained control over a process that might get into a blocking state very easily - in this case we're managing an external resource (like an OS process) that could take *forever* to respond, but we **must** be able to interact with the wrapper in a timely fashion if we need to shut it down: https://github.com/nebularis/systest/blob/resource2/src/systest_resource.erl

Insulating yourself and providing middle-man processes to make sure that you can avoid deadlocks is vital in a complex actor based system, and I suspect doing that purely with Channels might be hard, though I'm willing to be persuaded otherwise.
","Comment:rodlogic:11/08/12 03:57:28 PM:

mentioned","Comment:hyperthunk:11/08/12 08:19:34 PM:

> Created a pull request: hyperthunk/distributed-process-platformDPP-5

@rodlogic thanks that's perfect - I'll try and give it proper attention over the next day or so and hopefully we can get it merged over the weekend.","Comment:rodlogic:11/08/12 08:19:34 PM:

mentioned","Comment:rodlogic:11/09/12 10:53:25 AM:

Now that we have some sort of a starting point, I was pondering a bit about the direction we are taking here and wondering if the GenServer design could be even simpler by leveraging more of Haskell's strengths.

Barred some of the limitations of the current design, which we will fix over many iterations, can we simplify even more what someone building a server has to implement? Can we make this 'design survace area' even smaller? I think it should be dead simple to create a server and the type system should keep me honest even if that means a bit more complexity under the covers.

What is a server, really? A server is nothing more than a Process with an id and one or more channels of interaction (in a more abstract sense). These channels are either a call (request/response) or a cast (one-way) and a client may choose to use a call channel synchronously or asynchronously, and a server may choose to 'reply later', delegate the reply to another server/process, or just reply right there. I am ignoring info messages here.

So what could be better in the ""Counter"" example from the point of view of who is designing and implementing it? Having to implement CounterRequest and CounterResponse smells like cruft to me and somewhat error prone. It would be nice if I could design Counter with mostly functions and have the rest inferred automatically somehow. For instance, the count service in the Counter server could be just:
```haskell
count :: () -> Int
```
This is already specifying that there is a Request () and a Response Int. Could this Request and Response message be automatically generated  for us? Isn't this was Closure in CloudHaskell is proposing/implementing?

In addition, it would be great if the following could also be automatically derived from the above definition (and implementation):
```haskell
countAsync :: () -> Future Int
```

And something like the following for the handler type:
```haskell
countHandler :: () -> Promise Int -> Maybe Int
```
","Comment:hyperthunk:11/09/12 04:57:10 PM:

@rodlogic I like these ideas in general, though obviously we'll need to understand what the plumbing looks like before we can generate any of it. :)

As the input to any GenServer callback function must be an instance of `Typeable` then we can presumably use `{g}cast` to differentiate between the messages passed at runtime and possibly using `funResultTy` match the possibilities against the supplied callbacks, leaving the implementor to simply list the functions they want to expose as callbacks. With a bit of magic (a la Template Haskell or some such) then we can probably just magically grab everything the callback module is exporting.

And yes, the return type should tell us whether we're dealing with a future/promise or an immediate reply. Of course there are servers that **never** reply, dealing only with casts and casts in general would need to be handled a little differently, but some indication that we're not replying shouldn't be too hard to dream up.

Of course we *always* need to deal with timeouts as well, so even if we do avoid making the implementation return a data type indicating whether they're replying or not, we will need to somehow deal with timeouts, hibernation and so on. Ideally we should be able to do this with a type, though I'm struggling to see how we'd wrap the return type (whether it is Int, Future Int or Promise Int) without forcing the author to do *something* in their code.

I suppose, and I shudder to suggest this, that we can have timeout/hibernate throw an exception containing the required span, but that feels **really** clunky and I'm sure there's a better way.

One thing though:

> What is a server, really? A server is nothing more than a Process with an id and one or more channels of interaction (in a more abstract sense). These channels are either a call (request/response) or a cast (one-way) and a client may choose to use a call channel synchronously or asynchronously, and a server may choose to 'reply later', delegate the reply to another server/process, or just reply right there. I am ignoring info messages here.

Yes, and I am holding to my point about info messages, which is that we probably need a non-channel based option for people who **do** care about info messages coming from monitors.","Comment:rodlogic:11/09/12 04:57:10 PM:

mentioned","Comment:hyperthunk:11/13/12 04:34:17 PM:

Guys, I'm getting a bit stuck with this. I do **not** want to specify the exact input type for my gen server, as this means that it can **only deal with one kind of input message**. Honestly, what use is that for writing a supervisor, that needs to handle the following instructions:

* Monitor notifications from the children
* add_child
* delete_child
* restart_child
* stop_children

So how can we write a generic process that accepts all these different types of input messages and handles them uniformly by evaluating a callback function? The callback **can** be written in terms of generic inputs, for example

```haskell
handleCall :: (Serializable m) => m -> ProcessId -> Process ()
```

We need to handle these **5 different input types** now, so how does a record with 1 input type help us at all? As I've said in [distributed-process issue 71](https://github.com/haskell-distributed/distributed-process/issues/71) this seems to completely defeat the purpose of having `expect` and `receiveWait` defined in such a way that we can receive **anything** because we can't use the messages without fully specifying the types we're dealing with.

Now I completely understand why this is the case - how on earth is the type system supposed to *guess* what we mean if we don't specify the types? I don't know if opening up `AbstractMessage` as I've suggested in that (previously mentioned) issue is the right thing to do or not. It feels to me like we're dealing with chalk and cheese here - the idea of gen server which deals with a totally open domain of input types just **doesn't fit** Haskell's type system, which is exactly what I felt to begin with.

Having a gen server that accepts just one type of input is fine, if all you want to do is centralize the error handling, timeouts and so on. If that's what we want to do, then the gen server infrastructure might be overkill.

___ How I arrived at this....

What I tried (over the weekend) to get this working was several things. First of all, I tried to stop limiting the input domain of the handleX functions to a specific type. As long as we don't mind not being able to use record accessors we can match out the handleX functions we need...

```haskell
-- we can *try* using existential types for this....
data Server s = forall m. (Serializable m) => Server {
    init             :: .....
    handleCall :: m -> state -> ProcessAction s
    state          :: s
  }

handleRequest :: (Serializable m) => Server s -> m -> ProcessAction s
handleRequest Server{ handleCall = hc } = hc

```

But is `Serializable` specific enough to use in `receiveWait` and can it be applied to data taken out of `expect` using an `m <- expect` expression? I tried **numerous** approaches here, included attempting to encode the input messages in various forms of other types.

So @edsko my question is, taking the following, which compiles (therefore **it must work!**) ;) are we on the right track and is there some type trickery I'm missing to use the record instead of the type class, because that completely bombed out for me, and what's the *neat* way of dealing with the `Maybe s` that receiveTimeout returns in our `handleRequest` implementation? 

```haskell
{-_ LANGUAGE ExistentialQuantification _-}
{-_ LANGUAGE FlexibleContexts          _-}
{-_ LANGUAGE RankNTypes                _-}
{-_ LANGUAGE DeriveDataTypeable        _-}
{-_ LANGUAGE ScopedTypeVariables       _-}
{-_ LANGUAGE TemplateHaskell           _-}
{-_ LANGUAGE TypeFamilies              _-}

module Control.Distributed.Platform.GenProcess where

import Prelude hiding (catch, init)
import Control.Distributed.Process
import Control.Distributed.Process.Serializable
import Control.Monad (forever)
import Control.Concurrent
import Data.Typeable (Typeable)
import Data.Binary
import Data.DeriveTH

type Name = String

data TimeUnit = Hours | Minutes | Seconds | Millis 
data Timeout = Timeout TimeUnit Int | Infinity  

data ProcessAction = 
    ProcessContinue
  | ProcessTimeout Timeout
  | ProcessStop String

data GenMessage m where 
    Message :: (Typeable m) => GenMessage (m, ReplyTo)
        deriving (Typeable)
  
data ReplyTo = ReplyTo ProcessId | None 
    deriving (Typeable, Show)

data Gen = Call | Cast
    deriving (Typeable, Show)

$(derive makeBinary ''ReplyTo)
$(derive makeBinary ''Gen)

class (Typeable s) => GenProcess s where
    init       :: Process () -> a -> s
    state      :: Process (s) -> s
    handleCall :: (Serializable m) => s -> m -> ReplyTo -> (ProcessAction, s)

serverContinue :: (GenProcess s) => s -> Timeout -> Process (s)
serverContinue s t = do
  case t of
    (Infinity)    -> receiveWait handlers
    -- (Timeout u v) -> receiveTimeout (timeToMs u v) handlers
  where handlers = [ (match (\(Call, m, r) -> handleRequest s m r)) ]

handleRequest :: (GenProcess s, Serializable m) =>
    s -> m -> ReplyTo -> Process (s)
handleRequest s m r = do
    let (action, s2) = handleCall s m r
    case action of
        ProcessContinue    -> serverContinue s2 Infinity
        (ProcessTimeout t) -> serverContinue s2 t

timeToMs :: TimeUnit -> Int -> Int
timeToMs Millis  ms  = ms
timeToMs Seconds sec = sec * 1000
timeToMs Minutes min = (min * 60) * 1000

reply :: (Serializable m) => ReplyTo -> m -> Process ()
reply (ReplyTo pid) m = send pid m
reply _             _ = return () 

replyVia :: (Serializable m) => SendPort m -> m -> Process ()
replyVia p m = sendChan p m
 
``` 

  
","Comment:edsko:11/13/12 04:34:17 PM:

mentioned","Comment:hyperthunk:11/13/12 10:15:15 PM:

> So @edsko my question is, taking the following, which compiles (therefore it must work!)

Urgh - I take that back, I just hadn't reconfigured recently enough. 

```
src/Control/Distributed/Platform/GenProcess.hs:53:47:
    Ambiguous type variable `m0' in the constraints:
      (Typeable m0)
        arising from a use of `handleRequest'
        at src/Control/Distributed/Platform/GenProcess.hs:53:47-59
      (Binary m0)
        arising from a use of `handleRequest'
        at src/Control/Distributed/Platform/GenProcess.hs:53:47-59
    Probable fix: add a type signature that fixes these type variable(s)
    In the expression: handleRequest s m r
    In the first argument of `match', namely
      `(\ (Call, m, r) -> handleRequest s m r)'
    In the expression: (match (\ (Call, m, r) -> handleRequest s m r))
```

So how do I handle messages in the general case? I can be *more specific* by using an existential type, but it's still too ambiguous:

```haskell

type GenMessage = forall m. (Typeable m, Serializable m) => m
-- snip

serverContinue :: (GenProcess s) => s -> Timeout -> Process (s)
serverContinue s t = do
  case t of
    (Infinity)    -> receiveWait handlers
    -- (Timeout u v) -> receiveTimeout (timeToMs u v) handlers
  where handlers = [ (match (\(Call, m, r) -> handleRequest s m r)) ]

handleRequest :: (GenProcess s) =>
    s -> GenMessage -> ReplyTo -> Process (s)
handleRequest s m r = do
    let (action, s2) = handleCall s m r
    case action of
        ProcessContinue    -> serverContinue s2 Infinity
        (ProcessTimeout t) -> serverContinue s2 t
``` 

will still yield the same error. So do we actually *need* an API on `AbstractMessage` in order to do this then? Or is there some `Typeable` magic I can do here?

Interestingly, I wonder if instead of trying to define `handleCall` just once for the gen server, we should take the gen server state's type and a list of possible handlers, each being a pure function mapping from some domain of inputs to a `ProcessAction state` instead. That would alleviate the problem of knowing a single `handleCall` input type and we should be able, in theory, to pass these to `receiveWait` verbatim and get back the `ProcessAction s` for further inspection. 
","Comment:edsko:11/13/12 10:15:15 PM:

mentioned","Comment:rodlogic:11/14/12 02:51:31 AM:

@hyperthunk I have been hitting similar issues trying to improve GenServer in different ways:
* Generic dispatching of messages
* N message types per server
* Non-blocking client API (Future/etc).

```
So how can we write a generic process that accepts all these different types of input messages and handles them uniformly by evaluating a callback function?
```
The simplest answer is to force servers to use a sum type: e.g.:
```haskell
data SupervisionRequest
   = AddChild
   | DeleteChild
   | RestartChild
   | StopChildren
```
However, this will get cumbersome very quickly and is not really a solution considering the different types of messages a single server may need to handle. Besides this approach would mean that there would be no way to reuse protocols across servers without writing wrapper types. I think **we are on the same page here: GenServer has to support dispatching any number of different message types**.

```
Now I completely understand why this is the case - how on earth is the type system supposed to guess what we mean if we don't specify the types? I don't know if opening up AbstractMessage as I've suggested in that (previously mentioned) issue is the right thing to do or not. It feels to me like we're dealing with chalk and cheese here - the idea of gen server which deals with a totally open domain of input types just doesn't fit Haskell's type system, which is exactly what I felt to begin with.
```
I am also struggling with getting the dynamic dispatching right. However, I don't think this has anything to do with Haskell type system, but with our knowledge (or lack) of it. I can vaguely conceive a solution for this but I just can't get it to work properly. We need to construct a list of handlers that encapsulate the message type using an existential and then have a function associated with this handler that returns a Match type, which is what we need for receiveWait/receiveTimeout (note that the **match** primitive is also using an existential and hiding the original type from receiveWait/Timeout). The key thing is that who needs to return this Match type is each handler in the list since only the individual handler knows it's type hidden by the existential. That is the gist of what I a pursuing right now. 

```
But is Serializable specific enough to use in receiveWait and can it be applied to data taken out of expect using an m <- expect expression? I tried numerous approaches here, included attempting to encode the input messages in various forms of other types.
```
Serializable is also Binary and Typeable. Afaik, CloudHaskell's marshalling is generating a fingerprint of the message type using Typeable and this fingerprint is then compared when unmarshalling the message on the other end. So, as long as CloudHaskell can compute a fingerprint of the receiving type with the fingerprint that was unmarshalled, the receive/expect should work fine.

I am close to a stable set of changes after tinkering with this for a few days. I will share what I have as soon as I clean it up a bit.


","Comment:hyperthunk:11/14/12 02:51:31 AM:

mentioned","Comment:hyperthunk:11/14/12 07:44:57 AM:

mentioned","Comment:hyperthunk:11/14/12 07:53:14 AM:

Well....
> @hyperthunk I have been hitting similar issues trying to improve GenServer in different ways:

Maybe I'm wrong but I suspect this will require either some clever redesign on our part or opening up of AbstractMessage in CH.

> Non-blocking client API (Future/etc).

That doesn't seem so hard *if* we can solve the need for handling types we know nothing about.

> The simplest answer is to force servers to use a sum type: e.g.: 

In fact, I don't see how that solves the problem at all. You **still** have to write the code to either expect that specific type (which means its not a general purpose server) or pass in the input type(s) to the record or type class so that the code which uses them can be type checked properly.

> I am also struggling with getting the dynamic dispatching right. However, I don't think this has anything to do with Haskell type system, but with our knowledge (or lack) of it. I can vaguely conceive a solution for this but I just can't get it to work properly. We need to construct a list of handlers that encapsulate the message type using an existential and then have a function associated with this handler that returns a Match type, which is what we need for receiveWait/receiveTimeout (note that the match primitive is also using an existential and hiding the original type from receiveWait/Timeout). The key thing is that who needs to return this Match type is each handler in the list since only the individual handler knows it's type hidden by the existential. That is the gist of what I a pursuing right now. 

Oh no, there's nothing wrong with Haskell at all. I'm very familiar with the ocaml type system though less with Haskell but this constraint seems perfectly sensible to me. We just need to figure out what type to use to open up the API awe want. The list of marchers is close to what what I had in mind too, but you still can't write a function that operates on the existential. What this means is that each handler has to return the ProcessAction (state) and we write the handleRequest function (which is called after we've matched something) to decide on the looping and timeout.

> ... is also Binary and Typeable. Afaik, CloudHaskell's marshalling is generating a fingerprint of the message type using Typeable and this fingerprint is then compared when unmarshalling the message on the other end. So, as long as CloudHaskell can compute a fingerprint of the receiving type with the fingerprint that was unmarshalled, the receive/expect should work fine.

Great, but remember that the decision about being able to use a type in expect/receiveX is based on the clauses the handlers provide. We shouldn't need to care about that.

> I am close to a stable set of changes after tinkering with this for a few days. I will share what I have as soon as I clean it up a bit.

Great. I'm going to experiment with the same idea (list of handlers that produces [Match (ProcessAction)] in a separate module so as to minimise merge conflicts.","Comment:rodlogic:11/14/12 03:37:26 PM:

@hyperthunk Just as a quick side note re: ""..., but with our knowledge (or lack) of it"". Change that 'our' to a 'my' as I don't have a lot of practical experience with Haskell (nor ML) and was speaking with myself in mind.

__ 2nd GenServer iteration
I have committed another iteration of the GenServer based on the experiments from the past few days. At least back into a stable state after much tinkering so I can take a bit more space to consider the code you sent above.

This commit (the past 2 actually) reverts to using process messages as opposed to channel messages. I am assuming the implementation could be changed to support typed-channels by merging the channels, but considering that it may not be possible to receive process and channels messages at the same time I am keeping it simple for right now.

It also abstract the two basic protocols of gen_server, i.e. calls and casts, and assumes that specific variations will be based on either one of those (e.g. a AbstractMessage support would be implemented using the cast protocol, iow just simple, one-way messaging; the same for info messags, i.e. they come in and conform to the cast protocol.

See their types:
```haskell
type CallHandler a b        = a -> Process (CallResult b)
type CastHandler a          = a -> Process CastResult

data CallResult a
    = CallOk a
    | CallForward ServerId
    | CallStop a String
        deriving (Show, Typeable)

data CastResult
    = CastOk
    | CastForward ServerId
    | CastStop String
```

I think this is in line with your code snippets above. I.e. by separating the cast and call result types we can enforce a certain invariants: No way to send a reply in a CastHandler (I am assuming we can wrap these handlers in a monad to better control what can go on inside them), only process the message, stop the server or forward it. Or, in the case of the CallHandler, you are forced to either generate a CallOk with the reply, of forward the request to another server with a CallForward, or stop the server with a CallStop.

```haskell
handleReset ResetCount = do
    return $ CastForward mySlaveServerId
```

The code is working for the simple Counter example and shows how the GenServer can be used with a synchronous CallHandler, an async CastHandler, and two separate data types: CounterRequest and ResetCount. For example:

```haskell
-- Handler for the CounterRequest call
handleCounter IncrementCounter = return $ CallOk (CounterIncremented)
handleCounter GetCount = return $ CallOk (Count 0)

-- Handler for the ResetCount cast
handleReset ResetCount = return $ CastOk
```

__ Support for AbstractMessage?
I also added an experimental **handleAny** to deal with untyped messages based on CloudHaskell's AbstractMessage. Not sure if it works, but shows another flexibility of the design (or so it seems).

__ Threading state through the handlers
There is more refinements needed but the next todo there seems to figure out how to thread the server state through the handlers/callbacks. The approach I am pursuing now is to define a Server monad that is really a StateT monad wrapping the Process monad. This is immediately useful for managing the server state, but could also possibly be used to create additional DSLs on top of it (not sure).

```haskell
type Server s = StateT s Process

type InitHandler s            = Server s InitResult
type TerminateHandler s  = TerminateReason -> Server s ()
type CallHandler s a b      = a -> Server s (CallResult b)
type CastHandler s a        = a -> Server s CastResult
```
And a sample CastHandler:
```haskell
handleReset ResetCount = do
    state <- get
    -- do something with the state
    put state
    return $ CastOk
```

I am stuck now trying to figure out where to store this state **s** so that the MessageDispatcher can access it. The first iteration of GenServer was using a closure to do that but now I have N MessageDispatcher that share the same state and no closure around them.
","Comment:hyperthunk:11/14/12 03:37:26 PM:

mentioned","Comment:hyperthunk:11/14/12 03:59:04 PM:

> @hyperthunk Just as a quick side note re: ""..., but with our knowledge (or lack) of it"". Change that 'our' to a 'my' as I don't have a lot of practical experience with Haskell (nor ML) and was speaking with myself in mind.

I didn't mean to be touchy though - I've been struggling with some of the finer details here too. :)

So..... looking at your code, I think you're generally going in the right direction. Feel free to send a pull request when you're ready - I'm doing some experiments in parallel in a different module, so we shouldn't have any clashes.","Comment:hyperthunk:11/14/12 03:59:04 PM:

mentioned","Comment:rodlogic:11/14/12 07:37:29 PM:

@hyperthunk Ok, the pull request is in. I will not have time for this until the weekend.","Comment:hyperthunk:11/14/12 07:37:29 PM:

mentioned","Comment:hyperthunk:11/14/12 08:50:10 PM:

> @hyperthunk Ok, the pull request is in. I will not have time for this until the weekend.

That's absolutely fine, I'm doing this in my spare time too and your input is most welcome and appreciated! :)","Comment:edsko:11/18/12 10:05:02 AM:

Sorry guys, the Parallel Haskell project has come to an end and so I will only be able to look at this in my spare time, of which I have very little. You already seem to have made progress with this, but let me just explain something, perhaps it will help, because the 'expect' matching (or 'receiveTimeout' and co) is indeed a little confusing. You should think of  

```Haskell
expect :: Serializable a => Process a
```

which is really

```Haskell
expect :: (Typeable a, Binary a) => Process a
```

as having an (implicit) argument

```Haskell
expect :: Binary a => Fingerprint -> Process a
```

That `Fingerprint` comes from the `Typeable` class and it is what `expect` uses to find the right message in the process mailbox. Without that `Fingerprint` it will not be able to locate the message, and this materializes as a Haskell type error. So in the `GenServer` that suffered from the ambiguous type error it needed to decide on a type to instantiate `handleCall` and find the appropriate `Typeable` instance to go with it, so that it could extract the right message from the mailbox.

Introducing an existential type along the lines of

```Haskell
data Foo :: * where
  mkFoo :: forall e. Serializable e => ... -> Foo
```

*and then matching on `mkFoo`* introduces a type variable, but more importantly, it brings a `Typeable` instance into scope *and hence* a type `Fingerprint`. That `mkFoo` is really:

```Haskell
data Foo :: * where
  mkFoo :: forall e. Serializable e => `Fingerprint` -> ... -> Foo
```

So Cloud Haskell's `AbstractMessage` type is a layer of abstraction around an internal `Message` type which carries that `Fingerprint` with it. At the moment the `AbstractMessage` interface is incredibly minimal (all you can do is forward them) but we could add support for

```Haskell
wrap :: Serializable a => a -> AbstractMessage
matchAgainst :: [Match a] -> AbstractMessage -> Process (Maybe b)
```

and moreover make `AbstractMessage` itself an instance of `Serializable` (this is described in https://github.com/haskell-distributed/distributed-process/issues/30). I don't have time for this at the moment but pull requests are welcome :) 

Note by the way that making `AbstractMessage` an instance of `Serializable` is a bit confusing: `send`ing an `AbstractMessage` would require the remote end to `expect` an `AbstractMessage`, but `forward`ing an `AbstractMessage` which is really an `Int`, say, would require the remote process to match to `expect` an `Int`.
    ","Comment:hyperthunk:11/18/12 12:22:14 PM:

@edsko totally understood - we're all doing this on the side. I'll liaise with @rodlogic and we might potentially contribute some stuff to distributed-process a bit later on, though for the time being I think we'll try to build up some of the basic infrastructure with what's available now.","Comment:hyperthunk:11/19/12 09:39:24 AM:

> There is more refinements needed but the next todo there seems to figure out how to thread the server state through > the handlers/callbacks. The approach I am pursuing now is to define a Server monad that is really a StateT monad wrapping the Process monad. This is immediately useful for managing the server state, but could also possibly be used > to create additional DSLs on top of it (not sure).

I had a play with this too and found it pretty awkward. The dispatchers need to take `StateT ... {args...}` but the state constructor needs a parameter (say `s` for example) and that either complicates the existential or introduces a parameter to `Dispatcher` which makes **all the consuming code** rather more complicated.

> I am stuck now trying to figure out where to store this state s so that the MessageDispatcher can access it. The first iteration of GenServer was using a closure to do that but now I have N MessageDispatcher that share the same state and no closure around them.

Yes exactly. One approach I tried was along the lines of

```haskell
data Dispatcher s =
    forall a . (Serializable a) =>
    Dispatch    { handler    :: s -> Message a -> Process () }
  | forall a . (Serializable a) =>
    DispatchIf  { handler    :: s -> Message a -> Process (),
                  predicate  :: s -> Message a -> Bool }
  | DispatchAny { abstractMessagehandler :: s -> AbstractMessage -> Process () }

data GenProcess s = GenProcess {
    procInit         :: InitHandler s,      -- ^ initialization handler
    procDispatchers  :: [Dispatcher s],       -- ^ request dispatchers 
    procTerminate    :: TerminateHandler    -- ^ termination handler
  }
```

But as soon as you want to initialize the state your `InitHandler s` constructors come into play. So you want to go through a chain somewhat like `GenProcess s -> Process (InitResult s) -> ProcessState s` or some such, but that hardly feels very clean. Perhaps your idea of letting the handlers (init, call/cast, terminate) deal with the state themselves is better, but from previous excursions using the state monad I had expected to deal with this in the outer (control) functions and pass it into the handlers so they look more like `Dispatch { handler :: StateT s -> Message a -> ... }` and so on.","Comment:rodlogic:11/22/12 04:06:03 AM:

@edsko thanks for the additional info and details.

@hyperthunk there is another pull request with another iterative improvement to GenServer: now threading server state through the handlers (init, handle, terminate). The code could probably be much better, but at least it compiles and runs and it keeps us moving forward.","Comment:rodlogic:11/22/12 01:43:58 PM:

@edsko @hyperthunk Committed a few additional changes to support the handling of CallStop and CastStop (this is when the server can instruct a termination after handling the call/cast).

I am a bit unsure about the following:
* Call timeout's - how should the client API handle that?
At this point, the GenServer call API is throwing an error, which is obviously wrong:
```haskell
callServer :: (Serializable rq, Serializable rs) => ServerId -> Timeout -> rq -> Process rs
```
One option would be to change the result type to **Process (Maybe rs)** and another one would be to throw an exception instead. How is this usually handled in Erlang?

* Server termination
The server can terminate for a variety of reasons (CallStop, CastStop, termination by supervisor, etc), but how should this be notified back to the client? In the current Counter.hs example, the increment counter handler is stopping the server when the counter is great than 10, but, after that, the client is hanging, waiting for a response after sending the request. One simple answer would be to use a timeout when waiting for a response, but this doesnt seem like an acceptable answer. If the server is dead or doesnt exist, we should fail-fast and throw an exception, at least. How should we handle this case? Should CloudHaskell throw an exception when sending messages to a terminated or non-existent ProcessId?

* Async client API
Blocking while waiting for a call response may work for simple client/server interactions, but not for more complex protocols where a server may also act as a client and possibly have multiple pending calls. Ideally, call responses should come back as part of the server receive loop. Here is a straw man client API:

```haskell
-- | Sync call (in terms of callAsyn + waitReply)
call :: ServerId -> a -> Timeout -> Process (Either SomeException a)

-- | Asynchronous call to server
callAsync :: ServerId -> a -> Process (Future b)

-- | Wait for a reply blocking if necessary
waitReply :: Future a -> Timeout -> Process (Either SomeException a)

-- | Poll a future to see if there is a reply without blocking
pollReply :: Future a -> Process (Maybe (Either SomeException a))

-- | Cancel a future
cancelReply :: Future a -> Process ()
```
","Comment:rodlogic:11/27/12 02:28:24 AM:

@hyperthunk 

``` 
I had a play with this too and found it pretty awkward. The dispatchers need to take StateT ... {args...} but the state constructor needs a parameter (say s for example) and that either complicates the existential or introduces a parameter to Dispatcher which makes all the consuming code rather more complicated.
```
```haskell
data Dispatcher s =
    forall a . (Serializable a) =>
    Dispatch    { handler    :: s -> Message a -> Process () }
  | forall a . (Serializable a) =>
    DispatchIf  { handler    :: s -> Message a -> Process (),
                  predicate  :: s -> Message a -> Bool }
  | DispatchAny { abstractMessagehandler :: s -> AbstractMessage -> Process () }

data GenProcess s = GenProcess {
    procInit         :: InitHandler s,      -- ^ initialization handler
    procDispatchers  :: [Dispatcher s],       -- ^ request dispatchers 
    procTerminate    :: TerminateHandler    -- ^ termination handler
  }
```
Yes, threading the state explicitly adds noise dispatcher/handler functions and another reason to got for a StateT Monad, imo, to thread that state in and out of the handlers.

```
But as soon as you want to initialize the state your InitHandler s constructors come into play. So you want to go through a chain somewhat like GenProcess s -> Process (InitResult s) -> ProcessState s or some such, but that hardly feels very clean. Perhaps your idea of letting the handlers (init, call/cast, terminate) deal with the state themselves is better, but from previous excursions using the state monad I had expected to deal with this in the outer (control) functions and pass it into the handlers so they look more like Dispatch { handler :: StateT s -> Message a -> ... } and so on.
```

What do you think of the following as the contract between GenServer/Process and the user-defined handlers?
```haskell
type InitHandler s           = Server s InitResult
type TerminateHandler s       = TerminateReason -> Server s ()
type CallHandler s a b        = a -> Server s (CallResult b)
type CastHandler s a          = a -> Server s CastResult
```
The Server monad is just a StateT type alias that wraps the Process monad. The only part that deserves a bit more attention is where we have to call receiveWait/Timeout since that is in the Process monad and not in the Server monad. Apart from that it is quite clean and the best I can come with so far.

For example, implementing the handler that returns the current count in the counter server would look like:
```haskell
handleCounter GetCount = do
  count <- getState
  return $ CallOk (Count count)
```
Then the startCounter API would hook the handlers up with the following:
```haskell
startCounter :: Int -> Process ServerId
startCounter count = startServer count defaultServer {
  msgHandlers = [
    handleCall handleCounter,
    handleCast handleReset
]}
```
And the handleCall and handleCast would wrap the handler in a Dispatcher data type and set up the whole server process. There may be opportunities to simplify this a bit further with some helper functions or smart constructors, but that would be something minor at this point.

I also added an additional example very similar to the Counter server just as a 2nd exercise.

_ Next steps?

Now, where should we go from here? Unless you have very different ideas on how the GenProcess/Server should look like, it seems that we should give a 1st shot at the Supervisor module using this GenServer and fill any gaps we may find along the way. Any thoughts?

","Comment:hyperthunk:11/27/12 11:09:23 AM:

@rodlogic - first of all, thank you so much for your ongoing contributions; you've really pushed this forward and I think you've pretty much cracked the gen server API for the most part!

For my own part, I've been trying to reduce the amount of boilerplate required by the implementation code, and the reason I've not committed any of those changes is so as to avoid merge conflicts with your work (which is generally more complete and compiling properly than mine!) :)

> What do you think of the following as the contract between GenServer/Process and the user-defined handlers?

I think the API is just fine. One of the things I'm attempting to do is to reduce the amount of indirection between the server process and the handlers API, by returning an 'Action' (which is basically your `{Call,Cast}Result` type) directly from the handlers and dealing with the instruction set in `processReceive` rather than in the `handle{Call,Cast}` wrappers. This yields type signatures more like these:

```haskell
init :: Behaviour s -> Server s InitResult

loop :: Behaviour s -> Timeout -> Server s TerminateReason

processReceive :: [Dispatcher s] -> Timeout -> Server s (Either ProcessAction TerminateReason)
```

I'm also interested in whether the call/cast handlers can be made into pure functions and wrapped (by the API entry points) in the state monad, so that we get something more like

```haskell
data CastResult s =
    CastOk s
  | CastForward s ServerId
  | CastStop s String

type CastHandler s a = a -> CastResult s
```

Keeping the `Server` monad hidden from the implementor feels simpler to me, though it makes our code more complicated (and I've been having **fun** trying to implement that). 

Anyway, these are minor (and mainly cosmetic, from the API perspective) things and in general I think we will move forward with the gen server implementation you've come up with and if I manage to do these **simplifications** - and if they do turn out to be simpler, rather than more complicated - then we'll do some refactoring then.

> Now, where should we go from here? Unless you have very different ideas on how the GenProcess/Server should > look like, it seems that we should give a 1st shot at the Supervisor module using this GenServer and fill any gaps > we may find along the way. Any thoughts?

Yes, I agree we should move on to looking at supervisor now. As I said above, I'll bring those *simplifications* to gen server in on a branch and discuss them with you there before merging anything.

I think that before we can actually implement supervisor properly however, that we need to solve [distributed-process issue 69](https://github.com/haskell-distributed/distributed-process/issues/69), as without a way to kill processes we're going to struggle to implement supervision properly. I noticed that you've created a `Management` message to handle requesting termination, but I really don't think that we can sensibly force people to implement everything as a gen server just so that it can be supervised.

So what I propose is that we take a look at the *Cloud Haskell* [issue](https://github.com/haskell-distributed/distributed-process/issues/69) and see if we can solve it and contribute a pull request. This seems to involve having the node controller listen for a new kind of message (`Kill ProcessId DiedReason`) and throwing and exception (using `throwTo` afaict) to the local process. I haven't started to dig into this yet, but apparently the cloud haskell paper describes this so it seems pretty fundamental.
","Comment:rodlogic:12/04/12 03:27:55 PM:

@hyperthunk CH issue DPP-69 is behind us and a good base for us to continue. It was a great move, btw, to raise this as a CH issue.

Regarding the API, I am running out of ideas now on how to improve it and imo we are close enough. I also think that the implementation needs a few iterations to remove more cruft and simplify, but maybe not a good idea to get stuck there now. So giving the Supervisor a first shot seems a great next step.

I have also incorporated some additional folders/files/content to deal with tests, code coverage and benchmarking. There is nothing there at this point, but a small step forward. Please take a look when you have a chance and I can send a pull request if it makes sense.","Comment:hyperthunk:12/04/12 05:21:28 PM:

@rodlogic yes I think it was the right place to deal with that issue.

So regarding the API, I've just pushed a [new branch](https://github.com/hyperthunk/distributed-process-platform/tree/gen-process) which is worth a look. What I've done there is to leave your gen server alone, but add a corollary implementation called GenProcess.

This (gen process) is conceptually very similar to gen server, although it's somewhat less finished. The general idea behind it is that we keep the process loop, error handling (where we decide to catch whatever) and `reply` functionality in that API, which can be used to implement other kinds of generic process (like gen-FSM) as well as providing hooks that non-gen_server processes can call to *enter a gen server loop* and become *managed*. My other motivation here is that I'd like to provide means for server authors to write pure functions and have them automatically lifted into handlers/callbacks using template haskell or something of that ilk.

On this branch I was planning to change the GenServer module to add a bit of boilerplate that provides the Call/Cast abstraction (and the call/cast APIs for clients to use) over the top of GenProcess.   

> So giving the Supervisor a first shot seems a great next step.

Yes definitely. The catch/exit-handling needs to be put into the gen server first. I'm quite happy if you want to do that against master for now, as the changes in the gen-process branch aren't complete and I don't want to hold us up.

> I have also incorporated some additional folders/files/content to deal with tests, code coverage and benchmarking. There is nothing there at this point, but a small step forward. Please take a look when you have a chance and I can send a pull request if it makes sense.

That sounds very sensible. Please go ahead and send a pull request against master.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5,GenServer straw man,,11/08/12 02:58:34 PM,11/09/12 05:19:00 PM,Task,,Closed,Fixed,rodlogic,,"Comment:rodlogic:11/08/12 03:01:15 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/
","Comment:hyperthunk:11/09/12 05:18:01 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/22443e7bcd786f40b099a5dc0599ff46c04bda01
","Comment:hyperthunk:11/09/12 05:18:01 PM:

merged","Comment:hyperthunk:11/09/12 05:18:01 PM:

closed","Comment:hyperthunk:11/09/12 05:19:00 PM:

I'm merging this as-is for now, but let's get some smaller tasks in place so we can make a bit of progress. I suggest we break up the conversations we're having into smaller, more manageable issues, and then pick them up one at a time.","Comment:ericbmerritt:01/09/13 02:32:36 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/3a4415a18dd95f3a0583ae78d1431c9949e22e22
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6,support for handleCast,It is not necessary to always reply to a sender **at all** and we should support that notion.,11/09/12 05:19:54 PM,12/13/12 09:25:11 PM,Task,,Closed,Fixed,hyperthunk,duplicate enhancement,"Comment:rodlogic:12/13/12 02:02:31 PM:

I am wondering to what extent the current GenServer API already handles this appropriately or is there more to it?","Comment:hyperthunk:12/13/12 09:24:53 PM:

Yes I agree that we've already dealt with this.. Resolving as a duplicate.","Comment:hyperthunk:12/13/12 09:25:11 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7,Channel vs Process based GenServer,"I don't know if a full split is necessary or even a good idea, but for `handleInfo` and in particular in order to deal with messages sent to the process by monitors, we cannot be entirely oriented around typed channels. ",11/09/12 05:20:49 PM,01/04/13 03:52:56 PM,Task,Initial-Public-Release,Open,Unresolved,hyperthunk,enhancement API in-progress,"Comment:rodlogic:11/09/12 09:50:53 PM:

Yes, this is an important topic for discussions since it could affect the core GenServer design. I don't have a strong opinion about this either way at this point so I need to do some homework.

In the interest of gathering some additional data for my own understanding, here is a dull search for handle_info usage in Riak's code base (distinct tags: inet_async, 'EXIT', _ReqId, gen_event_EXIT, 'DOWN', tcp_closed, tcp_error, tcp, ssl_closed, ssl_error, ssl, nodeup, nodedown):

https://gist.github.com/9aca21d61fdb5eecf0a6

And here for RabbitMQ's server code base (distinct tags: 'DOWN', nodeup, new_pg2_fixed, 'EXIT', bump_credit, mnesia_system_event, delayed_restart, and inet_async):

https://gist.github.com/fedcf22f077d592df7a7

It is a rather simplistic view on handle_info usage, but gives an idea of how and how much it is used.

From the above sample handle_info messages, which ones are really dynamics ones for which the implementors had no a priori knowledge? And which ones are just part of a ""secondary"" protocol, but that could be as typed as the primary protocol messages?

Or is it that handle_info messages are part of a ""common"" protocol that is common across a set of disparate processes and should be defined once and reused many times? Even so I could see these implemented either as process or channel messages.

This bring me back to the point I wade earlier that a server is a collection of services and these services could be either calls or casts/info and be newly defined by the designer of the server and/or reused from other server/modules.

E.g. to illustrate the point:
```
Server: Counter
    service1: call(count :: () -> Int)
    service2 : cast(reset :: () -> ())                -- this is a new service for counter
    imported service2: cast(NodeDown -> ())  -- this is the piece that could be defined in a node module
```
Behind the scenes it seems that they could well be implemented with expect/receiveWait or receiveChan, but I am not sure if both are supported in CloudHaskell.","Comment:rodlogic:11/09/12 10:06:52 PM:

@edsko Could you chip in to give us an idea of Cloud Haskell's design wrt to channels vs process messages here? 

First, can one reliably use both at the same time to receive messages? I see in the API that it is possible to merge receive ports for channels and that it is possible to receiveWait on many types of messages, but could both receiveChan and receiveWait be used at the same time by a single process? Or the design assumes a process implementation has to choose one over the other?

A second, question I have is wrt to mutiple channels in a single process. When creating a process with spawnLocal the returned ProcessId can be used to send any message type, so no complications here. The spawnChannelLocal(), however, returns a single SendPort. Does the API assume that this SendPort should be used to 'discover' or retrieve additional ports from the Process? I am curious if this was motivated by a explicit design decision that could invalidate the whole idea of using multiple typed channels for a single process.
","Comment:edsko:11/09/12 10:06:52 PM:

mentioned","Comment:hyperthunk:11/10/12 03:26:46 AM:

> And here for RabbitMQ's server code base (distinct tags: 'DOWN', nodeup, new_pg2_fixed, 'EXIT', bump_credit, mnesia_system_event, delayed_restart, and inet_async):

In RabbitMQ, like most erlang code bases we use 'EXIT"" and 'DOWN' extensively. Cloud Haskell doesn't appear to support 'EXIT' signals in the same way erlang does, but I don't want to dwell on that here as it's a big(ish) conversation and I'd like to park it until we're looking at supervision trees in earnest. The `{'DOWN', MonitorRef, process, Pid, ExitReason}` tuple is what monitors deliver. In Cloud Haskell monitor signals can **only** be consumed by expect/receive* afaict, not via channels.

A gen_event_EXIT signal is given when a supervised gen_event handler crashes, and is used to provide restartable event handlers (see https://github.com/hyperthunk/nodewatch/blob/master/dxkit/src/dxkit_event_handler_bridge.erl for a very simple example of how this works in practice to force a restart via the parent supervisor). The `nodeup | nodedown` messages are what *node monitors* deliver.

The only other thing that isn't Rabbit specific there is inet_async. In erlang, sockets are implemented as [linked in drivers](http://www.erlang.org/doc/tutorial/c_portdriver.html) and data from sockets is delivered to the socket's controlling process as messages. In RabbitMQ the socket writer process needs to write a lot of data out quickly and the `gen_tcp:send/2` API call writes to the port driver and then blocks on a selective receive waiting for inet_async to indicate completion, which is **bad** in our case because the writer process may have a very large message queue that subsequently has to be scanned looking for the inet_async response.

> From the above sample handle_info messages, which ones are really dynamics ones for which the implementors had no a priori knowledge? And which ones are just part of a ""secondary"" protocol, but that could be as typed as the primary protocol messages?

So the answer is there's a mix. 

> Or is it that handle_info messages are part of a ""common"" protocol that is common across a set of disparate processes and should be defined once and reused many times? Even so I could see these implemented either as process or channel messages.

I think you're missing the point of handle_info. It *can* be used to deal with secondary protocols and often is, but its primary purpose is to allow the gen server to deal with unexpected traffic. If you don't do *something* with messages sent to the process then they just fill up your mailbox (which has performance implications) and because the gen server abstraction is meant to *manage* the mailbox on behalf of the server - which deals with the functional aspects only - we can't just ignore the fact that unsolicited mail could arrive.

> First, can one reliably use both at the same time to receive messages? I see in the API that it is possible to merge receive ports for channels and that it is possible to receiveWait on many types of messages, but could both receiveChan and receiveWait be used at the same time by a single process? Or the design assumes a process implementation has to choose one over the other?

That's an interesting question; I'm guessing you've got to choose one or the other.
","Comment:hyperthunk:11/10/12 03:34:22 AM:

> In RabbitMQ, like most erlang code bases we use 'EXIT"" and 'DOWN' extensively. Cloud Haskell doesn't appear to support 'EXIT' signals in the same way erlang does

I'd better quantify that briefly though, so @edsko doesn't think I've gone nuts. What I mean is that Cloud Haskell seems to have a different take on trapping exits (and I'm not sure about sending 'EXIT' signals to other processes without killing yourself either) which makes the design of various things surprisingly different. But as I said above, we can discuss that later.","Comment:edsko:11/10/12 03:34:22 AM:

mentioned","Comment:hyperthunk:11/10/12 03:42:04 AM:

Oh and @rodlogic 

> This bring me back to the point I wade earlier that a server is a collection of services and these services could be either calls or casts/info and be newly defined by the designer of the server and/or reused from other server/modules.
> 
> E.g. to illustrate the point:
> 
> 
> Server: Counter
>     service1: call(count :: () -> Int)
>     service2 : cast(reset :: () -> ())                -- this is a new service for counter
>     imported service2: cast(NodeDown -> ())  -- this is the piece that could be defined in a node module

I'm certain this will all look very pretty once we're done and require minimal wiring on the part of server authors. Next steps are to make some decisions about the segregation (or not) of channels and regular messages, make the code testable and reliable, then performance. Finally, usability comes into play, hopefully without sacrificing any of the others. If we can make design decisions now that aid all of those concerns at once, then I'm all for that.","Comment:rodlogic:11/10/12 03:42:04 AM:

mentioned","Comment:rodlogic:11/10/12 04:47:56 AM:

Ok, considering that there are use cases for untyped handle_info messages AND the fact that it is not possible to handle both channel and process messages at the same time (this based on a scan of Cloud Haskell's API), we should probably put the idea of typed channels aside for now and focus on process messages so we can move on. 

If there is a need in the future to leverage typed channels and assuming they can co-exist with process messages, we can always re-evaluate. Hopefully, the public GenServer API should hide most of that from it's users anyway. 

What do you think? We revert some of the recent changes to process messaging and go through a few iterations on the base GenServer API + some tests.","Comment:edsko:11/18/12 10:24:50 AM:

> First, can one reliably use both at the same time to receive messages? I see in the API that it is possible to merge receive ports for channels and that it is possible to receiveWait on many types of messages, but could both receiveChan and receiveWait be used at the same time by a single process? Or the design assumes a process implementation has to choose one over the other?

The interplay between typed channels and process messages was not considered in the original paper. I have [proposed one way to deal with this](https://github.com/haskell-distributed/distributed-process/issues/62) by introducing a new primitive

```Haskell
expectChan :: Serializable a => Process (ReceivePort a)
```

which would make it possible to receive messages of a certain type as a receive port, which can then be merged with other receive ports. The implementation of `expectChan` it not entirely trivial though and some of the spec needs to be fleshed out (for instance, presumably it should still be possible to receive those messages using a normal `expect` as well). 

So as things stand if you want to wait for *either* a `MonitorNotification` *or* some other message then that other message cannot arrive on a typed channel. So you can either try to implement `expectChan` or not use channels for now.","Comment:edsko:11/18/12 10:26:41 AM:

> A second, question I have is wrt to mutiple channels in a single process. When creating a process with spawnLocal the returned ProcessId can be used to send any message type, so no complications here. The spawnChannelLocal(), however, returns a single SendPort. Does the API assume that this SendPort should be used to 'discover' or retrieve additional ports from the Process? I am curious if this was motivated by a explicit design decision that could invalidate the whole idea of using multiple typed channels for a single process.

No, it just captures a common pattern: we want to start a server and send it requests on its `SendPort`. 

The primary reason, in fact, is that the implementation is not trivial: the channel must be created *on the remote process* (because the `ReceivePort` is cannot be shipped from one node to another). ","Comment:edsko:11/18/12 10:29:29 AM:

> I'd better quantify that briefly though, so @edsko doesn't think I've gone nuts. What I mean is that Cloud Haskell seems to have a different take on trapping exits (and I'm not sure about sending 'EXIT' signals to other processes without killing yourself either) which makes the design of various things surprisingly different. But as I said above, we can discuss that later.

This is also in large part due to the fact that we use the ""Unified Semantics for Future Erlang"" semantics, rather than Erlang's semantics, which makes some crucial changes to some of these things. In particular, many more primitives are asynchronous, and linking is unidirectional. 

We have talking a lot about this between Duncan Coutts, Simon Peyton Jones, Francesco Cesarini (from Erlang solutions) and my (personal) take on all this, given lots of examples, is that trapping exceptions is fraught with difficulties: if you find yourself in a position where you want to trap an exit signal (or, in Haskell parlor, catch an exception) you should consider using monitoring instead.","Comment:edsko:11/18/12 10:29:29 AM:

mentioned","Comment:edsko:11/18/12 10:31:37 AM:

> I think you're missing the point of handle_info. It can be used to deal with secondary protocols and often is, but its primary purpose is to allow the gen server to deal with unexpected traffic. If you don't do something with messages sent to the process then they just fill up your mailbox (which has performance implications) and because the gen server abstraction is meant to manage the mailbox on behalf of the server - which deals with the functional aspects only - we can't just ignore the fact that unsolicited mail could arrive.

Note that Cloud Haskell does provide `matchUnknown` which can be used to throw away (but not process) messages of unknown type, which you could use for that purpose. If that is not good enough, you might want to use `matchAny`, but then we are back to having to extend the `AbstractMessage` API (see https://github.com/haskell-distributed/distributed-process/issues/30 and https://github.com/hyperthunk/distributed-process-platform/issues/4).","Comment:hyperthunk:11/18/12 12:19:43 PM:

@edsko thanks for the clarifications, they're very helpful. I might have a crack at extending `AbstractMessage` at some point.","Comment:edsko:11/18/12 12:19:43 PM:

mentioned","Comment:rodlogic:12/13/12 01:59:02 PM:

fyi:See this pull request by Simon Marlow: 

https://github.com/haskell-distributed/distributed-process/commit/847abf494233523dba7d0b40628c3af9e870be91

It seems to address the issue of efficiently receiving channel and process messages.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8,Control.Distributed.Platform.Async,Updated title to reflect name change,11/09/12 05:22:51 PM,01/12/13 06:23:12 PM,Task,Initial-Public-Release,Closed,Fixed,hyperthunk,API,"Comment:hyperthunk:12/05/12 04:15:43 PM:

Some of the features [here](https://github.com/hyperthunk/distributed-process-platform/compare/timer) are not dissimilar, which might be worth looking to for inspiration.","Comment:edsko:12/05/12 04:29:12 PM:

Also have a look at the Haskell [async](http://hackage.haskell.org/package/async) library (the [talk about the design of this library](http://skillsmatter.com/podcast/home/high-performance-concurrency) is well worth watching).","Comment:rodlogic:12/10/12 05:26:52 AM:

I have added preliminary support for async calls: 

https://github.com/rodlogic/distributed-process-platform/commit/089681dfa76e5d482c0bf000dc2347e95c0ad9d8

In a nutshell:
```haskell
-- | Async data type
data Async a = Async MonitorRef (MVar a)

-- | Sync call to a server
call :: (Serializable rq, Show rq, Serializable rs, Show rs) => ServerId -> Timeout -> rq -> Process rs
call sid timeout rq = do
  a1 <- callAsync sid rq
  waitTimeout a1 timeout

-- | Async call to a server
callAsync :: (Serializable rq, Show rq, Serializable rs, Show rs) => ServerId -> rq -> Process (Async rs)
callAsync sid rq = do
    cid <- getSelfPid
    ref <- monitor sid
    --say $ ""Calling server "" ++ show cid ++ "" - "" ++ show rq
    send sid (CallMessage cid rq)
    respMVar <- liftIO newEmptyMVar
    return $ Async ref respMVar

-- | Wait for the call response
wait :: (Serializable a, Show a) => Async a -> Process a
wait a = waitTimeout a Infinity

-- | Wait for the call response given a timeout
waitTimeout :: (Serializable a, Show a) => Async a -> Timeout -> Process a
waitTimeout (Async ref respMVar) timeout =
  let
    receive to = case to of
        Infinity -> do
          resp <- receiveWait matches
          return $ Just resp
        Timeout t -> receiveTimeout (intervalToMs t) matches
    matches = [
      match return,
      match (\(ProcessMonitorNotification _ _ reason) -> do
        mayResp <- receiveTimeout 0 [match return]
        case mayResp of
          Just resp -> return resp
          Nothing -> error $ ""Server died: "" ++ show reason)]
  in do
    respM <- liftIO $ tryTakeMVar respMVar
    case respM of
      Just resp -> return resp
      Nothing -> do
        respM <- finally (receive timeout) (unmonitor ref)
        case respM of
          Just resp -> do
            liftIO $ putMVar respMVar resp
            return resp
          Nothing -> error ""Response-receive timeout""
``` 

There is one pending issue with Async and receiveWait/receiveTimeout: the ```waitTimeout``` in the ```call``` works but not when calling ```callAsync``` and then ```waitTimeout``` separately: the matching is not working.
","Comment:hyperthunk:12/10/12 09:20:44 AM:

This is totally cool, I love it. I'll be keen to use this internally (for stuff like gen server) when you've ironed out the kinks. Brilliant.","Comment:hyperthunk:12/10/12 10:28:34 AM:

@rodlogic - update on this: I would like to get the async functionality generalised and moved into `Control.Distributed.Platform.Async` so it can be used independently from `GenServer` (and therefore *by* the `GenServer` implementation).

Does that seem reasonable to you?","Comment:rodlogic:12/10/12 10:28:34 AM:

mentioned","Comment:rodlogic:12/10/12 11:08:19 AM:

Definitely! I had the same thought regarding Async.hs, but decided to get *an* implementation working first. This has been a bit of my approach here in generalI: i.e. am taking small incremental steps and then refactoring. 

I am also fully aware that the implementations I am cranking out could be way shorter if I were to reuse more of the existing Haskell functions to avoid, for instance, nested case expressions etc.
","Comment:rodlogic:12/10/12 12:48:25 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/
","Comment:rodlogic:12/11/12 02:33:02 PM:

fyi: the current Async implementation is not yet complete besides the wait/waitTimeout issue mentioned above. Basically, it is not correlating a response with the corresponding request. Instead of implementing this in the Async module it seems reasonable to leverage CH channels to do the correlation work for us.
","Comment:hyperthunk:12/11/12 03:25:21 PM:

I have done a bug of experimental refactoring that I'd like to discuss, as there are a couple of issues in the current implementation (completeness notwithstanding) so I'll push to my personal fork later on so we can talk about them. I still haven't got my access rights back since the repository was transferred to the haskell-distributed organisation.","Comment:rodlogic:12/11/12 04:04:15 PM:

@hyperthunk Sure. I know you are pushing for an alpha release, but it seems that we may need quite a few iterations before that specially with respect to the overall code structure, organization and reuse.

Regarding the use of channels for async responses, there is one caveat: monitoring notifications come in the process inbox and the fact that we can't receive on both (unless we resort to spawning threads and the complexity that arises with it) is a bit problematic reducing the usefulness of channels in situations like this.

This seems to be a broader question of how to use channels together with process messages and monitoring notifications and a question better answered by CH.","Comment:hyperthunk:12/11/12 04:04:15 PM:

mentioned","Comment:hyperthunk:12/11/12 04:58:06 PM:

@rodlogic 

> I know you are pushing for an alpha release, but it seems that we may need quite a few iterations before that specially with respect to the overall code structure, organization and reuse.

Yes you're right about that. Probably we'll see another few hundred commits before 0.1.0.

> This seems to be a broader question of how to use channels together with process messages and monitoring notifications and a question better answered by CH.

I've had some thoughts about this and will post them shortly along with a link to the experimenting I've been doing.","Comment:rodlogic:12/11/12 04:58:06 PM:

mentioned","Comment:hyperthunk:12/11/12 08:53:47 PM:

Ok. I've pushed a branch with some refactoring ideas. I'll go through and explain them in this thread when I've got a bit more time. There's currently a silly clause in the `waitTimeout` implementation which will overflow the stack in a hurry - that's quite easy to fix.

The change btw were not stylistic in nature and were motivated by specific things viz API consistency with the *other* async package but more importantly the need to prevent stray (unexpected) messages being sent to unsuspecting code. Anyway like I said I'll write up what was going through my mind and we can go through it and see which bits actually make sense. ","Comment:hyperthunk:12/11/12 08:59:25 PM:

Oh and I should probably point out that I deliberately reverted the gen server changes that were using async. That was just a temporary hack to avoid dealing with lots of compilation breakage. In reality this branch is a total experiment (and tool for discussion) so even if we do use some of its code, we will patch that back into a feature branch off development anyway.","Comment:rodlogic:12/11/12 09:40:34 PM:

@hyperthunk This is an interesting alternative.

First, you are spawning a child process to receive the call's response and monitoring notifications (a good reminder to me that threads are cheap in Haskell/Erlang and can be used in interesting ways) and solving the correlation problem this way.

Second, you created a richer protocol between the client and the async process with ```AsyncResult a```. So that the the client can react properly based on what happened.

Finally, I see how the async child process is also responsible for the applying the ```SpawnAsync``` function instead of applying that in the client process (I guess my initial implementation was trying to solve the hole async problem using the same client process inbox).

I didn't understand, though, why there is the notion of a wpid (worker process id?) in addition to the gpid (gathered process id) and also why the ```SpawnAsync``` function is returning the wpid. Does the design assumes that ```SpawnAsync``` will spawn another CH process (e.g. ```cancel``` is cancelling wpid)?



","Comment:hyperthunk:12/11/12 09:40:34 PM:

mentioned","Comment:hyperthunk:12/11/12 10:48:08 PM:

> First, you are spawning a child process to receive the call's response and monitoring notifications (a good reminder to me that threads are cheap in Haskell/Erlang and can be used in interesting ways) and solving the correlation problem this way.

Yes. This approach is idiomatic Erlang. :)

> Second, you created a richer protocol between the client and the async process with AsyncResult a. So that the the client can react properly based on what happened.

That was really inspired by the .NET async APIs, and Java's Future/Promise to some extent.

> I didn't understand, though, why there is the notion of a wpid (worker process id?) in addition to the gpid (gathered process id) and also why the SpawnAsync function is returning the wpid. Does the design assumes that SpawnAsync will spawn another CH process (e.g. cancel is cancelling wpid)?

Yes that's right, the `SpawnAsync` function actually goes off an creates a new worker process. I'll change the variable name as wpid is obviously a bit too pithy to be transparent to the reader.

The advantage of letting the caller provide the spawn function that creates the worker is that you can spawn a worker on any node you like, but the gatherer (which uses the `MVar` to stash the final result) is localised. The other reason for this is that if we just ran the process then it wouldn't necessarily execute asynchronously, as code encapsulated in the `Process` monad, when run in the same *context* doesn't necessarily have to do anything asynchronous. The fact that our first use case was `async $ send pid msg` made it seem thus, because sending is asynchronous by nature in CH but that could've been some blocking operation. For example, in the original implementation there wasn't anything to prevent a call such as this:

```haskell
getValue mvar = async $ do
    val <- liftIO $ takeMVar mvar
    return val    
```

The call to `async` makes the reader *think* that the `takeMVar` call will execute on another thread, but in practise it would have just been evaluated as an IO action (or in our case, an action in the `Process` monad). So ostensibly, I think that forcing the asynchronous action to take place in another thread by making the user supply it as a spawn function makes sense.

> also why the SpawnAsync function is returning the wpid. Does the design assumes that SpawnAsync will spawn another CH process (e.g. cancel is cancelling wpid)?

This is exactly right. My reasoning worked thus:

* we need both the worker and the listener/proxy pids so we can cancel 
* the listener needs the worker pid so it can monitor it on demand 
* the worker needs the listener's pid so that it knows where to reply to once it is finished
* because of the previous point, the `SpawnAsync` type needs to take the listeners pid as its first input
* we don't have the listeners pid until after it is spawned, so to avoid writing some complex startup protocol I just spawn the worker from inside the listener!
* the listener is spawned in the caller's thread, and uses the caller's pid to send back the worker's pid after it is spawned

I avoided using linking as that imposes a policy, which is better described by the user than the library API itself. 

I was thinking of adding to this... 

* some sugar in the API to reduce boilerplace
* additional utilities a la Simon Marlow's async package (STM support, deliberate races, etc)
* support for using typed channels instead of `MVar` to get the response (these support timeouts natively)

But before I do that, it is time to fix the stack overflow in `waitTimeout` and then write up some serious tests. I *do* think this API will be useful in gen server and other places. I might refactor the `Timer` module to use it as well, if doing so improves the code.","Comment:hyperthunk:12/11/12 10:57:48 PM:

On another note, after reading through Simon M's code in detail, I've noticed that he's in-lining in a few select places (that's less of a concern for us as most of our functions are likely too long to benefit) but he's also forcing strict evaluation in a few places too. I must admit that my knowledge of optimising code for the GHC runtime is somewhat limited, and I do worry that we will have a tough time making sure we're time and space efficient enough to be useful in a production environment. Hopefully at some point in the future, the other folks around the CH may have a bit of time to dip into that and point us in the right direction.","Comment:hyperthunk:12/12/12 10:29:22 AM:

@rodlogic - my latest commit makes a sane implementation of `waitTimeout` I think. If I put some test cases in place around this, how do you feel about moving this into a feature branch with a view to merging it?","Comment:rodlogic:12/12/12 10:29:22 AM:

mentioned","Comment:rodlogic:12/12/12 11:09:15 AM:

I like the changes: it generalizes it quite a bit. I was initially considering a single use-case when I extracted the first Async implementation: separating the asynchronous reply from GenServer's synchronous call. This new implementation makes it much more general. The only slight concern is what kind of overhead this adds when thinking about GenServer's call usage, but let that become a clear problem, if at all.

```haskell
getValue mvar = async $ do
    val <- liftIO $ takeMVar mvar
    return val
```
> The call to async makes the reader think that the takeMVar call will execute on another thread... 

Yes, I see how the initial async implementation was misleading in this regard. I was reading async above as 'get the reply/response asynchronously' only (very focused on GenServer's call semantics) and not 'execute this computation (locally or remotely) asynchrnously and get the reply/response'. 

Back to ```SpawnAsync```, although I see it as a very flexible mechanism, I am wondering if it is leaking a bit specially wrt wpid and gpid. Wouldnt is be tighter to have something like:

```haskell
async :: (Serializable a) => Process () -> Process (Async a)
async proc = ...
   wpid <- spawnProcess localNode proc
   ...
```
Where ```async``` is responsible for spawning (or not) a local or remote Process passed as an argument? This way wpid and gpid are completely internal concepts and don't leak to the outside. The client code's responsibility is just to provide the computation that is not tied to Async in any way. The only question then is to spawn or not to spawn, but that could be a Bool parameter or an extra variant of the async function.

> @rodlogic - my latest commit makes a sane implementation of waitTimeout I think. If I put some test cases in place around this, how do you feel about moving this into a feature branch with a view to merging it?

Sounds good! I can help integrating this into the GenServer, if you want.
","Comment:rodlogic:12/12/12 11:09:15 AM:

mentioned","Comment:rodlogic:12/12/12 12:52:10 PM:

@hyperthunk I think the improvements to Async are a good step in the right direction and we should move forward with it. I have, though, a few comments for consideration regarding how to better integrate 'client' operations within server handler's. First to make sure the thinking at least make sense and if so consider a possible solution, if one is needed.

Right now, a GenServer is a collection of handlers, or Behavior's as you nicely named in GenProcess, that are served by a single CH receive loop. When these Behavior's need to interact with other servers or processes, they act as a client. Before Async, that was done through a synchronous ```call```, which would block the current Behavior and, in fact, any other Behavior as the main receive loop isn't in the picture. The Async module improves this by allowing the current Behavior to perform asynchronous calls and choosing when to block waiting for the response(s), but this is done outside of the main receive loop. This gives the ability to perform calls concurrently instead of sequentially, or even to store the Async data structure somewhere so that the response could be received at a later point in time.

Based on this, here is one observation:
 * Should we have an Async API to wait/poll for a list of Async's. I am assuming you also have this in mind in subsequent Async iterations.

Now, the other observation is about the main receive loop vs the Async receive loop: 
 * Should the wait/poll receive be integrated with the main receive loop?
I think the answer could be a yes or a no depending on the Server's implementation. I.e. In some cases, the fact that Behavior 1 is blocked waiting for a response, means that a message for Behavior 2 should not execute. In other cases, Behavior 1 and Behavior 2 should be able to execute concurrently, which would/could increasing the concurrency. And in another case, Behavior 1 for Id1 is blocked, but Behavior 1 for Id2 should be able to execute concurrently as the main invariant here is to make sure messages are sequential for state identified by Id1 or Id2.
There are different ways we can go about this, but I am curious how this is handled in Erlang. The simplest would be to  map one ID to one Process so that concurrency across IDs is handled by CH (1 ID vs 1 Process). Another extreme would be to handle state within a single Process complicating the GenServers implementation but possibly simplifying other parts of the system (M IDs x 1 Process). There could also be a middle ground where a range of IDs are mapped to a single Process (M IDS x N Processes). How is the notion of ID and state handled in Erlang?




","Comment:hyperthunk:12/12/12 12:52:10 PM:

mentioned","Comment:hyperthunk:12/12/12 04:00:42 PM:

Hi @rodlogic - thanks for taking a look at this.

> Back to SpawnAsync, although I see it as a very flexible mechanism, I am wondering if it is leaking a bit specially wrt > wpid and gpid.

Well, the `Async` constructor isn't exported but yes, you're probably right. Hmn.

> Wouldnt is be tighter to have something like:
> > ```haskell
> > async :: (Serializable a) => Process () -> Process (Async a)`
> > ```

Yes I can see the argument for doing that, but...

> The only question then is to spawn or not to spawn, but that could be a Bool parameter or an extra variant of the async function.

That doesn't work, because if you don't spawn then the computation isn't *asynchronous* at all. However...

> This way wpid and gpid are completely internal concepts and don't leak to the outside. The client code's responsibility is just to provide the computation that is not tied to Async in any way. 

That is a very good point and would make for a *much* nicer API design. I'll add that shortly.

> Should we have an Async API to wait/poll for a list of Async's. I am assuming you also have this in mind in subsequent Async iterations.

Absolutely. I'm going to add that in once I've managed to get the tests passing. <cough> :)

Now, with regards the gen server design, I think we should probably move the discussion over to issue DPP-4, but I'll cover the basics here to get us started.

> How is the notion of ID and state handled in Erlang?

I'm going to stick to this question for now. Erlang's gen server is implemented as a single process (the server) that receives messages serially and defers to a user defined behaviour/module to process them. The result of that processing can do one of several things, possibly at the same time

* alter the state of the process/server
* cause the process/server to interact with another process (e.g., by message passing)
* cause the process/server to exit

The call/cast abstraction is just a bit of protocol layering on top of that. Because the gen server is implemented as a single process, it can only take messages from its mailbox one at a time. When the server/process loop calls the user defined behaviours, it does so sequentially. This leads to two important characteristics:

1. a gen server can **only** service one client at a time
2. a gen server can **only** be executing one behaviour (callback function) at a time

These characteristics are something that Erlang programmers rely heavily upon whilst using the gen server, and they prove quite useful when reasoning about one's code.

Now the `gen_server:cast/2` API simply allows the client to avoid dealing with a reply from the server. The server/process can nonetheless end up blocking for an arbitrary period of time whilst *handling* the cast request, even though the client doesn't know (or care) about this.

So how do I *think* this should work for us in Haskell? Here's my opinion, in two parts:

___ 1. If in doubt, do what Erlang does ;)

I think that our gen server should work very much the same way as the Erlang/OTP gen server. This is a battle tested, weather worn design that has stood the test of time and is **easy** to program.

The role of the `Async` module in our gen server design, is probably *just* to provide a simple way to implement the `call :: (Serializable a) => ServerId -> Process a` API without having to re-implement the whole *block until I see a result and maybe timeout if I say so* protocol.

I don't think that our async API has much of a role to play in the server process' implementation, although obviously the person writing a gen server can *easily* use this API to implement non-blocking themselves. Here's a typical example provided in both Erlang and Haskell - I think our CH version is much prettier personally.

```erlang

%% the client API - this *looks* to the caller as though we're
%% making a blocking call, even though it is non-blocking in the server

run_command(Server, Cmd) ->
    ok = gen_server:cast(Server, {execute, Cmd}),
    receive
        {done, Result} ->
            Result;
        {failed, Reason} ->
            {error, Reason}
    end.
 
%% and somewhere in the gen server callback module

handle_call({execute, Cmd}, From, State) ->
    Self = self(),
    {MRef, Pid} = spawn_monitor(
        fun() -> Result = command_executor:execute(Cmd), Self ! {finished, MRef, Result} end),
    %% do *not* block the server loop, nor reply to the client
    {noreply, add_client(MRef, From, State)};
%% .... snip

handle_info(Result={finished, _, _}, State) ->
    %% a task has finished
    reply(Result, State),
    {noreply, remove_client(MRef, State)};
handle_info({'DOWN', MRef, process, _, normal}, State) ->
    case lookup_client(MRef, State) of
        undefined ->
            %% we've already replied - just drop this message
            ok;
        Client ->
            %% oh dear, we've not seen a reply for this job!
            receive
                {finished, _, _}=R -> reply(R, State);
            after 0 ->
                gen_server:reply(Client, {failed, unknown})
            end
    end,
    {noreply, remove_client(MRef, State)};
handle_info({'DOWN', MRef, process, _, Reason}, State) ->
    Client = lookup_client(MRef, State),
    gen_server:reply(Client, {failed, Reason}),
    {noreply, remove_client(MRef, State)}.

reply({finished, MRef, Result}, State) ->
    Client = lookup_client(MRef, State),
    gen_server:reply(Client, {done, Result}).

```  

Versus a CH version....

```haskell

type TaskID
data TaskFailed = Reason String | Cancelled

submitTask :: ServerId -> Task -> Process TaskID
submitTask server task = call server $ Submit task -- returns jobId

waitForTask (Serializable a) => ServerId -> TaskID -> Process (Either a TaskFailed)
waitForTask server taskId = call server $ WaitFor taskId

-- and on the server side, as it were

handleTaskSubmitted task = getState >>= startWorker task >>= reply
  where
    stateWorker task state = do
        hAsync <- async $ task 
        (taskId, state') <- addWorker state hAsync
        putState state'
        return taskId

handleWaitForJob jobId =
    s <- getState
    hAsync <- lookupJob s jobId
    r <- wait hAsync
    case r of
        AsyncFailed r -> reply TaskFailed (show r)
        AsyncCancelled -> reply Cancelled
        AsyncDone  a -> reply a
```

___ 2. On the other hand, don't limit ourselves to what Erlang/OTP provides

Just because our gen server follows the pattern of OTP, doesn't mean the points you've raised aren't valid. Associating tasks with simultaneous clients is an important issue and the asynchronous bed-fellow of the reactor pattern is probably a very important abstraction.

There are other important abstractions that we *should* implement in the platform layer, and I will create tickets for each of these. One classic abstraction, for example is that of composable asynchronous tasks or higher order channels. Keep your eyes peeled on the issue tracker and by all means add your own concepts as tasks that we can plan and discuss!


","Comment:rodlogic:12/12/12 04:00:42 PM:

mentioned","Comment:hyperthunk:12/12/12 04:20:54 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/
","Comment:hyperthunk:12/12/12 04:27:03 PM:

Another possibility is that `Async` might be better implemented with typed channels instead of using an `MVar` to deal with blocking on the result.

This implementation would basically set up a channel and put the `SendPort` used for replying into the gatherer. The **big** advantage of this approach is that it would greatly simplify the implementation of `wait` and `waitTimeout` as we could simply defer to `receiveChanTimeout` and `receiveChan` instead of looping ourselves. We would not want to dispense with the gatherer process though, as this would remove our monitoring support which can be used to detect failures in async tasks.","Comment:hyperthunk:12/12/12 10:46:16 PM:

> Another possibility is that Async might be better implemented with typed channels instead of using an MVar to deal with blocking on the result.

I have pushed just that change set. It **is** much shorter and cleaner code, however it's far from perfect. In particular it still needs some kind of API for spawning the task on a remote node.","Comment:rodlogic:12/13/12 02:13:10 PM:

> In particular it still needs some kind of API for spawning the task on a remote node.

Isn't that already supported by CH:
```haskell
spawn :: NodeId -> Closure (Process ()) -> Process ProcessId
```

The K-means seems again a good reference: http://www.well-typed.com/blog/74","Comment:rodlogic:12/13/12 02:21:43 PM:

> Another possibility is that Async might be better implemented with typed channels instead of using an MVar to deal with blocking on the result.

Make sense. I did experiment with this in the first Async iteration but the fact that we couldnt efficiently receive channel and process (for notifications) messages was a bit problematic. Maybe this is no longer the case now that we are spawning a child/worker process, but, for what it is worth, see this commit by Simon Marlow:

https://github.com/haskell-distributed/distributed-process/commit/847abf494233523dba7d0b40628c3af9e870be91

","Comment:rodlogic:12/13/12 02:34:17 PM:

@hyperthunk Thanks for the detailed explanation of how Erlang handles this. It is quite helpful and puts the discussion in firm grounds.

The way I am 'reading' the Async module is that it is basically the ```async``` package by Simon Marlow but built on top of CH and so fully distributed. Maybe it could even be part of CH directly.","Comment:hyperthunk:12/13/12 02:34:17 PM:

mentioned","Comment:hyperthunk:12/13/12 03:31:30 PM:

Hi @rodlogic - if you look at the latest changes to that branch you'll see that I've got asyn working using typed channels now. Locally I have passing test cases for all the API calls and will
Push them once I'm out from behind the firewall. :)","Comment:rodlogic:12/13/12 03:31:30 PM:

mentioned","Comment:hyperthunk:01/11/13 02:14:33 PM:

This is coming along nicely now. There are two variants: channel based and STM based, where the latter is useful if you need to 'wait' or 'poll' on the asyncHandle from outside the process which created it. Neither module supports sending the async handle to another process - that kind of thing will be supported by the 'Task' API though.","Comment:hyperthunk:01/12/13 06:23:12 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9,Refactor tests to get some code sharing with distributed-process,"This will involve collaboration with the CH maintainers, but is worthwhile IMO. Updating title to reflect change of focus.",11/09/12 05:24:16 PM,01/04/13 03:53:03 PM,Task,Post-0.1.0,Open,Unresolved,hyperthunk,enhancement tests,"Comment:hyperthunk:11/09/12 05:27:13 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/
","Comment:rodlogic:12/03/12 08:36:43 PM:

After spending a bit of time looking at distributed-process tests, I am wondering if we can abstract some of the test case boiler plate for testing local/remove and assertions within/without platform server processes.","Comment:hyperthunk:12/04/12 06:02:11 PM:

@rodlogic I think that's a good idea.","Comment:rodlogic:12/04/12 06:02:11 PM:

mentioned","Comment:hyperthunk:12/06/12 02:06:30 AM:

also see issue DPP-14 and https://github.com/hyperthunk/distributed-process-platform/blob/timer/tests/TestTimer.hs",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10,Fix Travis-CI Install Procedure,,11/09/12 05:26:51 PM,01/12/13 04:06:52 PM,Task,Initial-Public-Release,Open,Unresolved,hyperthunk,build tests,"Comment:hyperthunk:11/09/12 05:26:51 PM:

assigned","Comment:hyperthunk:11/09/12 05:27:12 PM:

Depends on issue DPP-9","Comment:hyperthunk:12/09/12 11:39:01 AM:

We **are** on travis-ci now, but the build repeatedly fails to install dependencies.","Comment:rodlogic:12/10/12 12:55:45 AM:

The latest build failure is happening because the latest exit/catchExit changes have not been released by distributed-process, or so it seems.
 
```
src/Control/Distributed/Platform/GenServer.hs:52:60:
573    Module `Control.Distributed.Process' does not export `exit'
```","Comment:hyperthunk:12/10/12 10:35:40 AM:

> The latest build failure is happening because the latest exit/catchExit changes have not been released by distributed-process, or so it seems.

Hmn, you're quite right. I think I'll hack the makefile to handle that for the time being. Would be good to hear from @edsko when the next release is likely to be first though. Until it becomes available on hackage, I'll probably `git clone` the distributed-process repo into a temp folder and `cabal install` it from there as a pre-build hook. It'll be **much better** not to do that in future though....","Comment:edsko:12/10/12 10:35:40 AM:

mentioned","Comment:edsko:12/10/12 10:37:42 AM:

Whether or not we release a new version of CH any time soon should be irrelevant to this discussion. It's not unlikely that distributed-process-platform will continue to depend on features that are (temporarily) available in HEAD only. ","Comment:hyperthunk:12/10/12 10:57:53 AM:

> Whether or not we release a new version of CH any time soon should be irrelevant to this discussion. It's not unlikely > that distributed-process-platform will continue to depend on features that are (temporarily) available in HEAD only. 

Good point. I'll plonk some code into the makefile to handle this.","Comment:hyperthunk:01/12/13 04:06:52 PM:

Also.... It would be nice if travis-ci supported multiple versions of ghc and custom dependencies. We could use virthualenv to achieve this (for multiple ghc versions) or cabal-dev for custom dependencies (but not multiple ghc afaict). I might patch https://github.com/travis-ci/travis-build/blob/master/lib/travis/build/job/test/haskell.rb and send a pull request. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
11,GenServer: 2nd generation,"Entirely rewritten, it now:
* support multiple message types
* uses process messages instead of typed-channels
* simplifies a bit the exposed API
* Tries to statically enforce cast and call protocols (can we make sure that one 1 response is sent for a call? Or that the server either sends a reply or a forward?)",11/14/12 04:43:55 PM,11/14/12 10:58:35 PM,Task,,Closed,Fixed,rodlogic,,"Comment:hyperthunk:11/14/12 10:58:35 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/ec5dac69160ca23701a12ca793579c69c145f342
","Comment:hyperthunk:11/14/12 10:58:35 PM:

merged","Comment:hyperthunk:11/14/12 10:58:35 PM:

closed","Comment:ericbmerritt:01/09/13 02:32:36 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/5937daf8c74a360e78f6e32a2abb5a9328d2c822
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
12,Multiprocess Threadscope integration,"""Profiling of multi-process or distributed Haskell systems such as client/server or MPI programs"" and distributed-process-platform.

There are additional references here: 

http://www.haskell.org/haskellwiki/Parallel_GHC_Project

",11/14/12 09:49:38 PM,12/18/12 01:15:22 PM,Task,,Closed,Fixed,rodlogic,,"Comment:hyperthunk:11/18/12 12:17:49 PM:

+1 - we will get to this eventually and I agree it's a good idea.","Comment:hyperthunk:12/10/12 03:22:20 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/
","Comment:hyperthunk:12/18/12 01:15:17 PM:

I am moving this issue to core CH.","Comment:hyperthunk:12/18/12 01:15:22 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
13,Introduced a Server monad to wrap the server state,"The code compiles and runs. See the updated Counter.hs file for a simple example. 

Server handlers are now implemented in terms of a Server monad, which wraps the Process monad. The state can be manipulated using the helper functions: getState, putState, modifyState.

E.g., the following implements the Counter server:

```haskell
handleCounter IncrementCounter = do
  modifyState (+1)
  return $ CallOk (CounterIncremented)

handleCounter GetCount = do
  count <- getState
  return $ CallOk (Count count)

handleReset ResetCount = do
  putState 0
  return $ CastOk
```

The inferred signatures for the counter services would be something like:
```haskell
handleCounter :: CounterRequest -> Server Int (CallResult CounterResponse)
handleReset :: CountReset -> Server Int CastResult

-- based on GenServer's handlers:
type CallHandler s a b        = a -> Server s (CallResult b)
type CastHandler s a          = a -> Server s CastResult
```

Still lots of small todos such as the proper termination, handling of timeouts, and handling of all InitResult, CallResult, CastResult.

No attention has been paid so far to proper file/code formatting, documentation or even more idiomatic Haskell code.",11/22/12 03:50:42 AM,11/22/12 01:32:54 PM,Task,,Closed,Fixed,rodlogic,,"Comment:hyperthunk:11/22/12 01:32:54 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/6319db8072a478858a3867b50a5d97f7db6d82be
","Comment:hyperthunk:11/22/12 01:32:54 PM:

merged","Comment:hyperthunk:11/22/12 01:32:54 PM:

closed","Comment:ericbmerritt:01/09/13 02:32:36 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/ee2f9e42df715e34cdab869a2c02098455bb6251
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
14,"Preliminary test, coverage, benchmark support...","In addition, a few small changes to GenServer, Counter and converted the Kitty example to use GenServer",12/05/12 12:31:25 PM,12/06/12 02:05:48 AM,Task,,Closed,Fixed,rodlogic,,"Comment:rodlogic:12/05/12 12:40:46 PM:

@hyperthunk this is a partial set of commits. Let me clean the house a bit and I'll send another pull request.","Comment:hyperthunk:12/05/12 12:40:46 PM:

mentioned","Comment:rodlogic:12/05/12 12:40:46 PM:

closed","Comment:rodlogic:12/05/12 12:46:39 PM:

This compiles fine. Sorry for the noise.","Comment:rodlogic:12/05/12 12:46:39 PM:

reopened","Comment:rodlogic:12/05/12 12:57:56 PM:

@hyperthunk there are problems generating coverage reports, but I am assuming we'll work these out over the next iterations.","Comment:hyperthunk:12/05/12 12:57:56 PM:

mentioned","Comment:hyperthunk:12/05/12 01:32:56 PM:

@rodlogic that's fine - I'll merge this after work tonight.","Comment:rodlogic:12/05/12 01:32:56 PM:

mentioned","Comment:hyperthunk:12/05/12 03:48:56 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/7f98806300ae5c4898dfdd9f24acded5af2db6d0
","Comment:hyperthunk:12/05/12 03:48:56 PM:

merged","Comment:hyperthunk:12/05/12 03:48:56 PM:

closed","Comment:hyperthunk:12/06/12 02:05:48 AM:

Might be worth taking a look at https://github.com/hyperthunk/distributed-process-platform/blob/timer/tests/TestTimer.hs at some point too.","Comment:hyperthunk:12/06/12 02:06:30 AM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/
","Comment:ericbmerritt:01/09/13 02:32:36 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/f4da890bcaa2312f79df6bf02c6474a61a9710c8
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
15,implement timer module,"so we can

- sendAfter
- runAfter
- tick
- cancelTimer",12/05/12 03:44:32 PM,01/14/13 10:14:57 PM,Task,Initial-Public-Release,Closed,Fixed,hyperthunk,enhancement API,"Comment:hyperthunk:12/05/12 03:44:32 PM:

assigned","Comment:hyperthunk:12/05/12 03:46:10 PM:

see https://github.com/hyperthunk/distributed-process-platform/tree/timer for an initial sketch.","Comment:hyperthunk:12/07/12 11:31:49 PM:

This issue will remain open until the branch is fully merged.","Comment:hyperthunk:12/10/12 05:50:53 PM:

This issue depends on DPP-18 - bar the potential race in both cancelTimer and flushTimer were mostly done.","Comment:hyperthunk:01/14/13 10:14:57 PM:

Now issue DPP-18 is resolved and the `Timer` module was merged ages ago => resolved. ","Comment:hyperthunk:01/14/13 10:14:57 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
16,remove Main.hs,"This is a library, not an executable. I'd like to remove Main.hs and have done so in the [timer branch](https://github.com/hyperthunk/distributed-process-platform/compare/timer) as you can see.

Any objections to this? Once we've got the tests properly lined up, I don't see any need to Main.hs.",12/05/12 04:13:10 PM,12/13/12 10:30:39 AM,Task,,Closed,Fixed,hyperthunk,,"Comment:rodlogic:12/05/12 05:35:35 PM:

Not against but in full-favor of that. We should also get rid of the Naive module and possibly reconsider where Examples is located.","Comment:hyperthunk:12/09/12 11:36:20 AM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/e98c06505c13fc4619a30e9b4ad2f92e697fa8c6
","Comment:hyperthunk:12/13/12 10:30:12 AM:

closed","Comment:hyperthunk:12/13/12 10:30:39 AM:

This has been removed in master -> closing","Comment:ericbmerritt:01/09/13 02:32:36 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/5bb5460374c7227a716411d6402fdce163d9c8b9
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
17,testCancelTimer deadlocks!,"see [the test case](https://github.com/hyperthunk/distributed-process-platform/compare/timer-deadlock#L5R68) in the [timer-deadlock branch](https://github.com/hyperthunk/distributed-process-platform/tree/timer-deadlock) for details. We end up with

```
Timer Send:
  testSendAfter: [OK]
  testRunAfter: [OK]
  testCancelTimer: [Failed]
ERROR: thread blocked indefinitely in an MVar operation
  testPeriodicSend: [OK]
  testTimerReset: [OK]
  testTimerReset: [OK]

         Test Cases  Total      
 Passed  5           5          
 Failed  1           1          
 Total   6           6          
Test suite TestTimer: FAIL
```

But this does **not** happen in the [timer branch](https://github.com/hyperthunk/distributed-process-platform/tree/timer) and I'm struggling to see where the problem has been introduced. The two candidates I can see at the moment are

1. the fact that we've enabled some new GHC options in the cabal file or
2. having made `Tick` and instance of `Eq`

I'll be dumbfounded if it's down to (2) and I've applied the GHC options (mentioned in (1) above) to the timer branch and the test case still passes there. Hmn.",12/07/12 11:36:31 PM,12/10/12 04:21:39 PM,Task,,Closed,Fixed,hyperthunk,bug duplicate tests,"Comment:hyperthunk:12/10/12 04:21:38 PM:

Well this is nonsense, sort of. Closing as a duplicate of issue DPP-18.","Comment:hyperthunk:12/10/12 04:21:39 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
18,Test cases are racy!,"I've seen this fail intermittently several times now. Have also seen testSendAfter and testFlushTimer bomb out a couple of times. The *typical* symptom of this is that the tests never complete - an apparent deadlock.

In the timer tests, this is often due to the `ProcessMonitorNotification` in `testCancelTimer` not arriving. For `testTimerFlush` I'm less certain of what's happening as we're using timeouts for *all* of the cases here AFAICT.

I'm not really convinced that there isn't a race in Cloud Haskell's monitoring implementation at the moment, but without further evidence, I'm not going to point the finger either. Part of resolving this bug is getting the right profiling tools enabled so we can inspect what is happening when the tests do start hanging, so I'm opening another bug to look into that.",12/08/12 12:37:17 AM,01/14/13 10:14:12 PM,Task,Misc/Admin,Closed,Fixed,hyperthunk,bug tests,"Comment:hyperthunk:12/08/12 12:37:17 AM:

assigned","Comment:hyperthunk:12/10/12 03:22:20 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/
","Comment:hyperthunk:12/10/12 04:21:39 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/
","Comment:hyperthunk:12/10/12 05:50:53 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/
","Comment:hyperthunk:12/13/12 09:36:51 AM:

Well there's definitely a race in the code somewhere. This happens on 1 out of 4 test runs for me. Is anyone else seeing the tests hang?

I wonder if there's a subtle race in the timer module that I'm missing....","Comment:hyperthunk:12/23/12 11:15:45 AM:

There's also some raciness in the asyncChan tests for waitAnyTimeout. We end up blocked on an mvar operation (presumably the result mvar) and it's not obvious why.","Comment:hyperthunk:01/14/13 10:14:11 PM:

Argh, no no no no no. For the benefit of future readers....

It turns out there was **no races in the tests or the production code at all!** It turns out that I was mis-reading the various *timeout* APIs in distributed-process and passing what I thought were milliseconds, where microseconds was the expected unit of measure. Seemingly random/intermittent timing failures due to scheduling ensued. Fixing the code in the `Time` module has resolved this and I've not seen **any** test failures since, so => resolved. ","Comment:hyperthunk:01/14/13 10:14:12 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
19,Removed Naive module,,12/08/12 12:29:08 PM,12/08/12 12:41:45 PM,Task,,Closed,Fixed,rodlogic,,"Comment:hyperthunk:12/08/12 12:41:45 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/085619708818190b7b587b4d2b89408f003d3b79
","Comment:hyperthunk:12/08/12 12:41:45 PM:

merged","Comment:hyperthunk:12/08/12 12:41:45 PM:

closed","Comment:ericbmerritt:01/09/13 02:32:36 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/954eb2ed57a02adff496ed453a44ee5d94432341
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
20,Now calling terminateHandler on exceptions & implementation cleanup,"I have cleaned up the implementation and API  a bit by getting rid of the CallResult and CastResult: now there is a single Result and a single set of smart constructors: ok, stop, forward.

I have also added preliminary support for calling terminateHandler in the event of an exception. This is not using catchExit at this point as it deserves a second round of discussions. The most important thing for now is that calling ```exit pid reason``` will terminate the server and will call the terminateHandler. So, at least a small step forward.",12/09/12 05:09:33 PM,12/10/12 10:22:20 AM,Task,,Closed,Fixed,rodlogic,,"Comment:rodlogic:12/09/12 05:15:30 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/
","Comment:hyperthunk:12/09/12 09:26:26 PM:

In general I *like* where this change set is going. I am unable to merge it cleanly however, as I've recently done some tidying up of the `master` branch, noteably removing `Main.hs` and introducing some common internal types. Would you be willing to rebase your fork and resubmit once the pull request can be merged cleanly please?

Also, I'd like to suggest looking at the way you're handling *startServer/startSupervised/startMonitored* as these *overloads* are not only unnecessary but default to using `startLocal` which isn't always wanted or convenient. Instead of this, might I suggest that you follow the approach I used with [GenProcess](https://github.com/hyperthunk/distributed-process-platform/blob/gen-process/src/Control/Distributed/Platform/GenProcess.hs_L150) which is to make the caller pass in the function used to spawn the process (potentially on a different node) by taking a function of type `Process () -> Process ProcessId` which we simply *defer* to, i.e.:

```haskell
start :: s -> Behaviour s -> (Process () -> Process ProcessId) -> Process ProcessId
start state handlers spawnFun = spawnFun $ do
  _ <- ST.runStateT (runProc handlers) state
  return ()
```","Comment:rodlogic:12/09/12 10:39:36 PM:

I am rebasing right now. 

Btw, I saw that some of your commits removed extra empty lines between definitions. I am 100% fine with that btw just that before noticing that I was adding 2 more empty lines back in :-). I was basically following what I saw done in the async package, but fine to follow whatever code convention the project defines.
","Comment:hyperthunk:12/10/12 10:22:20 AM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/a58a7bdc5c7f788d83b5b87758793edaa113499d
","Comment:hyperthunk:12/10/12 10:22:20 AM:

merged","Comment:hyperthunk:12/10/12 10:22:20 AM:

closed","Comment:ericbmerritt:01/09/13 02:32:36 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/de59dfed1eba1dacc0f2a559bea9901ebe81907f
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
21,semantic versioning,"As per http://semver.org - I think this should be a given, although it probably isn't vitally important yet but once we get to a conceptual 0.1.0 it'll probably start to matter. Right now we're 0.0.1 IMO.",12/10/12 11:04:21 AM,12/13/12 12:56:15 AM,Task,Initial-Public-Release,Open,Unresolved,hyperthunk,API versioning in-progress,"Comment:hyperthunk:12/10/12 11:04:21 AM:

assigned","Comment:hyperthunk:12/10/12 11:24:15 AM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/
","Comment:qrilka:12/10/12 02:01:53 PM:

What about http://www.haskell.org/haskellwiki/Package_versioning_policy ?
It contradicts somewhat with semver but I presume it is a default policy for Haskell.","Comment:hyperthunk:12/10/12 02:29:35 PM:

@qrilka you're quite right, and it is almost certainly best to follow the recommendations you've linked to there. I think we can keep to that policy but stay pretty close to [semver](http://semver.org) however, by adding stricter constraints about how the single element of the minor release is handled.

For the major release version number, we can (and should IMO) treat the two parts as distinct with regards API changes where

* for x.y.z when we're creating a new major release, we increment y **if** we've made API changes as a result of a hotfix/bugfix or extended an existing feature
* for x.y.z when we're creating a new major release, we increment x **if** we've made API changes as a result of new features
* for x.y.z when changing z, we use some suffix scheme or other to differentiate between patch (bugfix) and build version increments - that one's a bit hazy for me ATM though  ","Comment:qrilka:12/10/12 02:29:35 PM:

mentioned",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
22,Development Process and Branching Model,"In terms of naming branches, I would like to keep `master` as one of the mainlines so I'd propose either:

Production:      `stable`
Development:  `master`

Or as an alternative, we could use the master branch for production and have something like this:

Production:      `master`
Development:  `development`

I *think* I prefer the latter but I'm open to having a conversation about it.

In either case, issues should be labelled with the appropriate tag so we can differentiate. Once we've written up the merge/rebase procedures for this, we should be able to work on things concurrently without stomping on one another's changes because

@rodlogic - if we can come up with a version of this scheme that works for us both and assign issues to ourselves, then as long as we're able to split up the work so we don't clash too often them I'm willing to set you up as a collaborator and grant you commit access to this project. If you'd prefer to continue working in a fork via pull requests, that's absolutely fine as well.

@edsko - would it make sense to move this project into the [haskell-distributed](https://github.com/haskell-distributed/) organisation? Github organisations make it easier to manage collaborators and configure access rights, and I'm comfortable opening it up to that wider community (which looks like mostly well-typed folks?) if you think that's appropriate.
",12/10/12 11:24:15 AM,01/04/13 11:14:14 PM,Task,Initial-Public-Release,Open,Unresolved,hyperthunk,API versioning in-progress,"Comment:rodlogic:12/10/12 11:24:15 AM:

mentioned","Comment:edsko:12/10/12 11:24:15 AM:

mentioned","Comment:hyperthunk:12/10/12 11:24:15 AM:

assigned","Comment:edsko:12/10/12 11:30:37 AM:

> @edsko - would it make sense to move this project into the haskell-distributed organisation? Github organisations make it easier to manage collaborators and configure access rights, and I'm comfortable opening it up to that wider community (which looks like mostly well-typed folks?) if you think that's appropriate.

Sure, if everybody agrees with that it's fine with me.","Comment:edsko:12/10/12 11:30:38 AM:

mentioned","Comment:hyperthunk:12/10/12 01:42:38 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/
","Comment:rodlogic:12/10/12 02:31:50 PM:

>  if we can come up with a version of this scheme that works for us both and assign issues to ourselves, then as long as we're able to split up the work so we don't clash too often them I'm willing to set you up as a collaborator and grant you commit access to this project. If you'd prefer to continue working in a fork via pull requests, that's absolutely fine as well.

That would be great, but it would be more sensible first to get me in line with the workflow we adopt here. Once clean pull requests are flowing from my end we can re-discuss.
","Comment:rodlogic:12/10/12 02:34:05 PM:

> Sure, if everybody agrees with that it's fine with me.

Works for me, if that makes sense for whoever owns the haskell-distributed organization.","Comment:edsko:12/10/12 03:23:17 PM:

> Works for me, if that makes sense for whoever owns the haskell-distributed organization.

That would be Well-Typed, and we're okay with it. Let me know.
","Comment:hyperthunk:12/10/12 04:20:44 PM:

> That would be Well-Typed, and we're okay with it. Let me know.

Ok yes please @edsko. I'll probably have to migrate the issue tracker data by hand (urgh) but that's something I can slog through tonight after work.

> That would be great, but it would be more sensible first to get me in line with the workflow we adopt here. Once clean > pull requests are flowing from my end we can re-discuss.

Ok cool, I'm going to write up a contributors guide this evening. I will include information about the branch/merge model we're going to adopt in that.","Comment:edsko:12/10/12 04:20:44 PM:

mentioned","Comment:edsko:12/10/12 04:30:01 PM:

I *think* that if you give me admin rights to this repository, I can move the entire repository to the haskell-distributed organization (including issues), rather than forking it.","Comment:hyperthunk:12/10/12 04:35:58 PM:

I'm not sure that's really true, but I have added you as a collaborator nonetheless. Let me know if it works! :)","Comment:edsko:12/10/12 04:37:38 PM:

You have given me push rights, but not admin rights. If you go to this repository, if you click on admin, in the danger zone there is a ""move this repository to another user or organization"". But you can't do that because you're not an admin on `haskell-distributed`, and I don't even get to see that admin tab on `haskell-distributed-platform` because I'm not an admin there.","Comment:edsko:12/10/12 04:45:19 PM:

Let me know if you prefer I just fork.","Comment:hyperthunk:12/10/12 05:46:29 PM:

I've moved the repo to my own org and given you admin rights so maybe try again?","Comment:edsko:12/10/12 05:52:20 PM:

Yup, I can do it now. Happy for me to make the transfer now?","Comment:hyperthunk:12/10/12 06:28:00 PM:

Yes let's go for it - this way the issues will be transferred across which works nicely for me.","Comment:rodlogic:12/10/12 06:37:21 PM:

I see the repo moved nicely and the fork automatically points to the new location. It seems that the only real change required in my local fork is to update the upstream url in my .git/config. Anything else?","Comment:hyperthunk:12/10/12 06:40:32 PM:

Nope, that's all you need to do.","Comment:hyperthunk:12/10/12 06:43:13 PM:

@edsko - I'm no longer a repo admin - is that something you can fix?

> In terms of using git-flow I'm going to switch to the following scheme:

Production: master
Development: development ","Comment:edsko:12/10/12 06:43:13 PM:

mentioned","Comment:hyperthunk:12/10/12 06:46:30 PM:

Erm @edsko I don't seem to have commit rights any more either. Eek! :/","Comment:edsko:12/10/12 06:46:30 PM:

mentioned","Comment:hyperthunk:12/10/12 07:13:49 PM:

Ok so @rodlogic 

> Ok cool, I'm going to write up a contributors guide this evening. I will include information about the branch/merge model we're going to adopt in that.

I have mostly written this and will add it to the wiki (and put some notes into the README) as soon as @edsko restores my admin/commit rights.","Comment:rodlogic:12/10/12 07:13:49 PM:

mentioned","Comment:edsko:12/10/12 07:13:49 PM:

mentioned","Comment:edsko:12/10/12 07:34:25 PM:

> Erm @edsko I don't seem to have commit rights any more either. Eek! :/

Mwahahahaahhaha :-D
","Comment:edsko:12/10/12 07:34:25 PM:

mentioned","Comment:edsko:12/10/12 07:39:32 PM:

I don't seem to have the rights to give you rights.. I will investigate and fix it :)","Comment:hyperthunk:12/10/12 07:55:34 PM:

Hehehe - ok fair enough.","Comment:hyperthunk:12/10/12 10:03:56 PM:

@rodlogic 

>> That would be great, but it would be more sensible first to get me in line with the workflow we adopt here. Once clean pull requests are flowing from my end we can re-discuss.
>
> Ok cool, I'm going to write up a contributors guide this evening. I will include information about the branch/merge model we're going to adopt in that.

Well, I can't do a thing right now as @edsko isn't an organisation admin and I'm therefore stuck. I've forked (back to) my hyperthunk account for the time being, so I can continue working.

I've added a sketch of a contributors guide on the wiki back there: https://github.com/hyperthunk/distributed-process-platform/wiki/Contributing. Let me know if that is

1. helpful and instructive
2. readable, sensible, etc

If there's anything wrong with it at all, please shout as I'd prefer not to put people off from contributing once we move over to this org *officially* but at the same time I know how much effort can go into managing contributions and if people know up front what to do it makes it easier for everyone.

@edsko - same question to you with regards the contributor guide.","Comment:rodlogic:12/10/12 10:03:56 PM:

mentioned","Comment:edsko:12/10/12 10:03:56 PM:

mentioned","Comment:rodlogic:12/11/12 03:06:15 PM:

@hyperthunk A few comments (I may have more later):
* the git workflow is quite similar to what I was following lately with one difference: I wasn't merging the feature branch back to my local master branch before issuing the pull request.
* I often see pull requests being used to converge into a final, agreed implementation: i.e. it goes through code reviews, comments and possibly multiple commits until it is ready to be accepted. So, although, a single, squashed commit is always goal for a given iteration, I think we shouldnt see it as a strict rule. The most important is to make sure the commits are rebased and up-to-date with master before a final merge (merge conflicts are not the responsibility of the maintainer).
* Is it a consensus that rebases are better than merges in git's best practices (real question)? If so, I think there was a way to automatically have git use rebase when ```git pull``` (?).
* I have been using stylish-haskell to automate reformatting: i.e. stripping trailing white spaces, replacing tabs with spaces, aligning and sorting imports (so no manual labor here) and pragmas, and performing some light weight reformatting of the code. I am not married to one style over another and the guidelines you wrote down/referenced above works for me, but it would be great if we could automate this as much as possible. Unless there are better tools I am not aware of, I would suggest using stylish-haskell and adding a ```.stylish-haskell.yaml``` to the root of the repo so we share the same settings.
* Although it adds a bit of labor (I wish tools such as stylish-haskell could automatically do this for me), I find it quite important to be explicit about the specific types being imported from modules. So ```import Control.Applicative (Applicative)``` instead of a ```import Control.Applicative``` as it become quite obvious where the type is coming from.","Comment:hyperthunk:12/11/12 03:06:15 PM:

mentioned","Comment:hyperthunk:12/11/12 03:57:08 PM:

Hi @rodlogic - thanks for the feedback.

> the git workflow is quite similar to what I was following lately with one difference: I wasn't merging the feature branch > back to my local master branch before issuing the pull request.

Well in theory that *only* matters in the target repository as you're keeping up to date by rebasing `upstream/master` onto your own fork. So merging your changes back onto your own master is probably the *wrong* thing to do, as we might make edits whilst merging the pull request, particularly if we're merging manually. You should just `git fetch upstream && git rebase upstream/master` to get your fork's master branch up to date.

>  often see pull requests being used to converge into a final, agreed implementation: i.e. it goes through code reviews, comments and possibly multiple commits until it is ready to be accepted. So, although, a single, squashed commit is always goal for a given iteration, I think we shouldnt see it as a strict rule.

I quite agree. Did my contributor guide make it sound like a single squashed commit was required? If so I should probably make it clearer that isn't a script requirement.

> The most important is to make sure the commits are rebased and up-to-date with master before a final merge (merge conflicts are not the responsibility of the maintainer).

Exactly.

> Is it a consensus that rebases are better than merges in git's best practices (real question)? 

The answer is, it depends. When you want to rewrite a branch's history then rebasing is the way to go in general. When we merge, for example, feature and bugfix branches into development/master then we do not want to rewrite history. It is much easier to review the history of the main (dev/master) branches and look at the changes that were integrated and when, across releases, by **not** rewriting the branch history. Instead, we disable git's default merge behaviour, which is to apply changes using *fast forward* if possible, by merging with `--no-ff` so that the disparate histories are maintained and the branch and merge points remain clear. 

> If so, I think there was a way to automatically have git use rebase when git pull (?).

As I mentioned just a moment ago, git will automatically *try* to do a fast-forward when merging using its default strategy. So on the whole, merges will often look like rebases if there are no differences between the two branches.

> I have been using stylish-haskell to automate reformatting

Yes, I like it in general, but...

> I am not married to one style over another and the guidelines you wrote down/referenced above works for me, but > it would be great if we could automate this as much as possible.

The problem I have with stylish-haskell is its formatting of the import statements. This works fine as long as you run it all the time (and remember to run it all the time) but it suffers from several problems

1. the formatting frequently breaks the 80 column width limit
2. every contributor has to install and use it, or maintain the formatting by hand

Forcing contributors to use a particular tool, especially a code formatting tool, seems a bit heavy handed. If the formatting that stylish-haskell chose for imports wasn't so compilcated, then this wouldn't matter, but it is *hard* to maintain that scheme by hand.

As an alternative, the formatting scheme I've recommended should never require **any** manual intervention, as new lines do not have to be aligned with their surroundings and when importing specific entities, the default indentation is being used just as you would for a function or whatever. I can't really see the overheads there, whereas there is an overhead in using stylish-haskell, even if it's just remember to run it - for those who aren't using haskell-mode or vim with this integrated for code style.

> Unless there are better tools I am not aware of, I would suggest using stylish-haskell and adding a .stylish-haskell.yaml to the root of the repo so we share the same settings. 

If we are going to do this, then just putting in the top of the repository isn't enough. We need to make sure that **any** contributor is able to use it, so we'd have to do a bit more:

1. have a rule in the makefile that installs stylish-haskell if it's not found on the path (using cabal) 
2. have a make rule that commits changes after running stylish-haskell `commit: tidy` as well. 
3. document in the contributors guide that the need to either run `make tidy` before committing or `make commit`

I **am** willing to consider this, but I'd like to hear from @edsko what he thinks as personally I find the import layout that it uses quite horrid and unfamiliar. Can the way that imports are aligned be customised at all? That would make me more included to standardise on this.

> Although it adds a bit of labor (I wish tools such as stylish-haskell could automatically do this for me), I find it quite > important to be explicit about the specific types being imported from modules.

I'm a bit lazy about this sometimes, but actually I completely agree with you that this is a good idea most of the time. Until you've imported more than 70% of the types in the module at which point it seems a bit moot.

You *could* always use eclipse with FP/Haskell support. It automatically adds imports for you and tends towards using specific ones rather than importing eveything. It will not do the same layout as stylish-haskell though. You *may* be able to get a combination of haskell-mode and scion to do this - I can't quite remember whether I had that level of automation sorted in emacs before, but since I've moved to the 2012 haskell platform most of the really nice emacs integration (scion, ghc-mode) seems too broken to be of much use.
","Comment:rodlogic:12/11/12 03:57:08 PM:

mentioned","Comment:edsko:12/11/12 03:57:08 PM:

mentioned","Comment:hyperthunk:12/11/12 03:58:45 PM:

> 1. have a rule in the makefile that installs stylish-haskell if it's not found on the path (using cabal)
> 2. have a make rule that commits changes after running stylish-haskell commit: tidy as well.
> 3. document in the contributors guide that the need to either run make tidy before committing or make commit

Oh and I should point out that this will become a bigger overhead as soon as you encounter someone who is trying to do `make tidy` and for some reason or other the automated (or manual) install of stylish-haskell doesn't work. That is basically another dependency we've got to maintain, outside the cabal build structure, just to make sure people can contribute to the project in a consistent way without having to manually align all the imports.","Comment:edsko:12/11/12 03:58:50 PM:

@hyperthunk -- You should have admin access to the repository now.","Comment:hyperthunk:12/11/12 04:00:37 PM:

@edsko - sweet, I'm in now. Thanks for sorting that.","Comment:edsko:12/11/12 04:04:30 PM:

@hyperthunk -- I've read through your contributing guidelines and they make perfect sense to me. I also completely agree with you re the layout of import statements -- I hate the way that that `stylish-haskell` does it (and don't use it for that reason). My personal preference is precisely what you suggest. ","Comment:rodlogic:12/11/12 04:11:13 PM:

@hyperthunk Re: stylish-haskell: sounds good! I don't have a strong preference and I buy the arguments.","Comment:hyperthunk:12/11/12 04:55:27 PM:

Thanks both of you for your feedback. I'm going to make some small amendments to the contributor guide and publish it onto the wiki. I'll also probably set up github.pages and reference it from there.

We now have 'development' and 'master' branches. The rest of the branching strategy will become obvious as I start merging things.

I'm going to hold off on making a tag for our current 0.1.0 until we've resolved a couple of things.

1. stability of the gen server implementation
2. hiding as much detail as possible

To help with (2) I have already added a top level `Platform` module from which to export commonly used entities and provide entry points into the most useful APIs and sub systems. Minimising the entities we export will make it *much* easier to go ahead with changes without having to make major version increments all the time.

In terms of (1) I think that we need to be a little bit more patient to let the gen server API settle down. IMO implementing supervisor on top of it will help us iron out the kinks nicely. I'm hoping that you're still interested in owning that API @rodlogic as you've done most of the work there. Obviously I'll still chip in to get things done.

re @rodlogic 
> Re: stylish-haskell: sounds good! I don't have a strong preference and I buy the arguments.

Ok well let's leave that alone for now. Maybe I'll find time to hack stylish-haskell and make the layout configurable some time in the future, at which point I'd be happier to integrate it.","Comment:hyperthunk:12/11/12 04:56:10 PM:

Note to all: I will close this issue once we've tagged a 0.1.0 release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
23,Support for Async and async calls (Issue #8),"See: https://github.com/hyperthunk/distributed-process-platform/issues/8

This extracts a reusable Async module and adds an explicit callTimeout. So now we have:
```haskell
call :: (Serializable rq, Show rq, Serializable rs, Show rs) => ServerId -> rq -> Process rs

callTimeout :: (Serializable rq, Show rq, Serializable rs, Show rs) => ServerId -> Timeout -> rq -> Process (Maybe rs)

-- | Async call to a server
callAsync :: (Serializable rq, Show rq, Serializable rs, Show rs) => ServerId -> rq -> Process (Async rs)
```

And the Async module, provides:
```haskell
async :: (Serializable b) => ProcessId -> Process () -> Process (Async b)

-- | Wait for the call response
wait :: (Serializable a, Show a) => Async a -> Process a

-- | Wait for the call response given a timeout
waitTimeout :: (Serializable a, Show a) => Async a -> Timeout -> Process (Maybe a)
```

Sync calls working as before, but async calls still has some issues.",12/10/12 12:45:59 PM,12/10/12 04:28:48 PM,Task,,Closed,Fixed,rodlogic,,"Comment:hyperthunk:12/10/12 01:42:38 PM:

According to github's pull request manager I'm going to have to merge this by hand. That's fine, but I want to finalise issue DPP-22 and tag an initial alpha release soon thereafter. ","Comment:hyperthunk:12/10/12 01:52:46 PM:

Hmn, this history is a bit messy. @rodlogic are you able to squash this into a single commit so I can merge it cleanly, even if not automatically? It would also make following the history a lot easier for future readers, as the API churn is getting quite confusing.

On a side note, it's probably a good idea to factor each conceptual change into its own branch and merge/pull them individually, rather than introducing a lot of changes in one go.","Comment:rodlogic:12/10/12 01:52:46 PM:

mentioned","Comment:rodlogic:12/10/12 02:19:27 PM:

@hyperthunk I will squash them, but it will have to wait for a day or so.","Comment:hyperthunk:12/10/12 02:19:27 PM:

mentioned","Comment:hyperthunk:12/10/12 02:21:57 PM:

@rodlogic - ok leave it, I'll do this by hand as I want to move the `Timer` module over to using async where possible.","Comment:rodlogic:12/10/12 02:21:57 PM:

mentioned","Comment:rodlogic:12/10/12 02:28:00 PM:

@hyperthunk Sorry, this is an area in git I am not 100% comfortable (I am a long time Subversion user) and will need to spend more time than I can at this moment.

Additional discussions related to git-flow or whatever workflow we decide we'll be very beneficial to get me in line with git/github best practices.","Comment:hyperthunk:12/10/12 02:28:00 PM:

mentioned","Comment:hyperthunk:12/10/12 03:12:10 PM:

merged","Comment:hyperthunk:12/10/12 03:12:10 PM:

closed","Comment:rodlogic:12/10/12 04:24:26 PM:

@hyperthunk Thanks for sorting out the mess.","Comment:hyperthunk:12/10/12 04:24:26 PM:

mentioned","Comment:hyperthunk:12/10/12 04:28:48 PM:

@rodlogic sure no problem. 

>  this is an area in git I am not 100% comfortable (I am a long time Subversion user) and will need to spend more time > than I can at this moment.

I completely understand. Knowing that you're more familiar with svn is helpful, as I can just point out the commands (or tutorials to read) that'll get you sorted from now on.

> Additional discussions related to git-flow or whatever workflow we decide we'll be very beneficial to get me in line with > git/github best practices.

Cool. To make contributions easier in general, I'm going to add the relevant git commands to the contributors guide.","Comment:rodlogic:12/10/12 04:28:48 PM:

mentioned",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
24,expose profiling data for test runs,Initially so that we can investigate issue #18. This probably feeds into issue #12 as well.,12/10/12 03:22:20 PM,12/13/12 12:56:15 AM,Task,Initial-Public-Release,Open,Unresolved,hyperthunk,bug enhancement question build tests,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
25,Async Channel support ,"Once we've cracked issue #8 (and have passing tests) then we should really extend the `Control.Distributed.Platform.Async` module to support Channels too. For this extension, I think it actually **does** make sense to make the async task definition a function whose type is *aware* of the reply channel

```haskell

-- NB: AsyncTask is a replacement for SpawnAsync

-- | A task to be performed asynchronously. This can either take the
-- form of an action that runs over some type @a@ in the @Process@ monad,
-- or a tuple that adds the node on which the asynchronous task should be
-- spawned - in the @Process a@ case the task is spawned on the local node
type AsyncTask a = Process a | (NodeId, Process a)

-- | A task to be performed asynchronously using typed channels
type AsyncChanTask a = SendPort a -> Process () | (NodeId, SendPort a -> Process ())

``

It is **possible** that the implementation of `Async` (in issue #8) could change to use channels internally anyway, at which point this might become a moot point.",12/12/12 04:20:54 PM,01/04/13 03:36:22 PM,Task,,Closed,Fixed,hyperthunk,duplicate enhancement API,"Comment:rodlogic:12/13/12 01:58:41 PM:

fyi:See this pull request by Simon Marlow: 

https://github.com/haskell-distributed/distributed-process/commit/847abf494233523dba7d0b40628c3af9e870be91

It seems to address the issue of efficiently receiving channel and process messages.
","Comment:hyperthunk:01/04/13 03:36:22 PM:

The async-refactoring branch now contains two async modules, one for STM based interactions and another one that uses channels. Resolving as a duplicate of issue DPP-38.","Comment:hyperthunk:01/04/13 03:36:22 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26,Worker Pool,"A supervised pool of asynchronous tasks. These come in at least two distinct flavours.

1. a coordinator process spawns tasks on demand
2. a coordinator process that pre-allocates taks that have complex (time consuming) setup logic

The first variety benefits from having a configurable max workers limit, which can be used to throttle over-active producers and prevent resource starvation. ",12/12/12 04:30:01 PM,01/04/13 03:53:21 PM,Task,Post-0.1.0,Open,Unresolved,hyperthunk,enhancement API distributed/algorithms,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
27,Any roadmap for the project?,"Is there anything like subj or just a list of issues with no particular milestones?
I think such a project need some info in wiki (which seems to be empty at the moment)
",12/12/12 05:56:11 PM,12/13/12 01:12:09 AM,Task,Misc/Admin,Closed,Fixed,qrilka,question,"Comment:hyperthunk:12/12/12 07:13:00 PM:

@qrilka - duly noted. We'll get some more things in place soon.","Comment:qrilka:12/12/12 07:13:00 PM:

mentioned","Comment:hyperthunk:12/13/12 12:57:11 AM:

assigned","Comment:hyperthunk:12/13/12 01:05:20 AM:

This issue (of having a roadmap) has been addressed: we have an initial milestone set up and several issues planned against it. I do not see any point in trying to set dates as the project is run by volunteers.

Your point about needing additional documentation is well taken - that's something we're very keen to do and I've already started adding some content to the wiki.","Comment:hyperthunk:12/13/12 01:05:20 AM:

closed","Comment:hyperthunk:12/13/12 01:07:19 AM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
28,Project Homepage,"For now, I've set up a simple project page, generated from github pages. We should probably move from a totally generated page to a jekyll site if we're going to stick with github pages, but I just wanted to get started with something for now.

Repository admins can simply click on 'settings > github-pages > automatic page generator' to edit what is already there. Please feel free if you have the itch.",12/13/12 01:10:12 AM,12/13/12 10:32:50 AM,Task,Misc/Admin,Closed,Fixed,hyperthunk,,"Comment:hyperthunk:12/13/12 10:32:46 AM:

Forgot to link to it! http://haskell-distributed.github.com/distributed-process-platform/","Comment:hyperthunk:12/13/12 10:32:50 AM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
29,Misc/Administrative,Things we have to do that do not find their way into releases (like github admin tasks and so on).,12/13/12 01:10:56 AM,01/04/13 03:50:41 PM,Task,,Closed,Fixed,hyperthunk,,"Comment:hyperthunk:12/13/12 01:13:01 AM:

closed","Comment:hyperthunk:12/13/12 01:14:35 AM:

reopened","Comment:hyperthunk:01/04/13 03:50:41 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
30,Logging infrastructure,"OTP provides some nice library and application support for logging. This is not distributed logging mind you, but just deals with logging on the local node.

Should we think about doing the same, building perhaps on the core CH APIs to add more features?",12/13/12 09:31:37 AM,12/17/12 07:05:31 PM,Task,Initial-Public-Release,Open,Unresolved,hyperthunk,enhancement API,"Comment:hyperthunk:12/17/12 07:05:31 PM:

We want to move logging (via `say`) out of CH, so this has become higher priority","Comment:hyperthunk:01/07/13 05:57:22 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
31,Application Concept,"OTP is heavily dependent on the concept of an application: a top level process that launches the root supervisor and provides one or two other goodies.

Applications are a central unit of reuse, control (e.g., initialisation, upgrade) and dependency in OTP. I suspect we'd do well to borrow this idiom.

What might we do differently in CH to OTP? Can we improve in that model (which isn't without it's flaws and was designed decades ago) and what kind of feature set do we want?",12/13/12 09:35:12 AM,01/15/13 05:04:53 PM,Task,Initial-Public-Release,Open,Unresolved,hyperthunk,enhancement question API,"Comment:hyperthunk:12/18/12 03:23:20 AM:

Some of the features OTP applications provide are:

* standardised configuration
* dependencies (between applications)
* loading/starting applications (see below)
* shutdown handling
* distributed applications

The application concept in OTP is quite simple. When you start an application, the application resource file is loaded and any dependent applications need to be started if they're not already running. The application is started by the application controller, which calls a behaviour (callback module) for your application (defined in the .app resource file) that in turn is supposed to return the pid of a fully initialised top-level supervisor.

In practise things are a bit more complex typically. Putting aside OTP features that aren't relevant to CH right now (such as releases and hot code upgrade) there are other *parts* of starting up an application that bear some consideration. Application's support the concept of start phases (and stop phases too), which allow us to initialise e.g., certain subsystems one at a time. These *boot steps* can depend on one another and define behaviours that need to be run successfully in order to complete. Sometimes a *boot step* might return some intermediate response that needs to be provided to subsequent steps.

In OTP you need to either explicitly start dependent applications in your code or build a release which creates a boot script that does this (in order) for you. In OTP applications can include other applications, which is a way of saying that you want to start the top-level supervisor for that application yourself and put it beneath one of your own supervisors. One way of looking at this is to see the included application as providing a sub-system, for example a web server process might be just one subsystem within the overall application hierarchy.

OTP applications have start-types which define how the runtime system should behave if the application fails (i.e., the top level supervisor crashes). You might want to

* shut down
* restart
* do either just for you application
* or for *all* dependent applications
* or for all applications *on the right* as it were

So - I think that the application concept can be worked into quite a clean model for CH. Without thinking about distributed (yet) we might extend the OTP definitions up a bit like so.

* an application is a kind of (glorified) root supervision tree
* all applications have a single top-level supervisor process which is managed for them (by the platform)
* an application must define (e.g., export and specify in configuration) one user-defined top-level supervisor
* the top level process might launch this supervisor when the application is started
* an application might want to use functionality provided by other applications
* to use *other applications* the application specifies them as dependencies - these are referred to as sub-systems
* application dependencies can be started and installed into the top level (managed) supervisor process automatically
* application dependencies can be started (and installed) into *any* supervision tree within another application or sub-system

That's ignoring boot steps, but it **does** address the issue of application startup/restart/shutdown policy, simply by defining applications in terms of supervision trees.

The assumption at the moment is that the **absolute top level** application that starts on a node is the *application controller* application. This in turn will treat the applications loaded underneath it as sub-systems, even though they *appear* to be top-level (as they are allowed to be independent of one another).


","Comment:hyperthunk:12/18/12 03:24:51 AM:

Nice-2-have stuff that OTP doesn't support:

* distributed applications that aren't just about fail-over
* parallel startup
* parallel distributed startup
* more than one mode of start/stop synchronisation
","Comment:hyperthunk:01/07/13 05:57:22 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/
","Comment:hyperthunk:01/07/13 06:08:32 PM:

See also https://github.com/jepst/distributed-process-global","Comment:hyperthunk:01/09/13 10:44:27 AM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/
","Comment:ericbmerritt:01/14/13 12:30:08 AM:

@hyperthunk I think for clarity we need to boil the concept of an Application down to its most basic definition, then we need to layer the higher level concept of the Release Handler on top of it. There are a lot of special cases that come up once you have this basic application support in place (start phases as you mention, are one of them). However, for the first most basic implementation *all of that can be ignored*. I am also going to ignore the tooling Erlang has to support releases. I am just going to talk about Applications and Releases in terms of the somewhat concreatized concept not Erlang`s implementation of that concept. Its not that we don't need these things, just that they are built on top of this functionality. So for version 0.0.0.1 its just a distraction. 

__ Releases

  In Release is simply a list of Applications that work together to provide a specific functionality. That functionality is usually at the level of a system. This list of Applications is usually described a file that the Release Handler uses to start each Application.

** Note: ** The concept of a Release is what most other systems would call an Application. Unfortunately, in Erlang the  concept of an Application is overridden. So we use the term Release. For example the 'Riak' Release specifies a set of OTP Applications that work together to provide a No SQL database. In the same vein, the 'ejabberd' Release is a set of OTP Applications that work together to provide a Chat Server. 

__ Release Handler

The Release Handler is a system process that is responsible for starting and stoping all the Applications described in the Release file in an orderly way. It uses metadata 'exported' by the Applications to figure out what order the Applications should be started in and what modules/functions should be used to start the Application. Once all Applications are started, it then monitors those Applications. If one of those Applications stops for some reason, it attempts to shut down all applications in the Release. It also reacts to external events asking for a shutdown by also attempting to shut down all Applications in the Release.
 
__ Applications

An Application, in the OTP sense, is a unit of functionality that is expected to provide a fine grained, long lived set of 'services' to the system. It includes the definition of those long lived services, related support code, and metadata that tells the release handler how to start the Application and what prerequisites must exist for the Application to be started  successfully.   

** Note 1: ** In Erlang there are Active Applications and Library Applications. That is because, in Erlang, Applications provide both the concept of a Library and a living set of Services. However, Haskell already has a very good module system that completely covers the idea of a 'Library Application' as expressed in Erlang. So Library Applications can be more or less ignored here. 

** Note 2: ** I am completely ignoring supervision trees. From a Release Handling/Application Lifecycle perspective they don't matter one bit. 

___ Lifecycle

The Application behavior supports two fundamental lifecycle operations, `start` and `stop`.

____ Start

The Application must export a `start` function that is responsible for starting a long running process and returning that process id. This process is expected to be long lived. If the process stops for some reason, that is considered an failure and the release handler begins shutting down the entire release. 

In OTP Applications, the process id returned is always a Supervisor. 

____ Stop

The Application must export a `stop` function that is responsible for gracefully stopping the Application. 

___ Metadata

The metadata (in Erlang) is simply a configuration file in a well defined format and a well defined location that the Application uses to describe to the Release Handler how to start and stop it, and what other Applications need to be already running be fore this application starts.    

__ Wrapping Up

This is a really stripped down description of Applications and Releases. However, it is a complete description and, if implemented' would provide a base on which to build the more complex additional features you describe above. ","Comment:hyperthunk:01/14/13 12:30:08 AM:

mentioned","Comment:ericbmerritt:01/14/13 04:03:40 AM:

To comment on my own issues. There is no reason not to create the Release Handler that takes a list of App descriptions as its argument. Doing that you leverage the compiler to handle versioning and you just worry about the mechanics of starting the applications in the right order. With that App descriptions are just records. That way you let the compiler do all the checking for you on the App metadata. If the part of the metadata is the relevant start and stop functions you remove any need at all to do any discovery. ","Comment:hyperthunk:01/14/13 03:37:01 PM:

Yes I completely agree that we should leverage the compiler as much as possible. I think we should make some decisions about whether or not leaving it up to the user to return a supervisor pid is the best way to *manage* an 'application' though.

I also think that you've made a good point about the terminology here: I don't like using *application* to describe what is a component when you're developing it, yet is (or forms part of) a sub-system when it's being used at runtime. I also think *release* is not a good word to use for the overall system, as that's overloaded too.","Comment:ericbmerritt:01/15/13 05:04:52 PM:

Releases are not about managing an application at all. They are about providing the application lifecycle. That is starting up and shutting down. Application management is what the supervision structure it is for. I think its somewhat important to keep those concepts distinct. 

Granted, 99.99% of the time people are going to use supervisors and thats absolutely what we should encourage in all the documentation. If we place that restriction on the apps it really will be an arbitrary restriction, one we have to go out of our way to enforce.  So we both add some (slight) complexity and reduce the potential developers ability to try both new and interesting things, like a new, better implementation of the concept of application supervision. 

I agree with you on the terms. Coming up with new ones is going to be painful though from a thinking up useful names perspective. 

On a side note. I realize that *all* we have to worry about is lifecycle here. Cabal actually provides all of the other features of a standard OTP Application. We should encourage one app per cabal package and then just let cabal manage the rest. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
32,Examples/Demos,"We should have some of these. I suspect we will need to break up this repo into a multi-project repo.

We could then absorb other projects (like logging) that are related but not core to the library.

Should we just administer them at the top level, or use hit sub modules, or git subtree merging? I favour the latter, as it makes it easy to push back and forth, but perhaps just putting everything into a single repo is fine. I'd appreciate some reasoned opinions on this.",12/13/12 09:42:40 AM,12/13/12 09:42:40 AM,Task,Misc/Admin,Open,Unresolved,hyperthunk,examples/demos,"Comment:hyperthunk:01/04/13 03:48:26 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
33,Monitoring/Stats,"Erlang/OTP provides rich support for monitoring and profiling the state of a running system. This support can be enabled at any time, doesn't require special compilation and has negligible impact on runtime performance.

To what extent can we provide similar facilities for CH? Do they belong in the platform or core CH layers? What would such metrics look like an what kind of APIs would we want to expose for reading them?",12/13/12 10:27:07 AM,12/19/12 04:08:43 PM,Task,Post-0.1.0,Open,Unresolved,hyperthunk,question API monitoring infrastructure,"Comment:rodlogic:12/13/12 02:39:41 PM:

See: [EKG](http://hackage.haskell.org/package/ekg), specially the core part where gauges, counter, labels, etc can be installed and then consumed/monitored.

At first, a set of per OS process metrics and then an automatic way of aggregating the metrics across the CH cluster. I am sure Erlang may have some good role models here too. ","Comment:hyperthunk:12/13/12 06:24:35 PM:

Sounds like a good place to start. Erlang provides stats on a per erlang process (e.g., per green thread) basis. It makes sense to do that because each thread is garbage collected individually and has its own stack + heap. That might make less sense for Haskell, where garbage collection is global. Reminds me to look at whether the GHC parallel GC efforts got anywhere.

Anyway, having EKG would be a start. Need to look at what constraints this puts on how the code needs to be compiled and decide whether they're acceptable or whether that should just be optional.

We should also consider providing OS level stats via a background worker - see the erlang os_mon application for example.

Support for SNMP would also be nice.

User defined updatable performance counters would also be handy, though I wonder whether that already exists and/or should really live outside CH/Platform","Comment:hyperthunk:12/14/12 03:00:29 AM:

Also see the issue I raised for core CH to provide useful metrics [here](https://github.com/haskell-distributed/distributed-process/issues/65_issuecomment-11363541).","Comment:hyperthunk:12/17/12 01:20:45 AM:

And some initial process status in [distributed-process issue 89](https://github.com/haskell-distributed/distributed-process/issues/89) which I'm currently doing some work on.","Comment:hyperthunk:12/19/12 04:08:43 PM:

I think the focus here will be on statistics, monitoring and subscribing to system events. There is definitely an overlap with the management API I've proposed for CH and even more so the management tools for -platform.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
34,rename intervalToMs and timeToMs,"Other options on a postcard, but I'm not feeling so keen about these now. The `intervalToMs` function could, for example, become

* fromInterval
* intervalToMillis(econds)
* toMillis(econds)

How I hate choosing names for things.... Sigh.",12/17/12 01:18:57 AM,01/14/13 10:16:54 PM,Task,Initial-Public-Release,Closed,Fixed,hyperthunk,enhancement API,"Comment:hyperthunk:01/14/13 10:16:47 PM:

assigned","Comment:hyperthunk:01/14/13 10:16:54 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
35,Provide linkOnFailure API,See [distributed-process issue 75](https://github.com/haskell-distributed/distributed-process/issues/75) for details.,12/17/12 12:50:02 PM,01/04/13 09:05:48 PM,Task,Initial-Public-Release,Closed,Fixed,hyperthunk,enhancement API,"Comment:hyperthunk:12/17/12 12:50:02 PM:

assigned","Comment:hyperthunk:01/04/13 09:05:21 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/
","Comment:hyperthunk:01/04/13 09:05:48 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
36,Distributed.Process.Management Toolset,"In line with CH [issue 97](https://github.com/haskell-distributed/distributed-process/issues/97) there are times when you'd like to get an overview of running processes in your system. Issue #89 is already starting to provide some of the primitives required to achieve something like this. There are other things a system administrator might want to do, such as manually running `exit` to `terminate` on a given process, set up links/monitors or even manually injecting messages (although that last one's a bit far-out).

Whilst the idea behind `Network.Transport.Management` is just about providing an API so we can build tools, the tools for **this** feature already exist by and large (viz getProcessInfo, exit, kill, link, etc) so this issue is really about providing the *actual* tools themselves.

Now this is the kind of thing that makes administrators and tech support *love* rather than *hate* your application. 

Both command line and web based interfaces would be nice.

I suspect this should be a top level project. Whether it belongs here or bundled in with -platform is another thing to consider.",12/18/12 04:34:41 PM,01/04/13 03:53:31 PM,Task,Post-0.1.0,Open,Unresolved,hyperthunk,API infrastructure,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
37,Break up the test suites,"I don't like `TestMain.hs` - I prefer the way we do things in distributed-process, where there are multiple independent test suites and the cabal configuration ensures they're all run. I **do** want to keep the code they all rely on shared though.",12/20/12 04:41:26 PM,01/19/13 02:39:10 AM,Task,Initial-Public-Release,Closed,Fixed,hyperthunk,enhancement build tests in-progress,"Comment:hyperthunk:12/20/12 04:41:26 PM:

assigned","Comment:hyperthunk:01/19/13 02:39:10 AM:

Done in HEAD.","Comment:hyperthunk:01/19/13 02:39:11 AM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
38,Async Package,This is an initial draft of an `Async` API for distributed-process based applications. Modelled on the other `Async` package by @simonmar.,12/20/12 08:16:54 PM,01/11/13 01:27:38 PM,Task,Initial-Public-Release,Closed,Fixed,hyperthunk,,"Comment:simonmar:12/20/12 08:16:54 PM:

mentioned","Comment:hyperthunk:01/04/13 03:36:22 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/
","Comment:hyperthunk:01/11/13 01:27:38 PM:

merged","Comment:hyperthunk:01/11/13 01:27:39 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
39,Mirrored Supervisor,See https://github.com/rabbitmq/rabbitmq-server/blob/master/src/mirrored_supervisor.erl. Depends on issue #3.,01/03/13 11:54:31 AM,01/03/13 11:54:37 AM,Task,Post-0.1.0,Open,Unresolved,hyperthunk,enhancement API distributed/algorithms,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
40,Distributed Transactions / Leader Election,"We *should* build this. Paxos is the reference architecture starting point, though there are numerous variations on the same. See also...

* https://github.com/jepst/distributed-process-global
* issue #48 (a corollary feature)",01/03/13 11:55:20 AM,01/07/13 06:07:36 PM,Task,Post-0.1.0,Open,Unresolved,hyperthunk,enhancement API distributed/algorithms,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
41,Contributors Guide,Sync with d-p issue 94.,01/04/13 03:40:54 PM,01/04/13 03:50:57 PM,Task,Misc/Admin,Open,Unresolved,hyperthunk,documentation in-progress,"Comment:hyperthunk:01/04/13 03:40:54 PM:

assigned",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
42,Elevator Demo,"Following on from issue #32, it might be nice to copy the Erlang/OTP elevator demo: http://erlang.org/pipermail/erlang-questions/2013-January/071488.html. This depends on various other issues being resolved first.",01/04/13 03:48:26 PM,01/04/13 03:50:10 PM,Task,Misc/Admin,Open,Unresolved,hyperthunk,examples/demos,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
43,ProcessId wrapper for hot-loading/migration,"An obstacle to migrating processes between nodes is how to deal with ProcessIds. 

If we assume that any process can be migrated to another node at any time, ProcessIds are no longer reliable. Instead, we need another way to refer to migratable processes.

This problem may be separable into two sub-problems:

1. Within an Application which will be migrated as a whole, processes may refer to each other. Here, it may be possible to substitute ProcessIds with correct values as part of migration.
2. References by ProcessId to the Application from elsewhere will need to be updated.

It might make sense to establish a wrapper type, MigratableProcessId, that provides this additional layer of indirection. Sending a message to a MigratableProcessId will resolve to the actual current ProcessId of the given process.",01/04/13 05:57:08 PM,01/19/13 02:38:40 AM,Task,Post-0.1.0,Open,Unresolved,jepst,enhancement API,"Comment:hyperthunk:01/19/13 02:38:40 AM:

> An obstacle to migrating processes between nodes is how to deal with ProcessIds. 

There are other obstacles too. Whilst I think migrating between nodes might be do-able, I'm not done with exploring the possibility of loading changes on the fly in a running executable. Isolating dynamically loaded entities **is** possible, dynamic loaded does work (as plugins/hs-plugins demonstrates), [safecopy](http://hackage.haskell.org/package/safecopy) or something like it might provide the means to deal with conversions....

We will see. But I do think migrating process state would be easier and therefore this enhancement remains a perfectly valid proposal until we decide *what* we want to do.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
44,Maintainers' Guide,How to be a good citizen...,01/04/13 06:37:10 PM,01/04/13 06:37:10 PM,Task,Misc/Admin,Open,Unresolved,hyperthunk,documentation misc/admin in-progress,"Comment:hyperthunk:01/04/13 06:37:10 PM:

assigned",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
45,Implemented linkOnFailure,As per issue #35 ,01/04/13 09:05:21 PM,01/04/13 09:05:47 PM,Task,,Closed,Fixed,hyperthunk,,"Comment:hyperthunk:01/04/13 09:05:47 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/4df888e69111a47c9847112c310ee84343393028
","Comment:hyperthunk:01/04/13 09:05:47 PM:

merged","Comment:hyperthunk:01/04/13 09:05:47 PM:

closed","Comment:hyperthunk:01/04/13 09:06:27 PM:

head_ref_deleted",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
46,Configuration APIs,"Following on from issues #30 and #31 (though many more features may depend on it), it is fairly commit to need static configuration in a distributed system. It'd be nice to have a standard API for managing this, though I suspect there will be as many opinions about which format is best as there are developers using the tools.",01/07/13 05:57:22 PM,01/15/13 04:53:05 PM,Task,,Open,Unresolved,hyperthunk,enhancement question API,"Comment:ericbmerritt:01/13/13 11:42:16 PM:

I think its more about providing the person assembling the Release (at least in Erlang) with a single stable means to configure all the applications in that release. If each application (in the OTP sense) has some different configuration semantic it would be a nightmare for the Release Assembler. ","Comment:hyperthunk:01/14/13 03:38:34 PM:

Yes indeed. I think your comments in issue DPP-31 about making the compiler check all the configuration is probably the right thing to do here. We should aim to provide some standardisation for components and sub-systems, but probably that'll just come out in the wash as the *application* (or whatever) concept grows.","Comment:ericbmerritt:01/15/13 04:53:04 PM:

In general, for the high level release configuration I don't think you are going to be able to compile that. That could very well be something that is going to need to change for every host that the 'release' is running on. 

Thats very different then application/release metadata which is, for all intents and purposes, static.

Just something to keep in mind. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
47,Group Services,"A la [Apache ZooKeeper](http://wiki.apache.org/hadoop/ZooKeeper) perhaps, and I'm quite fond of ZAB. It would be sensible however to consider using [global](https://github.com/jepst/distributed-process-global) to implement this based on the existing cluster protocols initially, looking to more complex (but perhaps reliable) forms that use atomic broadcast later on - see [RabbitMQ's implementation](http://hg.rabbitmq.com/rabbitmq-server/file/default/src/gm.erl) for example.",01/07/13 06:01:57 PM,01/07/13 06:01:57 PM,Task,,Open,Unresolved,hyperthunk,enhancement API distributed/algorithms,"Comment:hyperthunk:01/07/13 06:03:36 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
48,Atomic Broadcast / Guaranteed Multicast,"See RabbitMQ's implementation and ZaB as reference architectures:

* http://wiki.apache.org/hadoop/ZooKeeper/Zab
* http://hg.rabbitmq.com/rabbitmq-server/file/default/src/gm.erl",01/07/13 06:05:18 PM,01/07/13 06:05:18 PM,Task,,Open,Unresolved,hyperthunk,enhancement API distributed/algorithms,"Comment:hyperthunk:01/07/13 06:07:36 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
49,Support for automating derived/composed RemoteTables,"An enabler for issue #31 is the ability to generate an initial `RemoteTable` from a bunch of dependencies.

### Why do we want this?

Consider an application that uses both d-p-platform and d-p-global and another imaginary application d-p-consensus which add support for distributed transactions built on d-p-global's locking. The code in d-p-global will build its own `RemoteTable` by composing d-p-platform's, and d-p-consensus, if it wants to use d-p-global will compose that `RemoteTable` with its own definitions. This is simple for the user, because all they need to do is compose the d-p-consensus `RemoteTable` with their own, however there may be other libraries and/or definitions which are included in the application as *dependencies* but not part of d-p-consensus.

An example of this might be an application composed of (illustrative imagined applications such as) d-p-monitoring, d-p-pool, d-p-admin and so on. Each time the application developer wants to use a library/component, any `__remoteTable` definitions for the components will need to be composed. It would be good if we could automate this process to some extent. 

### What would this look like?

I'm not so sure about this. Perhaps build-time support for building a CH *Application* that reads the set of inter-dependent components and generates the `RemoteTable` properly. Perhaps just having some template haskell support that can be invoked by the main application module would do.

```haskell
$(deriveRemoteTables dependencyList)
``` ",01/09/13 10:44:27 AM,01/09/13 10:44:27 AM,Task,Post-0.1.0,Open,Unresolved,hyperthunk,enhancement API,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
50,SNMP Agent,See http://www.erlang.org/doc/apps/snmp/index.html for reference.,01/09/13 10:51:56 AM,01/09/13 10:51:56 AM,Task,Post-0.1.0,Open,Unresolved,hyperthunk,enhancement API,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
51,some minor 'cleanup' changes to dip,,01/09/13 01:59:32 PM,01/10/13 03:01:28 PM,Task,,Closed,Fixed,ericbmerritt,,"Comment:hyperthunk:01/09/13 05:00:37 PM:

Eric - this looks fine but doesn't merge cleanly. Would you mind rebasing and updating the pull request?","Comment:ericbmerritt:01/10/13 02:50:43 PM:

@hyperthunk done","Comment:hyperthunk:01/10/13 02:50:43 PM:

mentioned","Comment:hyperthunk:01/10/13 03:01:12 PM:

Referenced in commit:
https://github.com/haskell-distributed/distributed-process-platform/1e13c64d74afc6973c9d8be1c84367cea22a6d84
","Comment:hyperthunk:01/10/13 03:01:12 PM:

merged","Comment:hyperthunk:01/10/13 03:01:13 PM:

closed","Comment:hyperthunk:01/10/13 03:01:28 PM:

Merged - thanks Eric.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
52,Unified API for making AsyncTask,"At the moment we have `asyncDo` but that's pretty ugly, leaving a default local spawn looking like this:

```haskell
h <- async $ asyncDo $ do ..... 
```

Urgh. I suspect type synonym families might help here.",01/11/13 02:22:47 PM,01/19/13 02:34:55 AM,Task,Initial-Public-Release,Closed,Fixed,hyperthunk,enhancement API in-progress,"Comment:hyperthunk:01/11/13 02:22:47 PM:

assigned","Comment:hyperthunk:01/19/13 02:34:55 AM:

merged changes to the async API solve this neatly enough.","Comment:hyperthunk:01/19/13 02:34:55 AM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
53,rename Process to GenProcess,"along with some cleanup commits.

Note that I only renamed Process, not ProcessAction, RPC or any of the others. I think those are fine as is, but that depends on your logic. ",01/11/13 08:56:58 PM,01/13/13 12:40:18 AM,Task,Initial-Public-Release,Closed,Fixed,ericbmerritt,,"Comment:hyperthunk:01/11/13 09:10:12 PM:

assigned","Comment:hyperthunk:01/11/13 09:11:58 PM:

:+1: will merge shortly.","Comment:hyperthunk:01/13/13 12:39:55 AM:

closed","Comment:hyperthunk:01/13/13 12:40:18 AM:

Merged - thanks Eric.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
54,Control.Distributed.Process.Platform.Task,"Building on the facilities available in `Control.Distributed.Process.Platform.Async`, it would be good to be able to pass a handle to an *Async* to another process or send it to another node. That's currently impossible because we use typed channels and STM in the two Async implementations, so we'll need a central server to track the task state. This will open up lots other possibilities, including sharing async results with multiple consumers and using task pools of varying configurations to create async processing units.",01/12/13 06:02:26 PM,01/12/13 06:02:26 PM,Task,Initial-Public-Release,Open,Unresolved,hyperthunk,enhancement API in-progress,"Comment:hyperthunk:01/12/13 06:02:26 PM:

assigned",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
55,Unify Async APIs,"Trying to unify the APIs for AsyncChan and AsyncSTM in d-p-platform. I've thought about doing this with type (or data) families but ... we need to handle `AsyncChan a` or `AsyncSTM a` so...

```haskell
class Async a where
  type AsyncHandle a :: *
  poll :: AsyncHandle a -> Process (AsyncResult b)
  wait :: AsyncHandle a -> Process (AsyncResult b)
  ... etc
```

but ...

```haskell
instance Async (STM.AsyncSTM a) where
  type AsyncHandle (STM.AsyncSTM a) = STM.AsyncSTM a
  poll = STM.poll
  wait = STM.wait
```

this isn't quite what I want, as the types cannot be inferred properly. And I couldn't quite figure out whether declaring `data AsyncHandle a` in the type class and having the instance as `data AsyncHandle (STM.AsyncSTM a) = HStm (STM.AsyncSTM a)` was the right approach either, but of course you can't leave a gap in the type signature without telling the compiler what the relation is between the two:

```haskell
instance Async (STM.AsyncSTM a) where
  data AsyncHandle (STM.AsyncSTM a) = HStm (STM.AsyncSTM a)
  poll (HStm h) = STM.poll h  -- illegal because we don't know that the `a` in the associated type is related to the `a` in the instance declaration. 
```

I wondered about functional dependencies here, but couldn't see how to apply them. Anyhow, I realise this is probably a pretty basic question, but I don't see a neat way to handle it. Should I just live with having the two APIs without a common method of interchanging between them? At the moment, for the most part all you need to do to switch from `AsyncChan` to `AsyncSTM` is change your imports.",01/12/13 06:03:03 PM,01/19/13 02:34:06 AM,Task,,Closed,Fixed,hyperthunk,enhancement API,"Comment:hyperthunk:01/12/13 06:03:03 PM:

assigned","Comment:hyperthunk:01/19/13 02:34:06 AM:

This is done for a subset of the operations and I'm inclined to close it until some more specific requirement comes up.","Comment:hyperthunk:01/19/13 02:34:07 AM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
56,GenProcess doesn't drain its mailbox properly,"If you pass `infoHandlers = []` then no info messages are consumed and the process' mailbox will just keep on growing. So a standard handler that applies the default policy is probably a good idea.

estimate: 10 mins",01/14/13 10:06:57 PM,01/18/13 07:57:21 PM,Task,Initial-Public-Release,Closed,Fixed,hyperthunk,bug,"Comment:hyperthunk:01/14/13 10:06:57 PM:

assigned","Comment:hyperthunk:01/15/13 02:03:41 AM:

Fixed on the genproc2 branch.","Comment:hyperthunk:01/15/13 02:04:16 AM:

closed","Comment:hyperthunk:01/15/13 02:13:40 AM:

reopened","Comment:hyperthunk:01/18/13 07:57:21 PM:

d78185e6 has been merged into master => resolved.","Comment:hyperthunk:01/18/13 07:57:21 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
57,Provide versions of handle{Cast|Call}_ that work in the Process monad,"Consider these two cast handlers:
```haskell
handleCastIf_ (\(c :: String, _ :: Delay) -> c == ""timeout"")
                            (\(""timeout"", Delay d) -> timeoutAfter_ d),
handleCast    (\s' (""ping"", pid :: ProcessId) ->
                                 send pid ""pong"" >> continue s')
```
There's no great reason why we shouldn't be able to skip the state in the latter as we do with the former, but the `continue_` function returns `(s -> Process (ProcessAction s))` which means it can't be used in monadic code like this.",01/14/13 10:12:01 PM,01/18/13 07:58:19 PM,Task,Initial-Public-Release,Closed,Fixed,hyperthunk,enhancement in-progress,"Comment:hyperthunk:01/14/13 10:12:01 PM:

assigned","Comment:hyperthunk:01/18/13 07:58:18 PM:

Resolved in db00e3d.","Comment:hyperthunk:01/18/13 07:58:19 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
58,Remove dependency on Template Haskell,"It is *entirely* possible to install distributed-process without requiring Template Haskell. We should not therefore, IMO, force consumers of d-p-platform to use Template Haskell if they wish not to.

This is quite easy to do for the most part, although we will need to un-template the uses of `mkClosure` and `remotable` in Call.hs first.  ",01/18/13 01:53:39 PM,01/18/13 01:53:39 PM,Task,Initial-Public-Release,Open,Unresolved,hyperthunk,enhancement,"Comment:hyperthunk:01/18/13 01:53:39 PM:

assigned",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
59,Genproc2,A heavily refactored `GenProcess` with a tested and documented API.,01/18/13 06:06:32 PM,01/18/13 06:10:58 PM,Task,Initial-Public-Release,Closed,Fixed,hyperthunk,,"Comment:hyperthunk:01/18/13 06:06:38 PM:

assigned","Comment:hyperthunk:01/18/13 06:10:58 PM:

merged","Comment:hyperthunk:01/18/13 06:10:58 PM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
60,Unified API for Async,"Not quite finished yet, but this is pretty close to done, though I won't merge it immediately.",01/19/13 01:35:37 AM,01/19/13 02:34:22 AM,Task,Initial-Public-Release,Closed,Fixed,hyperthunk,,"Comment:hyperthunk:01/19/13 02:25:16 AM:

merged","Comment:hyperthunk:01/19/13 02:25:16 AM:

closed","Comment:hyperthunk:01/19/13 02:34:22 AM:

assigned",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
61,developer mode build support,"I've been using virthualenv to install development libraries (esp. the HEAD dependency on d-process) and automating that would be useful, if only so that we can get travis-ci up and running.",01/19/13 02:42:53 AM,01/19/13 02:42:53 AM,Task,Initial-Public-Release,Open,Unresolved,hyperthunk,enhancement,"Comment:hyperthunk:01/19/13 02:42:53 AM:

assigned",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
62,GenProcess improvements,"There are (3) main bits to this....

1. better support for invariants and pre/post conditions
2. pre/post processing filter chains
3. dynamic upgrade support

Items (1, 2) are motivated by this kind of code:

```haskell
-- | Start a counter server
startCounter :: Int -> Process ProcessId
startCounter startCount =
  let server = defaultProcess {
     dispatchers = [
          handleCallIf (state (\count -> count > 10))   -- invariant
                       (\_ (_ :: Increment) ->
                            noReply_ (TerminateOther ""Count > 10""))

        , handleCall handleIncrement
        , handleCall (\count (_ :: Fetch) -> reply count count)
        , handleCast (\_ Fetch -> continue 0)
        ]
    } :: ProcessDefinition State
  in spawnLocal $ start startCount init' server >> return ()
  where init' :: InitHandler Int Int
        init' count = return $ InitOk count Infinity
```
Quite what this 'pre/post' condition API will look like will hopefully emerge soon...

Item (3) is **not** a /hot code upgrade/ as it will only work with what's in the current running image, but it **does** provide a way to change the process definition on the fly, which is quite useful. This would, for example, allow you to dynamically assign tasks to workers in a pool, or allow you to turn on and off support for certain application level protocols. It's also quite simple to implement, as the [upgrades](https://github.com/haskell-distributed/distributed-process-platform/tree/upgrades) branch demonstrates. We send a `Closure (Process TerminateReason)` to the server (bundled in a message) and an automatically registered handler returns `ProcessUpgrade fun` and that 'fun' gets evaluated and becomes the main loop. The code that *uses* this mechanisms calls `become newState` and the `upgradeHandler` in `newState` gets called with `oldDefinition -> oldState`.",01/20/13 01:43:37 AM,01/20/13 01:45:25 AM,Task,Initial-Public-Release,Open,Unresolved,hyperthunk,enhancement API in-progress,"Comment:hyperthunk:01/20/13 01:43:37 AM:

assigned",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
63,GenProcess `call` interrupt results in chaos,"I'm pretty sure this is a bug. If a callback decides to terminate mid-way through a gen-call it can create havoc for the client if they're monitoring the server. The bit that *really* worries me is this:

```
Mon Jan 21 17:34:13 UTC 2013 pid://127.0.GenServerTests: 0.too few bytes. Failed reading at byte position 651
:8080:0:6: terminating....
Mon Jan 21 17:34:13 UTC 2013 pid://127.0.0.1:8080:0:6: terminating counter when state = 10 because TerminateOther ""Count > 10""
Mon Jan 21 17:34:13 UTC 2013 pid://127.0.0.1:8080:0:58: call: remote process died: DiedNormal
  exceed counter limits: [Failed]
ERROR: thread blocked indefinitely in an MVar operation
```

Now if we ignore the odd stdio interleaving, the key thing I'm concerned about is that the decoding of some message has blown up. Quite why this is happening, I'm not sure, though This might be to do with the use of `terminate` or (previously) `fail` in the async call worker process, but it's pretty awkward to debug.",01/21/13 05:58:49 PM,01/22/13 01:52:00 AM,Task,Initial-Public-Release,Closed,Fixed,hyperthunk,bug,"Comment:hyperthunk:01/21/13 05:58:49 PM:

assigned","Comment:hyperthunk:01/22/13 01:52:00 AM:

Turns out this was a silly bug in the `Binary` instance for `ProcessExitException`. Resolved with a fix in d-process for now.","Comment:hyperthunk:01/22/13 01:52:00 AM:

closed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
64,Decoding Message data in ProcessExitException (show) creates havoc,"What *actually* goes wrong is that the show implementation for `ProcessExitException` breaks, despite the fact that the logic therein works if taken in isolation and dropped into a test case. Some further investigation into the runtime characteristics is probably worthwhile, but fairly low priority. I've reverted the changes to d-process for the time being. ",01/21/13 08:04:15 PM,01/22/13 01:03:14 AM,Task,Post-0.1.0,Open,Unresolved,hyperthunk,bug,"Comment:hyperthunk:01/21/13 08:04:15 PM:

assigned",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
65,"AsyncChan should behave more like a... well, channel.","Currently `AsyncChan` is a *one shot deal*, much like its cousin `AsyncSTM`. This is probably wrong however, in as much as the suffix 'Chan' makes it sound like a channel, whilst it behaves more like a synchronised cell or an `MVar`.

I think it would be cool if `AsyncChan` actually behaved like a channel, allowing multiple values to be streamed back to the caller *but* without them knowing any details about the implementation (i.e., we continue to hide the typed channel inside).

It'd also be nice if these could be composed, though perhaps that's another issue by itself.",01/26/13 01:40:23 AM,01/26/13 01:40:23 AM,Task,Initial-Public-Release,Open,Unresolved,hyperthunk,enhancement,"Comment:hyperthunk:01/26/13 01:40:23 AM:

assigned",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
